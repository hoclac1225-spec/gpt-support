# -*- coding: utf-8 -*-
import unicodedata
import os, json, time, re, requests, numpy as np, faiss, threading, random
from collections import deque
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from openai import OpenAI
import hmac, hashlib, base64
from typing import Optional

# --- Flask & CORS ---
app = Flask(__name__)
CORS(app, resources={
    r"/api/*": {
        "origins": [
            "https://aloha.id.vn",
            "https://www.aloha.id.vn",
            "https://9mn9fa-6p.myshopify.com",
        ],
        "supports_credentials": True,
        "allow_headers": ["Content-Type", "Authorization"],
        "methods": ["GET", "POST", "OPTIONS"],
    }
})

# Load .env TRÆ¯á»šC khi Ä‘á»c os.getenv
load_dotenv()


# --- text normalize helpers (cÃ³ & khÃ´ng dáº¥u)
def _strip_accents(s: str) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if not unicodedata.combining(ch))

def _normalize_text(s: str) -> str:
    # giá»¯ láº¡i chá»¯, sá»‘, khoáº£ng tráº¯ng, cÃ¡c báº£ng chá»¯ cÃ¡i má»Ÿ rá»™ng
    return re.sub(r"[^0-9a-z\u00c0-\u024f\u1e00-\u1eff\u4e00-\u9fff\u0E00-\u0E7F ]", " ", (s or "").lower()).strip()

def _norm_both(s: str):
    """Tráº£ vá» tuple (cÃ³_dáº¥u, khÃ´ng_dáº¥u) Ä‘Ã£ normalize & lower."""
    n1 = _normalize_text(s)
    n2 = _normalize_text(_strip_accents(s))
    return n1, n2

def _char_ngrams(s: str, n=2):
    s = _normalize_text(s)
    s = re.sub(r"\s+", "", s)
    if not s:
        return set()
    if len(s) < n:
        return {s}
    return {s[i:i+n] for i in range(len(s)-n+1)}

# --- Cross-language helpers ---
CJK_RE = re.compile(r"[\u4e00-\u9fff]")
# --- ZH compat: chuyá»ƒn má»™t sá»‘ kÃ½ tá»± Phá»“n thá»ƒ -> Giáº£n thá»ƒ Ä‘á»ƒ so khá»›p
_ZH_T2S = str.maketrans({
    "éŒ¶":"è¡¨","é˜":"é’Ÿ","å¸¶":"å¸¦","éˆ":"é“¾","é‹¼":"é’¢","è† ":"èƒ¶","æ®¼":"å£³","è­·":"æŠ¤","è²¼":"è´´",
    "æ»¿":"æ»¡","é¡":"é•œ","é ­":"å¤´","æ©Ÿ":"æœº","è‡º":"å°","æª”":"æ¡£","è³ª":"è´¨","é©":"é€‚","æ¥µ":"æ",
    "é¬†":"æ¾","é¡¯":"æ˜¾","å…§":"å†…","é¡†":"é¢—","ç‡Ÿ":"è¥","ç¾":"ç°","ç¶²":"ç½‘","è¯":"è”","é–€":"é—¨",
    "é–‹":"å¼€","å»£":"å¹¿","é›»":"ç”µ","èˆŠ":"æ—§","é›œ":"æ‚","æ“š":"æ®","éŒ„":"å½•","å¹€":"å¸§",
})

def _zh_compat(s: str) -> str:
    return (s or "").translate(_ZH_T2S)

def _any_cjk(s: str) -> bool:
    return bool(CJK_RE.search(s or ""))

def _cjk_in_hits(hits, k=None) -> bool:
    if k is None:
        k = TITLE_MAX_CHECK
    for d in hits[:k]:
        if _any_cjk(d.get("title")) or _any_cjk(d.get("tags")) or _any_cjk(d.get("product_type")):
            return True
    return False

def _score_gate(q: str, hits: list, best: float) -> bool:
    """
    Tráº£ vá» True náº¿u best score Ä‘á»§ tá»‘t sau khi Ã¡p dá»¥ng ngÆ°á»¡ng Ä‘á»™ng.
    - CÃ¢u ngáº¯n: háº¡ 0.05
    - Danh má»¥c/tiÃªu Ä‘á» ZH (chÃ©o ngÃ´n ngá»¯): háº¡ 0.08
    - SÃ n tá»‘i thiá»ƒu: 0.18
    """
    th = SCORE_MIN
    if len(_normalize_text(q).split()) <= 3:
        th -= 0.05
    if _any_cjk(q) or _cjk_in_hits(hits):
        th -= 0.08
    th = max(0.18, th)
    return best >= th


# --- Title overlap config (Ä‘áº·t á»Ÿ cáº¥p module, sau load_dotenv) ---
TITLE_MIN_WORDS = int(os.getenv("TITLE_MIN_WORDS", "2"))
TITLE_CJK_MIN_COVER = float(os.getenv("TITLE_CJK_MIN_COVER", "0.25"))
TITLE_MAX_CHECK = int(os.getenv("TITLE_MAX_CHECK", "5"))

def _has_title_overlap(
    q: str,
    hits: list,
    min_words: Optional[int] = None,
    min_cover: float = 0.6
) -> bool:
    """
    So khá»›p tiÃªu Ä‘á» vá»›i cÃ¢u há»i:
    - NgÃ´n ngá»¯ cÃ³ khoáº£ng tráº¯ng: dá»±a vÃ o sá»‘ tá»« trÃ¹ng tá»‘i thiá»ƒu (min_words) hoáº·c tá»‰ lá»‡ phá»§ >= min_cover.
    - CJK/khÃ´ng khoáº£ng tráº¯ng/chuá»—i ráº¥t ngáº¯n: dÃ¹ng n-gram kÃ½ tá»± (bigrams) + báº£n chuyá»ƒn TWâ†’CN.
    - Tráº£ vá» True náº¿u *báº¥t ká»³* tiÃªu Ä‘á» nÃ o trong hits (tá»‘i Ä‘a TITLE_MAX_CHECK) Ä‘áº¡t Ä‘iá»u kiá»‡n.
    """
    if not q or not hits:
        return False

    if min_words is None:
        min_words = TITLE_MIN_WORDS

    # Chuáº©n hoÃ¡ cÃ¢u há»i
    qn1, qn2 = _norm_both(q)
    qtok = [w for w in qn1.split() if len(w) > 1]  # token theo tá»«
    is_cjk = _any_cjk(qn1)                         # cÃ³ kÃ½ tá»± CJK?

    # Chuáº©n bá»‹ n-grams kÃ½ tá»± cho cÃ¡c trÆ°á»ng há»£p cáº§n (CJK hoáº·c cÃ¢u ráº¥t ngáº¯n)
    qgrams: set = set()
    if is_cjk or len(qtok) <= 1:
        for s in (qn1, qn2, _zh_compat(qn1), _zh_compat(qn2)):
            qgrams |= _char_ngrams(s, 2)

    # Duyá»‡t cÃ¡c hit (giá»›i háº¡n Ä‘á»ƒ trÃ¡nh tá»‘n CPU)
    for d in hits[:TITLE_MAX_CHECK]:
        title = (d.get("title") or "").strip()
        if not title:
            continue

        t1, t2 = _norm_both(title)

        # ---- 1) So theo tá»« (ngÃ´n ngá»¯ cÃ³ khoáº£ng tráº¯ng)
        matched = sum(1 for w in qtok if (w in t1) or (w in t2))
        ok_words = False
        if qtok:
            cond_min_words = (len(qtok) >= min_words and matched >= min_words)
            cond_cover = (matched / max(1, len(qtok))) >= min_cover
            ok_words = cond_min_words or cond_cover

        # ---- 2) So theo kÃ½ tá»± (CJK / cÃ¢u ráº¥t ngáº¯n / khÃ´ng khoáº£ng tráº¯ng)
        ok_cjk = False
        if qgrams:
            # thÃªm báº£n TWâ†’CN cá»§a tiÃªu Ä‘á» vÃ o n-grams
            t1c, t2c = _zh_compat(t1), _zh_compat(t2)
            tgrams = _char_ngrams(t1, 2) | _char_ngrams(t2, 2) | _char_ngrams(t1c, 2) | _char_ngrams(t2c, 2)
            if tgrams:
                cover = len(qgrams & tgrams) / max(1, len(qgrams))
                th = TITLE_CJK_MIN_COVER
                # ná»›i ngÆ°á»¡ng cho truy váº¥n cá»±c ngáº¯n
                if len(qgrams) <= 3:
                    th = max(0.18, th - 0.08)
                ok_cjk = cover >= th

        if ok_words or ok_cjk:
            return True

    return False

# (Alias tÆ°Æ¡ng thÃ­ch)
_ = _has_title_overlap
# ========= BOOTSTRAP =========

APP_SECRET = os.getenv("FB_APP_SECRET", "")

# Cho phÃ©p táº¡m táº¯t verify chá»¯ kÃ½ khi test ná»™i bá»™ (Ä‘áº·t trong .env: DISABLE_FB_SIG_VERIFY=true)
DISABLE_FB_SIG_VERIFY = os.getenv("DISABLE_FB_SIG_VERIFY", "false").lower() == "true"


def _verify_fb_sig(req) -> bool:
    """
    Há»— trá»£ cáº£ X-Hub-Signature-256 (sha256) vÃ  X-Hub-Signature (sha1).
    """
    if DISABLE_FB_SIG_VERIFY:
        return True  # chá»‰ dÃ¹ng khi test

    sig256 = req.headers.get("X-Hub-Signature-256", "")
    sig1   = req.headers.get("X-Hub-Signature", "")

    raw = req.get_data()
    if APP_SECRET and sig256.startswith("sha256="):
        digest = hmac.new(APP_SECRET.encode(), raw, hashlib.sha256).hexdigest()
        return hmac.compare_digest("sha256=" + digest, sig256)

    if APP_SECRET and sig1.startswith("sha1="):
        digest = hmac.new(APP_SECRET.encode(), raw, hashlib.sha1).hexdigest()
        return hmac.compare_digest("sha1=" + digest, sig1)

    return False
OPENAI_API_KEY   = os.getenv("OPENAI_API_KEY", "")
VERIFY_TOKEN     = os.getenv("VERIFY_TOKEN", "aloha_verify_123")
# ---- Multi-page map ----
# ---- Multi-id -> token (FB Page & IG) ----
TOKEN_MAP = {}

def _add_map(key, token):
    if key and token:
        TOKEN_MAP[str(key)] = token

for i in range(1, 11):
    pid = os.getenv(f"FB_PAGE_ID_{i}")
    ptk = os.getenv(f"FB_PAGE_TOKEN_{i}")
    _add_map(pid, ptk)  # map Page ID -> Page token

    igid = os.getenv(f"IG_ACCOUNT_ID_{i}")
    _add_map(igid, ptk) # map IG Account ID -> dÃ¹ng CHUNG Page token Ä‘Ã£ liÃªn káº¿t IG
print("TOKEN_MAP size:", len(TOKEN_MAP))


VECTOR_DIR       = os.getenv("VECTOR_DIR", "./vectors")
# Shopify
SHOPIFY_SHOP = os.getenv("SHOPIFY_STORE", "")  # domain *.myshopify.com (tham chiáº¿u)
# Link shop máº·c Ä‘á»‹nh (fallback)
SHOP_URL         = os.getenv("SHOP_URL", "https://shop.aloha.id.vn/zh")
# Äa ngÃ´n ngá»¯
SUPPORTED_LANGS  = [s.strip() for s in os.getenv("SUPPORTED_LANGS", "vi,en,zh,th,id").split(",")]
DEFAULT_LANG     = os.getenv("DEFAULT_LANG", "vi")
SHOP_URL_MAP = {
    "vi": os.getenv("SHOP_URL_VI", SHOP_URL),
    "en": os.getenv("SHOP_URL_EN", SHOP_URL),
    "zh": os.getenv("SHOP_URL_ZH", SHOP_URL),
    "th": os.getenv("SHOP_URL_TH", SHOP_URL),
    "id": os.getenv("SHOP_URL_ID", SHOP_URL),
}
# --- Always-answer & shop identity ---
ALWAYS_ANSWER = os.getenv("ALWAYS_ANSWER", "true").lower() == "true"
SHOP_NAME = os.getenv("SHOP_NAME", "Aloha")
SHOP_BRAND_TAGLINE = os.getenv("SHOP_BRAND_TAGLINE", "Cá»­a hÃ ng phá»¥ kiá»‡n & lifestyle")

def shop_identity(lang: str):
    url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
    return (
        f"SHOP_IDENTITY:\n"
        f"- TÃªn/Brand: {SHOP_NAME}\n"
        f"- Tagline: {SHOP_BRAND_TAGLINE}\n"
        f"- Website: {url}\n"
        f"- NgÃ´n ngá»¯ há»— trá»£: {', '.join(SUPPORTED_LANGS)}\n"
        f"- LÆ°u Ã½: Chá»‰ nÃªu GIÃ/Tá»’N KHO khi cÃ³ trong CONTEXT; náº¿u khÃ´ng cÃ³ dá»¯ kiá»‡n thÃ¬ xin thÃªm thÃ´ng tin hoáº·c dáº«n link.\n"
    )


REPHRASE_ENABLED = os.getenv("REPHRASE_ENABLED", "true").lower() == "true"
EMOJI_MODE       = os.getenv("EMOJI_MODE", "cute")  # "cute" | "none"

# Lá»c & ngÆ°á»¡ng Ä‘iá»ƒm
SCORE_MIN = float(os.getenv("PRODUCT_SCORE_MIN", "0.26"))
STRICT_MATCH = os.getenv("STRICT_MATCH", "true").lower() == "true"

print("=== BOOT ===")
print("VECTOR_DIR:", os.path.abspath(VECTOR_DIR))
print("TOKEN_MAP size:", len(TOKEN_MAP))
print("OPENAI key set:", bool(OPENAI_API_KEY))




# OpenAI
OPENAI_URL   = "https://api.openai.com/v1/responses"
OPENAI_MODEL = "gpt-4o-mini"
EMBED_MODEL  = os.getenv("EMBED_MODEL", "text-embedding-3-small")

oai_client   = OpenAI(api_key=OPENAI_API_KEY)

# ========= SMALL IN-MEMORY SESSION =========
SESS = {}
SESS_LOCK = threading.RLock()
SESSION_TTL = 60 * 30  # 30 phÃºt
SESS_MAX = int(os.getenv("SESS_MAX", "2000"))

def _purge_sessions():
    now = time.time()
    # xoÃ¡ háº¿t session háº¿t háº¡n
    expired = [k for k,v in SESS.items() if now - v.get("ts",0) > SESSION_TTL]
    for k in expired:
        SESS.pop(k, None)
    # náº¿u váº«n vÆ°á»£t quÃ¡ SESS_MAX â†’ LRU trim
    if len(SESS) > SESS_MAX:
        extra = len(SESS) - SESS_MAX
        for k,_ in sorted(SESS.items(), key=lambda kv: kv[1].get("ts",0))[:extra]:
            SESS.pop(k, None)

def _get_sess(user_id):
    now = time.time()
    with SESS_LOCK:
        s = SESS.get(user_id)
        if not s or now - s["ts"] > SESSION_TTL:
            s = {"hist": deque(maxlen=8), "last_mid": None, "ts": now}
            SESS[user_id] = s
        s["ts"] = now
        # chá»‰ purge khi cáº§n Ä‘á»ƒ tiáº¿t kiá»‡m CPU
        if len(SESS) > SESS_MAX:
            _purge_sessions()
        return s
    
def _remember(user_id, role, text):
    with SESS_LOCK:
        s = _get_sess(user_id)
        s["hist"].append({"role": role, "content": text})




# ========= OPENAI WRAPPER =========
def _to_chat_messages(messages):
    """Chuyá»ƒn format responses -> chat.completions Ä‘á»ƒ fallback."""
    chat_msgs = []
    ALLOWED = {"input_text", "output_text", "text"}  # <-- thÃªm output_text
    for m in messages:
        role = m.get("role", "user")
        parts = m.get("content", [])
        text = "\n".join([p.get("text","") for p in parts if p.get("type") in ALLOWED]).strip()
        chat_msgs.append({"role": role, "content": text})
    return chat_msgs


def call_openai(messages, temperature=0.7):
    """
    Æ¯u tiÃªn /v1/responses; náº¿u lá»—i -> fallback /v1/chat/completions.
    messages: [{"role":..., "content":[{"type":"input_text","text":"..."}]}]
    """
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": OPENAI_MODEL, "input": messages, "temperature": temperature}
    try:
        t0 = time.time()
        r = requests.post(OPENAI_URL, headers=headers, json=payload, timeout=40)
        dt = (time.time() - t0) * 1000
        print(f"ğŸ” OpenAI responses status={r.status_code} in {dt:.0f}ms")
        if r.status_code == 200:
            data = r.json()
            try:
                reply = data["output"][0]["content"][0]["text"]
            except Exception:
                reply = data.get("output_text") or "MÃ¬nh Ä‘ang á»Ÿ Ä‘Ã¢y, sáºµn sÃ ng há»— trá»£ báº¡n!"
            return data, reply

        print(f"âŒ responses body: {r.text[:800]}")
        # Fallback sang chat.completions
        chat_msgs = _to_chat_messages(messages)
        rc = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": chat_msgs, "temperature": temperature},
            timeout=40
        )
        print(f"ğŸ” OpenAI chat.status={rc.status_code}")
        if rc.status_code == 200:
            data = rc.json()
            reply = (data.get("choices") or [{}])[0].get("message", {}).get("content") or "..."
            return data, reply

        print(f"âŒ chat body: {rc.text[:800]}")
        return {}, "Xin lá»—i, hiá»‡n mÃ¬nh gáº·p chÃºt trá»¥c tráº·c. Báº¡n nháº¯n láº¡i giÃºp mÃ¬nh nhÃ©!"
    except Exception as e:
        print("âŒ OpenAI error:", repr(e))
        return {}, "Xin lá»—i, hiá»‡n mÃ¬nh gáº·p chÃºt trá»¥c tráº·c. Báº¡n nháº¯n láº¡i giÃºp mÃ¬nh nhÃ©!"

# === Rephrase má»m + emoji cute ===
EMOJI_SETS = {
    "generic": ["âœ¨","ğŸ™‚","ğŸ˜Š","ğŸŒŸ","ğŸ’«"],
    "greet":   ["ğŸ‘‹","ğŸ˜Š","ğŸ™‚","âœ¨"],
    "browse":  ["ğŸ›ï¸","ğŸ§­","ğŸ”","âœ¨"],
    "product": ["ğŸ›ï¸","âœ¨","ğŸ‘","ğŸ’–"],
    "oos":     ["ğŸ™","â›”","ğŸ˜…","ğŸ›’"],
    "policy":  ["â„¹ï¸","ğŸ“¦","ğŸ›¡ï¸","âœ…"]
}
def em(intent="generic", n=1):
    if EMOJI_MODE == "none": return ""
    arr = EMOJI_SETS.get(intent, EMOJI_SETS["generic"])
    return " " + " ".join(random.choice(arr) for _ in range(max(1, n))).strip()

def rephrase_casual(text: str, intent="generic", temperature=0.7, lang: str = None) -> str:
    """LÃ m má»m cÃ¢u + thÃªm 1â€“2 emoji nháº¹ nhÃ ng, Ä‘Ãºng ngÃ´n ngá»¯ lang."""
    if not REPHRASE_ENABLED:
        return text + (em(intent,1) if intent!="generic" else "")
    try:
        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
        msgs = [
            {"role":"system","content":f"Báº¡n lÃ  trá»£ lÃ½ bÃ¡n hÃ ng, viáº¿t {lang or 'vi'} tá»± nhiÃªn, thÃ¢n thiá»‡n, ngáº¯n gá»n; thÃªm 1â€“2 emoji phÃ¹ há»£p (khÃ´ng láº¡m dá»¥ng). Giá»¯ nguyÃªn dá»¯ kiá»‡n/giÃ¡, khÃ´ng bá»‹a."},
            {"role":"user","content": f"Viáº¿t láº¡i Ä‘oáº¡n sau báº±ng {lang or 'vi'} theo giá»ng thÃ¢n thiá»‡n, káº¿t thÃºc báº±ng 1 cÃ¢u chá»‘t hÃ nh Ä‘á»™ng.\n---\n{text}\n---\n{em(intent,2)}"}
        ]
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": msgs, "temperature": temperature, "max_tokens": 220},
            timeout=20
        )
        if r.status_code == 200:
            data = r.json()
            out = (data.get("choices") or [{}])[0].get("message", {}).get("content")
            return out.strip() if out else text
        else:
            print("âš ï¸ rephrase status=", r.status_code, r.text[:200])
            return text + em(intent,1)
    except Exception as e:
        print("âš ï¸ rephrase error:", repr(e))
        return text + em(intent,1)
def handle_smalltalk(text: str, lang: str = "vi") -> str:
    # tráº£ lá»i ngáº¯n gá»n, khÃ´ng gá»i rephrase Ä‘á»ƒ trÃ¡nh thÃªm CTA bÃ¡n hÃ ng
    raw = f"{t(lang, 'smalltalk_hi')} {t(lang, 'smalltalk_askback')}".strip()
    return raw


# ========= FACEBOOK SENDER =========
def fb_call(path, payload=None, method="POST", params=None, page_token=None):
    if not page_token:
        print("âŒ missing page_token for fb_call")
        return None
    url = f"https://graph.facebook.com/v19.0{path}"
    params = params or {}
    params["access_token"] = page_token

    # appsecret_proof (recommended / required náº¿u báº­t)
    if APP_SECRET:
        try:
            proof = hmac.new(APP_SECRET.encode(), page_token.encode(), hashlib.sha256).hexdigest()
            params["appsecret_proof"] = proof
        except Exception as e:
            print("âš ï¸ cannot compute appsecret_proof:", repr(e))

    try:
        r = requests.request(method, url, params=params, json=payload, timeout=15)
        return r
    except Exception as e:
        print("âš ï¸ FB API error:", repr(e))
        return None

def fb_mark_seen(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"mark_seen"}, page_token=page_token)

def fb_typing_on(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"typing_on"}, page_token=page_token)

def fb_send_text(user_id, text, page_token):
    if not page_token:
        print("âŒ missing page_token for fb_send_text")
        return
    msg = (text or "").strip()
    if len(msg) > 1900:  # Messenger khuyáº¿n nghá»‹ <= ~2000 kÃ½ tá»±
        msg = msg[:1900] + "â€¦"
    payload = {
        "recipient": {"id": user_id},
        "messaging_type": "RESPONSE",
        "message": {"text": msg}
    }
    r = fb_call("/me/messages", payload, page_token=page_token)
    print(f"ğŸ“© Send text status={getattr(r,'status_code',None)}")

def fb_send_buttons(user_id, text, buttons, page_token):
    if not buttons: return
    payload = {
        "recipient": {"id": user_id},
        "messaging_type": "RESPONSE",
        "message": {
            "attachment": {
                "type": "template",
                "payload": {"template_type": "button", "text": text, "buttons": buttons[:3]}  # â† cho phÃ©p 3 nÃºt
            }
        }
    }
    r = fb_call("/me/messages", payload, page_token=page_token)
    print(f"ğŸ”˜ ButtonAPI status={getattr(r,'status_code',None)} body={getattr(r,'text','')[:400]}")


# === Reload vectors (FAISS) ===
CANONICAL_DOMAIN = os.getenv("CANONICAL_DOMAIN", SHOP_URL).rstrip("/")
ALIAS_DOMAINS = [d.strip().rstrip("/") for d in os.getenv("ALIAS_DOMAINS","").split(",") if d.strip()]

def _canon_url(u: str) -> str:
    if not u: return u
    uu = u.strip()
    for dom in ALIAS_DOMAINS:
        if uu.startswith(dom + "/") or uu == dom:
            return CANONICAL_DOMAIN + uu[len(dom):]
    return uu

def _apply_canonical_urls(meta):
    if not meta: return meta
    for d in meta:
        if d.get("url"):
            d["url"] = _canon_url(d["url"])
    return meta

# ========= RAG (FAISS) =========

def _safe_read_index(prefix):
    try:
        idx_path  = os.path.join(VECTOR_DIR, f"{prefix}.index")
        meta_path = os.path.join(VECTOR_DIR, f"{prefix}.meta.json")
        if not (os.path.exists(idx_path) and os.path.exists(meta_path)):
            print(f"âš ï¸ Missing index/meta for '{prefix}'")
            return None, None
        idx  = faiss.read_index(idx_path)
        meta = json.load(open(meta_path, encoding="utf-8"))

        # --- Ãp canonical domain cho má»i URL trong meta ---
        meta = _apply_canonical_urls(meta)

        print(f"âœ… {prefix} loaded: {len(meta)} chunks")
        return idx, meta
    except Exception as e:
        print(f"âŒ Load index '{prefix}':", repr(e))
        return None, None
    
IDX_PROD, META_PROD = _safe_read_index("products")
IDX_POL,  META_POL  = _safe_read_index("policies")

def _reload_vectors():
    global IDX_PROD, META_PROD, IDX_POL, META_POL
    try:
        IDX_PROD, META_PROD = _safe_read_index("products")
        IDX_POL,  META_POL  = _safe_read_index("policies")
        ok = (IDX_PROD is not None or IDX_POL is not None)
        print("ğŸ”„ Reload vectors:", ok,
              "| prod_chunks=", (len(META_PROD) if META_PROD else 0),
              "| policy_chunks=", (len(META_POL) if META_POL else 0))
        return ok
    except Exception as e:
        print("âŒ reload vectors:", repr(e))
        return False

ADMIN_TOKEN = os.getenv("ADMIN_TOKEN", "")

@app.post("/admin/reload_vectors")
def admin_reload_vectors():
    if ADMIN_TOKEN and request.headers.get("X-Admin-Token") != ADMIN_TOKEN:
        return jsonify({"ok": False, "error": "unauthorized"}), 401
    ok = _reload_vectors()
    return jsonify({"ok": ok})


def _embed_query(q: str) -> Optional[np.ndarray]:
    try:
        t0 = time.time()
        resp = oai_client.embeddings.create(model=EMBED_MODEL, input=[q])
        v = np.array(resp.data[0].embedding, dtype="float32")[None, :]
        faiss.normalize_L2(v)
        print(f"ğŸ§© Embedding in {(time.time()-t0)*1000:.0f}ms")
        return v
    except Exception as e:
        print("âš ï¸ _embed_query error:", repr(e))
        return None



def search_products_with_scores(query, topk=8):
    if IDX_PROD is None:
        return [], []

    v = _embed_query(query)
    if v is None:  # >>> thÃªm dÃ²ng an toÃ n
        return [], []

    try:
        D, I = IDX_PROD.search(v, topk)
        hits, scores, seen = [], [], set()
        for idx, score in zip(I[0], D[0]):
            if 0 <= idx < len(META_PROD):
                d = META_PROD[idx]
                key = (d.get("url"), (d.get("title") or "").lower().strip())
                if key in seen:
                    continue
                seen.add(key)
                hits.append(d)
                scores.append(float(score))
        print(f"ğŸ“š product hits: {len(hits)}")
        return hits, scores
    except Exception as e:
        print("âš ï¸ search_products_with_scores:", repr(e))
        return [], []
def retrieve_context(question, topk=6):
    if IDX_PROD is None and IDX_POL is None:
        return ""

    v = _embed_query(question)
    if v is None:  # >>> thÃªm dÃ²ng an toÃ n
        return ""

    ctx = []
    if IDX_PROD is not None:
        try:
            _, Ip = IDX_PROD.search(v, topk)
            ctx += [META_PROD[i]["text"] for i in Ip[0] if i >= 0]
        except Exception as e:
            print("âš ï¸ search products:", repr(e))
    if IDX_POL is not None:
        try:
            _, Ik = IDX_POL.search(v, topk)
            ctx += [META_POL[i]["text"] for i in Ik[0] if i >= 0]
        except Exception as e:
            print("âš ï¸ search policies:", repr(e))
    print("ğŸ§  ctx pieces:", len(ctx))
    return "\n\n".join(ctx[:topk]) if ctx else ""
def _parse_ts(s):
    try:
        s = (s or "").replace("Z", "").replace("T", " ")
        return time.mktime(time.strptime(s[:19], "%Y-%m-%d %H:%M:%S"))
    except Exception:
        return 0

def get_new_arrivals(days=30, topk=4):
    """TÃ¬m sp má»›i theo timestamp/tags 'new|má»›i|vá»«a vá»'; fallback FAISS náº¿u trá»‘ng."""
    if not META_PROD:
        return []
    now = time.time()
    cutoff = now - days*86400
    new_items = []
    for d in META_PROD:
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            if d.get(k):
                ts = max(ts, _parse_ts(d.get(k)))
        tags = (d.get("tags","") or "").lower()
        flag_new = any(x in tags for x in ["new","má»›i","vá»«a vá»","new arrivals"])
        if flag_new or (ts and ts >= cutoff):
            new_items.append(d)

    if not new_items and IDX_PROD is not None:
        hits, _ = search_products_with_scores("new arrivals hÃ ng má»›i vá»«a vá»", topk=topk*2)
        new_items = hits

    def _key(d):
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            ts = max(ts, _parse_ts(d.get(k)))
        return ts
    new_items.sort(key=_key, reverse=True)
    return new_items[:topk]

def compose_new_arrivals(lang: str = "vi", items=None):
    items = items or []
    if not items:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return rephrase_casual(t(lang, "browse", url=url), intent="browse", lang=lang)

    lines = []
    for d in items[:2]:
        title = d.get("title") or "Sáº£n pháº©m"
        stock = _stock_line(d)
        pval  = _price_value(d)
        currency = d.get("currency") or ("â‚«" if lang == "vi" else "")
        pstr  = _fmt_price(pval, currency) if pval is not None else None

        line = f"â€¢ {title}"
        if pstr: line += f" â€” {pstr}"
        line += f" â€” {stock}"
        lines.append(line)

    raw = f"{t(lang,'new_hdr')}\n" + "\n".join(lines) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)



# ========= INTENT, PERSONA, FEW-SHOT & NATURAL REPLY =========
GREETS = {"hi","hello","hey","helo","heloo","hÃ­","hÃ¬","chÃ o","xin chÃ o","alo","aloha","hello bot","hi bot"}
def is_greeting(text: str) -> bool:
    t = re.sub(r"\s+", " ", (text or "").lower()).strip()
    return any(w in t for w in GREETS) and len(t) <= 40

# â€”â€”â€” NgÃ´n ngá»¯: detect & cÃ¢u chá»¯
def detect_lang(text: str) -> str:
    txt = (text or "").strip()
    if not txt: return DEFAULT_LANG
    if re.search(r"[\u4e00-\u9fff]", txt):  # CJK
        return "zh" if "zh" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[\u0E00-\u0E7F]", txt):  # Thai
        return "th" if "th" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[ÄƒÃ¢ÃªÃ´Æ¡Æ°Ä‘Ã¡Ã áº£Ã£áº¡áº¯áº±áº³áºµáº·áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹áº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»á»‘á»“á»•á»—á»™Æ¡Ã³á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ]", txt, flags=re.I):
        return "vi" if "vi" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"\b(yang|dan|tidak|saja|terima|kasih)\b", txt.lower()):
        return "id" if "id" in SUPPORTED_LANGS else DEFAULT_LANG
    return "en" if "en" in SUPPORTED_LANGS else DEFAULT_LANG


# ====== MULTI-LANG PATTERNS (smalltalk & new arrivals) ======
# Má»—i ngÃ´n ngá»¯ lÃ  1 list regex. CÃ³ thá»ƒ bá»• sung dáº§n mÃ  khÃ´ng Ä‘á»¥ng chá»— khÃ¡c.
# ==== Smalltalk & New arrivals (multi-lang) ====

# ========= I18N STRINGS & HELPERS =========
LANG_STRINGS = {
    "vi": {
        "greet": "Xin chÃ o ğŸ‘‹ Ráº¥t vui Ä‘Æ°á»£c phá»¥c vá»¥ báº¡n! Báº¡n muá»‘n mÃ¬nh giÃºp gÃ¬ khÃ´ng nÃ¨? ğŸ™‚",
        "browse": "Má»i báº¡n vÃ o web tham quan áº¡ ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "Xin lá»—i ğŸ™ sáº£n pháº©m Ä‘Ã³ hiá»‡n **Ä‘ang háº¿t hÃ ng** táº¡i shop. Báº¡n thá»­ xem cÃ¡c máº«u tÆ°Æ¡ng tá»± trÃªn web nhÃ© ğŸ‘‰ {url} âœ¨",
        "fallback": "MÃ¬nh chÆ°a Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ cháº¯c cháº¯n ğŸ¤”. Báº¡n mÃ´ táº£ thÃªm máº«u/kiá»ƒu dÃ¡ng/cháº¥t liá»‡u Ä‘á»ƒ mÃ¬nh tÆ° váº¥n chuáº©n hÆ¡n nha âœ¨",
        "suggest_hdr": "MÃ¬nh Ä‘á» xuáº¥t vÃ i lá»±a chá»n phÃ¹ há»£p",
        "product_pts": "Báº¡n thÃ­ch kiá»ƒu máº£nh hay thá»ƒ thao? MÃ¬nh lá»c thÃªm mÃ u & size giÃºp báº¡n nhÃ©.",
        "highlights": "{title} cÃ³ vÃ i Ä‘iá»ƒm ná»•i báº­t nÃ¨",
        "policy_hint": "Theo chÃ­nh sÃ¡ch shop:",
        "smalltalk_hi": "Hi ğŸ‘‹ MÃ¬nh khá»e nÃ¨ ğŸ˜„",
        "smalltalk_askback": "HÃ´m nay cá»§a báº¡n tháº¿ nÃ o?",
        "new_hdr": "HÃ ng má»›i vá» nÃ¨ âœ¨",
        "btn_view": "Xem sáº£n pháº©m",
        "quick_view": "Xem nhanh:",

    },
    "en": {
        "greet": "Hello ğŸ‘‹ Happy to help! How can I assist you today? ğŸ™‚",
        "browse": "Feel free to explore our store ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "Sorry ğŸ™ that item is **out of stock** right now. Check similar picks here ğŸ‘‰ {url} âœ¨",
        "fallback": "Iâ€™m missing a bit of info ğŸ¤”. Share style/material/size and Iâ€™ll refine the picks âœ¨",
        "suggest_hdr": "Here are a few good options",
        "product_pts": "Prefer a slim or sporty style? I can filter color & size for you.",
        "highlights": "{title} highlights",
        "policy_hint": "Store policy:",
         "smalltalk_hi": "Hi ğŸ‘‹ I'm good! ğŸ˜„",
        "smalltalk_askback": "How's your day going?",
        "new_hdr": "New arrivals âœ¨",
        "btn_view": "View product",
        "quick_view": "Quick view:",
    },
    "zh": {
        "greet": "ä½ å¥½ ğŸ‘‹ å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼éœ€è¦æˆ‘å¸®ä½ åšä»€ä¹ˆå‘¢ï¼ŸğŸ™‚",
        "browse": "æ¬¢è¿é€›é€›æˆ‘ä»¬çš„å•†åº— ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "æŠ±æ­‰ ğŸ™ è¯¥å•†å“ç›®å‰**ç¼ºè´§**ã€‚å¯ä»¥å…ˆçœ‹çœ‹ç±»ä¼¼çš„æ¬¾å¼ ğŸ‘‰ {url} âœ¨",
        "fallback": "è¿˜éœ€è¦ä¸€äº›ä¿¡æ¯å“¦ ğŸ¤”ã€‚è¯´ä¸‹é£æ ¼/æè´¨/å°ºå¯¸ï¼Œæˆ‘å†ç²¾å‡†æ¨è âœ¨",
        "suggest_hdr": "ç»™ä½ å‡ æ¬¾åˆé€‚çš„é€‰æ‹©",
        "product_pts": "æƒ³è¦çº¤ç»†è¿˜æ˜¯è¿åŠ¨é£ï¼Ÿæˆ‘å¯ä»¥æŒ‰é¢œè‰²å’Œå°ºç å†ç­›ä¸€è½®ã€‚",
        "highlights": "{title} çš„äº®ç‚¹",
        "policy_hint": "åº—é“ºæ”¿ç­–ï¼š",
        "smalltalk_hi": "å—¨ ğŸ‘‹ æˆ‘å¾ˆå¥½å–” ğŸ˜„",
        "smalltalk_askback": "ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ",
        "new_hdr": "æ–°å“ä¸Šæ¶ âœ¨",
        "btn_view": "æŸ¥çœ‹å•†å“",
        "quick_view": "å¿«é€ŸæŸ¥çœ‹ï¼š",
    },
    "th": {
        "greet": "à¸ªà¸§à¸±à¸ªà¸”à¸µ ğŸ‘‹ à¸¢à¸´à¸™à¸”à¸µà¹ƒà¸«à¹‰à¸šà¸£à¸´à¸à¸²à¸£à¸™à¸°à¸„à¸£à¸±à¸š/à¸„à¹ˆà¸° à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹ƒà¸«à¹‰à¸Šà¹ˆà¸§à¸¢à¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡ ğŸ™‚",
        "browse": "à¹€à¸Šà¸´à¸à¸Šà¸¡à¸ªà¸´à¸™à¸„à¹‰à¸²à¹ƒà¸™à¹€à¸§à¹‡à¸šà¹„à¸”à¹‰à¹€à¸¥à¸¢ ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "à¸‚à¸­à¸­à¸ à¸±à¸¢ ğŸ™ à¸ªà¸´à¸™à¸„à¹‰à¸²à¸Šà¸´à¹‰à¸™à¸™à¸±à¹‰à¸™ **à¸«à¸¡à¸”à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§** à¸„à¹ˆà¸° à¸¥à¸­à¸‡à¸”à¸¹à¸£à¸¸à¹ˆà¸™à¹ƒà¸à¸¥à¹‰à¹€à¸„à¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸™à¸µà¹ˆ ğŸ‘‰ {url} âœ¨",
        "fallback": "à¸‚à¸­à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹€à¸à¸´à¹ˆà¸¡à¸­à¸µà¸à¸™à¸´à¸”à¸™à¸°à¸„à¸°/à¸„à¸£à¸±à¸š à¹€à¸Šà¹ˆà¸™à¸ªà¹„à¸•à¸¥à¹Œ/à¸§à¸±à¸ªà¸”à¸¸/à¸‚à¸™à¸²à¸” âœ¨",
        "suggest_hdr": "à¸‚à¸­à¹à¸™à¸°à¸™à¸³à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡",
        "product_pts": "à¸Šà¸­à¸šà¹à¸šà¸šà¹€à¸à¸£à¸µà¸¢à¸§à¸«à¸£à¸·à¸­à¸ªà¸›à¸­à¸£à¹Œà¸•à¸”à¸µ? à¹€à¸”à¸µà¹‹à¸¢à¸§à¸Šà¹ˆà¸§à¸¢à¸„à¸±à¸”à¸ªà¸µà¹à¸¥à¸°à¹„à¸‹à¸‹à¹Œà¹ƒà¸«à¹‰à¸­à¸µà¸à¹„à¸”à¹‰à¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š",
        "highlights": "à¸ˆà¸¸à¸”à¹€à¸”à¹ˆà¸™à¸‚à¸­à¸‡ {title}",
        "policy_hint": "à¸™à¹‚à¸¢à¸šà¸²à¸¢à¸£à¹‰à¸²à¸™:",
         "smalltalk_hi": "à¹„à¸® ğŸ‘‹ à¸ªà¸šà¸²à¸¢à¸”à¸µà¸¡à¸²à¸à¹€à¸¥à¸¢à¸™à¸° ğŸ˜„",
        "smalltalk_askback": "à¸§à¸±à¸™à¸™à¸µà¹‰à¸‚à¸­à¸‡à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™à¸¢à¸±à¸‡à¹„à¸‡à¸šà¹‰à¸²à¸‡?",
        "new_hdr": "à¸ªà¸´à¸™à¸„à¹‰à¸²à¹€à¸‚à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆ âœ¨",
        "btn_view": "à¸”à¸¹à¸ªà¸´à¸™à¸„à¹‰à¸²",
        "quick_view": "à¸”à¸¹à¸”à¹ˆà¸§à¸™:",
    },
    "id": {
        "greet": "Halo ğŸ‘‹ Senang membantu! Ada yang bisa saya bantu? ğŸ™‚",
        "browse": "Silakan jelajahi toko kami ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "Maaf ğŸ™ produk itu **sedang kosong**. Coba lihat yang mirip di sini ğŸ‘‰ {url} âœ¨",
        "fallback": "Butuh info tambahan ğŸ¤”. Sebutkan gaya/bahan/ukuran ya, biar saya saringkan âœ¨",
        "suggest_hdr": "Beberapa pilihan yang cocok",
        "product_pts": "Suka model tipis atau sporty? Saya bisa saring warna & ukuran.",
        "highlights": "Hal menarik dari {title}",
        "policy_hint": "Kebijakan toko:",
       "smalltalk_hi": "Hai ğŸ‘‹ Aku baik-baik saja ğŸ˜„",
        "smalltalk_askback": "Harinya kamu gimana?",
        "new_hdr": "Produk baru âœ¨",
        "btn_view": "Lihat produk",
        "quick_view": "Lihat cepat:",
    },
}


def t(lang: str, key: str, **kw) -> str:
    lang2 = lang if lang in LANG_STRINGS else DEFAULT_LANG
    s = (LANG_STRINGS.get(lang2, {}).get(key)
         or LANG_STRINGS.get(DEFAULT_LANG, {}).get(key)
         or "")
    try:
        return s.format(**kw)
    except Exception:
        return s


def greet_text(lang: str) -> str:
    return t(lang, "greet")

# ==== Smalltalk & New arrivals (multi-lang) ====

SMALLTALK_PATTERNS = {
    "vi": [
        # bn/báº¡n khá»e/khá»e/khoáº» khÃ´ng/ko/hÃ´ng/hem/hok
        r"\b(bn|báº¡n)\s*(kh[oÃ³Ã²á»Ãµá»Æ¡á»›á»á»Ÿá»¡á»£]e|khoe|khoáº»|khá»e)\s*(kh[oÃ´]ng|ko|k|h[oÆ¡Ã´Ã³Ã²Ãµá»á»]ng|hong|hÃ´ng|hem|hok)\b",
        r"\b(kh[oÃ³Ã²á»Ãµá»Æ¡á»›á»á»Ÿá»¡á»£]e|khoe|khoáº»|khá»e)\s*(kh[oÃ´]ng|ko|k|h[oÆ¡Ã´Ã³Ã²Ãµá»á»]ng|hong|hÃ´ng|hem|hok)\b",
        # á»•n khÃ´ng
        r"\b(á»•n|on)\s*(kh[oÃ´]ng|ko|k|hong|h[oÆ¡]ng|hem|hok)\b",
        # hÃ´m nay tháº¿ nÃ o / nay sao
        r"\b(h[oÃ´]m?\s*nay|nay)\s*(báº¡n|bn)?\s*(th[áº¿e]\s*n[aÃ ]o|sao|ok\s*kh[oÃ´]ng)\b",
        # Ä‘ang lÃ m gÃ¬ / dáº¡o nÃ y
        r"\b(Ä‘ang\s*lÃ m\s*gÃ¬|lÃ m\s*gÃ¬( váº­y| Ä‘Ã³)?|lÃ m\s*chi|lam\s*gi)\b",
        r"\b(dáº¡o\s*nÃ y|dao\s*nay)\b",
        # Äƒn cÆ¡m chÆ°a / ngá»§ chÆ°a
        r"\b(Äƒn\s*cÆ¡m\s*chÆ°a|Äƒn\s*chÆ°a|an\s*chua|uá»‘ng\s*chÆ°a|ng[uÆ°]\s*ch[aÄƒ]u?)\b",
        # cáº£m Æ¡n / thanks
        r"\b(c[áº£a]m\s?Æ¡n|c[Ã¡a]m\s?Æ¡n|thanks?|thank you|ty|tks|thx)\b",
        # cÆ°á»i/emoji
        r"\b(haha+|hihi+|hehe+|kkk+|=D|:d|:v|:3)\b|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "en": [
        r"\b(how('?s)?\s*it\s*going|how\s*are\s*(you|u)|how\s*r\s*u|how\s*u\s*doin?g?)\b",
        r"\b(what('?s)?\s*up|wass?up|sup|wyd)\b",
        r"\b(have\s*you\s*eaten|had\s*(lunch|dinner)|grabbed\s*(lunch|food))\b",
        r"\b(thanks?|thank\s*(you|u)|ty|thx|tysm|tks)\b",
        r"\b(lol|lmao|rofl|haha+|hehe+|:d)\b|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "zh": [
        r"(ä½ å¥½å—|å¦³å¥½å—|æœ€è¿‘æ€ä¹ˆæ ·|æœ€è¿‘å¦‚ä½•|æœ€è¿‘è¿˜å¥½|è¿˜å¥½å—|å¿ƒæƒ…å¦‚ä½•|å¼€å¿ƒå—|éå¾—æ€æ¨£|è¿‡å¾—æ€æ ·)",
        r"(åƒé¥­äº†å—|åƒè¿‡é¥­æ²¡|åƒäº†æ²¡|åƒäº†å—)",
        r"(è°¢è°¢|å¤šè°¢|è¬è¬|æ„Ÿè¬|æ„Ÿè°¢|è¬å•¦|è°¢è°¢å•¦|è°¢å•¦)",
        r"(å“ˆå“ˆ+|å˜¿å˜¿+|å‘µå‘µ+|å—¨å—¨+)|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "th": [
        r"(à¸ªà¸šà¸²à¸¢à¸”à¸µ(à¹„à¸«à¸¡|à¸¡à¸±à¹‰à¸¢|à¸›à¹ˆà¸²à¸§)|à¹€à¸›à¹‡à¸™(à¹„à¸‡|à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£)à¸šà¹‰à¸²à¸‡|à¹‚à¸­à¹€à¸„(à¹„à¸«à¸¡|à¸¡à¸±à¹‰à¸¢))",
        r"(à¸—à¸³à¸­à¸°à¹„à¸£à¸­à¸¢à¸¹à¹ˆ|à¸à¸³à¸¥à¸±à¸‡à¸—à¸³à¸­à¸°à¹„à¸£|à¸—à¸³à¹„à¸£à¸­à¸¢à¸¹à¹ˆ)",
        r"(à¸à¸´à¸™à¸‚à¹‰à¸²à¸§(à¸«à¸£à¸·à¸­)?à¸¢à¸±à¸‡|à¸—à¸²à¸™à¸‚à¹‰à¸²à¸§(à¸«à¸£à¸·à¸­)?à¸¢à¸±à¸‡)",
        r"(à¸‚à¸­à¸šà¸„à¸¸à¸“(à¸„à¸£à¸±à¸š|à¸„à¹ˆà¸°)?|à¸‚à¸­à¸šà¹ƒà¸ˆ|thanks?|thank you|ty)",
        r"(à¸®à¹ˆà¸²+à¹†+|555+)|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "id": [
        r"(apa\s*kabar|gimana\s*kabarnya|gmn\s*kabar|kabarnya\s*gimana)",
        r"(lagi\s*apa|lg\s*apa|sedang\s*apa|ngapain(\s*nih)?)",
        r"(sudah|udah)\s*makan\s*(belum|blm)",
        r"(terima\s*kasih|terimakasih|trimakasih|makasih|makasi|thanks?|thank you|thx|ty)",
        r"(wkwk+|wk+|haha+|hehe+|:d)|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
}

NEW_ITEMS_PATTERNS = {
    "vi": [
        r"(hÃ ng|sp|máº«u|sáº£n\s*pháº©m).*(má»›i|vá»«a\s*vá»|new\s*arrivals)",
        r"(cÃ³|Ä‘Ã£).*(máº«u|sáº£n\s*pháº©m).*(má»›i|vá»«a\s*vá»)",
        r"\b(new|má»›i|vá»«a vá»|new arrivals)\b",
    ],
    "en": [
        r"(new\s*arrivals?|new\s*products?|what's\s*new)",
        r"(any|have).*(new\s*items?)",
    ],
    "zh": [
        r"(æ–°å“|æ–°åˆ°|æ–°è²¨|æ–°è´§)",
        r"(æœ‰.*æ–°(å“|è´§|è²¨)|ä¾†äº†.*æ–°|æ¥äº†.*æ–°)",
    ],
    "th": [
        r"(à¸ªà¸´à¸™à¸„à¹‰à¸²à¹€à¸‚à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆ|à¸‚à¸­à¸‡à¹€à¸‚à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆ|à¸‚à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆ|à¸¡à¸²à¹ƒà¸«à¸¡à¹ˆ)",
        r"(à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¸¡à¹ˆ|à¸¡à¸µà¸ªà¸´à¸™à¸„à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆà¹„à¸«à¸¡)",
    ],
    "id": [
        r"(produk baru|barang baru|baru datang)",
        r"(ada yang baru|ada produk baru)",
    ],
}

def _pat(pats: dict, lang: str):
    """Láº¥y list pattern theo ngÃ´n ngá»¯, fallback vá» DEFAULT_LANG náº¿u khÃ´ng cÃ³."""
    return pats.get(lang) or pats.get(DEFAULT_LANG, [])
# ===== GiÃ¡ / Price questions (multi-lang) =====
PRICE_PATTERNS = {
    "vi": [r"\bgiÃ¡\b", r"bao nhiÃªu", r"nhiÃªu tiá»n", r"\bgiÃ¡ bao nhiÃªu\b", r"\bbao nhieu\b"],
    "en": [r"\bprice\b", r"how much", r"\bcost\b"],
    "zh": [r"(ä»·æ ¼|å¹¾éŒ¢|å¤šå°‘é’±|å¤šå°‘éŒ¢)"],
    "th": [r"(à¸£à¸²à¸„à¸²|à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆ|à¹€à¸—à¹ˆà¸²à¹„à¸£)"],
    "id": [r"(harga|berapa)"],
}
def is_price_question(text: str, lang: str) -> bool:
    raw = (text or "")
    return any(re.search(p, raw, flags=re.I) for p in _pat(PRICE_PATTERNS, lang))


SYSTEM_STYLE = (
    "Báº¡n lÃ  trá»£ lÃ½ bÃ¡n hÃ ng Aloha tÃªn lÃ  Aloha Bot. TÃ´ng giá»ng: thÃ¢n thiá»‡n, chá»§ Ä‘á»™ng, "
    "tráº£ lá»i ngáº¯n gá»n nhÆ° ngÆ°á»i tháº­t; dÃ¹ng 1â€“3 emoji há»£p ngá»¯ cáº£nh (khÃ´ng láº¡m dá»¥ng). "
    "LuÃ´n dá»±a vÃ o CONTEXT (ná»™i dung RAG). KhÃ´ng bá»‹a. "
    "KHÃ”NG Ä‘Æ°á»£c nÃªu giÃ¡/tá»“n kho/thuá»™c tÃ­nh cá»¥ thá»ƒ náº¿u CONTEXT khÃ´ng cÃ³ dá»¯ kiá»‡n; "
    "khi thiáº¿u dá»¯ kiá»‡n thÃ¬ há»i láº¡i 1 cÃ¢u lÃ m rÃµ hoáº·c má»i xem link cá»­a hÃ ng. "
    "TrÃ¬nh bÃ y dá»… Ä‘á»c: gáº¡ch Ä‘áº§u dÃ²ng khi liá»‡t kÃª; 1 cÃ¢u chá»‘t hÃ nh Ä‘á»™ng."
)
# FEW_SHOT_EXAMPLES
FEW_SHOT_EXAMPLES = [
    {"role":"user","content":[{"type":"input_text","text":"helo"}]},
    {"role":"assistant","content":[{"type":"output_text","text":"Xin chÃ o ğŸ‘‹ Ráº¥t vui Ä‘Æ°á»£c phá»¥c vá»¥ báº¡n! Báº¡n muá»‘n mÃ¬nh giÃºp gÃ¬ khÃ´ng nÃ¨? ğŸ™‚"}]},
    {"role":"user","content":[{"type":"input_text","text":"shop báº¡n cÃ³ nhá»¯ng gÃ¬"}]},
    {"role":"assistant","content":[{"type":"output_text","text":f"Má»i báº¡n tham quan cá»­a hÃ ng táº¡i Ä‘Ã¢y áº¡ ğŸ›ï¸ ğŸ‘‰ {SHOP_URL_MAP.get('vi', SHOP_URL)}"}]},
]
# ---- Intent routing ----
POLICY_KEYWORDS  = {"chÃ­nh sÃ¡ch","Ä‘á»•i tráº£","báº£o hÃ nh","ship","váº­n chuyá»ƒn","giao hÃ ng","tráº£ hÃ ng","refund"}
PRODUCT_KEYWORDS = {
    "mua","bÃ¡n","giÃ¡","size","kÃ­ch thÆ°á»›c","cháº¥t liá»‡u","mÃ u","há»£p","phÃ¹ há»£p",
    "dÃ¢y","Ä‘á»“ng há»“","vÃ²ng","case","Ã¡o","quáº§n","Ã¡o phÃ´ng","tshirt","t-shirt","Ã¡o thun",
    "sáº£n pháº©m","sp","bÃ¡nh","crepe","bÃ¡nh crepe","bÃ¡nh sáº§u riÃªng","milktea","trÃ  sá»¯a"
}
BROWSE_KEYWORDS  = {"cÃ³ nhá»¯ng gÃ¬","bÃ¡n gÃ¬","cÃ³ gÃ¬","danh má»¥c","catalog","xem hÃ ng","tham quan","xem shop","xem sáº£n pháº©m","shop cÃ³ gÃ¬","nhá»¯ng sáº£n pháº©m gÃ¬"}
_BROWSE_PATTERNS = [
    r"(shop|bÃªn báº¡n|bÃªn mÃ¬nh).*(bÃ¡n|cÃ³).*(gÃ¬|nhá»¯ng gÃ¬|nhá»¯ng sáº£n pháº©m gÃ¬)",
    r"(bÃ¡n|cÃ³).*(nhá»¯ng\s+)?sáº£n pháº©m gÃ¬",
]
# ==== Smalltalk & New arrivals ====



def detect_intent(text: str):
    raw = (text or "")
    t0  = re.sub(r"\s+", " ", raw.lower()).strip()
    lang = detect_lang(raw)

    if any(k in t0 for k in POLICY_KEYWORDS):  return "policy"
    if is_greeting(raw):                       return "greet"

    # smalltalk Ä‘a ngÃ´n ngá»¯
    if any(re.search(p, raw, flags=re.I) for p in _pat(SMALLTALK_PATTERNS, lang)):
        return "smalltalk"

    # browse: tá»« khÃ³a + pattern chung
    if any(k in t0 for k in BROWSE_KEYWORDS):  return "browse"
    if any(re.search(p, t0, flags=re.I) for p in _BROWSE_PATTERNS):
        return "browse"


    # há»i hÃ ng má»›i Ä‘a ngÃ´n ngá»¯
    if any(re.search(p, raw, flags=re.I) for p in _pat(NEW_ITEMS_PATTERNS, lang)):
        return "new_items"
    # Há»i giÃ¡ â†’ Æ°u tiÃªn product_info
    if is_price_question(raw, lang):
        return "product_info"

    # sáº£n pháº©m & mÃ´ táº£
    if any(k in t0 for k in PRODUCT_KEYWORDS): return "product"
    if "cÃ³ bÃ¡n" in t0 or "bÃ¡n khÃ´ng" in t0 or "bÃ¡n ko" in t0: return "product"
    if "cÃ³ gÃ¬ Ä‘áº·c biá»‡t" in t0 or "Ä‘iá»ƒm Ä‘áº·c biá»‡t" in t0 or "cÃ³ gÃ¬ Ä‘áº·t biá»‡t" in t0: return "product_info"

    return "other"

def build_messages(system, history, context, user_question):
    msgs = [{"role":"system","content":[{"type":"input_text","text":system}]}]
    msgs.extend(FEW_SHOT_EXAMPLES)
    for h in list(history)[-3:]:
        ctype = "output_text" if h["role"] == "assistant" else "input_text"
        msgs.append({"role": h["role"], "content":[{"type": ctype, "text": h["content"]}]})
    user_block = f"(Náº¿u há»¯u Ã­ch thÃ¬ dÃ¹ng CONTEXT)\nCONTEXT:\n{context}\n\nCÃ‚U Há»I: {user_question}"
    msgs.append({"role":"user","content":[{"type":"input_text","text":user_block}]})
    return msgs


# ---- Hiá»ƒn thá»‹ tá»“n kho/OOS + emoji ----
def _stock_line(d: dict) -> str:
    if d.get("available") and (d.get("inventory_quantity") is None or d.get("inventory_quantity", 0) > 0):
        return "cÃ²n hÃ ng âœ…"
    if d.get("inventory_quantity") == 0 or (d.get("status") and d.get("status") != "active"):
        return "háº¿t hÃ ng táº¡m thá»i â›”"
    return "tÃ¬nh tráº¡ng Ä‘ang cáº­p nháº­t â³"

def _shorten(txt: str, n=280) -> str:
    t = (txt or "").strip()
    return (t[:n].rstrip() + "â€¦") if len(t) > n else t
def _fmt_price(p, currency="â‚«"):
    if p is None:
        return None
    try:
        # náº¿u p lÃ  string: chá»‰ giá»¯ chá»¯ sá»‘
        s = re.sub(r"\D", "", str(p))
        if not s:
            return None
        val = int(float(s))
        return f"{val:,.0f}".replace(",", ".") + (f" {currency}" if currency else "")
    except Exception:
        return None

def _extract_price_number(txt: str):
    """Báº¯t 199k / 199.000Ä‘ / 1,299,000 VNDâ€¦ â†’ sá»‘ (float)."""
    if not txt:
        return None
    low = str(txt).lower()
    m = re.search(r"(\d[\d\.\s,]{2,})(?:\s*)(Ä‘|â‚«|vnd|vnÄ‘|k)?\b", low)
    try:
        if m:
            digits = re.sub(r"\D", "", m.group(1))  # chá»‰ chá»¯ sá»‘
            if not digits:
                return None
            v = float(digits)
            return v * 1000 if (m.group(2) == "k") else v
        m2 = re.search(r"\b(\d{4,})\b", low)  # fallback chuá»—i sá»‘ dÃ i
        return float(m2.group(1)) if m2 else None
    except Exception:
        return None


def _price_value(d: dict):
    for k in ("price","min_price","max_price"):
        v = d.get(k)
        if v is not None:
            try:
                digits = re.sub(r"\D", "", str(v))
                return float(digits) if digits else None
            except Exception:
                pass
    return _extract_price_number(d.get("text",""))


def _category_key_from_doc(d: dict):
    """XÃ¡c Ä‘á»‹nh 'dÃ²ng' sáº£n pháº©m Ä‘á»ƒ so minâ€“max: Æ°u tiÃªn product_type; fallback theo synonyms trong title/tags."""
    pt = (d.get("product_type") or "").strip()
    if pt:
        return _normalize_text(pt)
    raw = " ".join([d.get("title",""), d.get("tags","")])
    n1, n2 = _norm_both(raw)
    for key in VN_SYNONYMS.keys():
        k1, k2 = _norm_both(key)
        if k1 in n1 or k2 in n2:
            return _normalize_text(key)
    return "misc"

def _minmax_in_category(base_doc: dict):
    """TÃ¬m 1 máº«u ráº» nháº¥t vÃ  1 máº«u Ä‘áº¯t nháº¥t cÃ¹ng dÃ²ng (loáº¡i). Náº¿u khÃ´ng Ä‘á»§, fallback toÃ n shop."""
    if not META_PROD:
        return None, None
    cat = _category_key_from_doc(base_doc)
    def same_cat(x):
        return _category_key_from_doc(x) == cat
    cands = [x for x in META_PROD if same_cat(x)]
    if len(cands) < 2:
        cands = [x for x in META_PROD]  # fallback toÃ n shop

    items = []
    for x in cands:
        pv = _price_value(x)
        if pv is not None:
            items.append((pv, x))
    if not items:
        return None, None

    # loáº¡i chÃ­nh ra khá»i candidates náº¿u trÃ¹ng URL hoáº·c title
    def is_same(a, b):
        return (a.get("url") and a.get("url")==b.get("url")) or \
               ((a.get("title") or "").strip().lower()==(b.get("title") or "").strip().lower())

    items = [(p, x) for (p, x) in items if not is_same(x, base_doc)]
    if not items:
        return None, None

    items.sort(key=lambda t: t[0])
    low = items[0][1]
    high = items[-1][1]
    return low, high


def _extract_features_from_text(text_block: str):
    lines = []
    m = re.search(r"Specs:\s*(.+)", text_block, flags=re.I)
    if m:
        lines.extend(re.split(r"\s*\|\s*|\s*;\s*|,\s*", m.group(1)))
    m2 = re.search(r"Body:\s*(.+?)\s*URL:", text_block, flags=re.I|re.S)
    if m2:
        body = re.sub(r"\s+", " ", m2.group(1)).strip()
        chunks = re.split(r"(?<=[.!?])\s+", body)
        lines.extend(chunks)
    lines = [re.sub(r"\s+", " ", l).strip("â€¢- \n\t") for l in lines if l and len(l.strip()) > 0]
    uniq = []
    for l in lines:
        if l not in uniq:
            uniq.append(l)
        if len(uniq) >= 5:
            break
    return ["â€¢ " + _shorten(x, 80) for x in uniq[:5]]

# ====== Tá»ª KHÃ“A / Äá»’NG NGHÄ¨A (Ä‘a ngÃ´n ngá»¯) ======
VN_SYNONYMS = {
    # ===== Äá»“ng há»“ & phá»¥ kiá»‡n =====
    "Ä‘á»“ng há»“": [
        "dong ho","dong-ho","dongho","watch","watchface","watch face","bezel",
        "galaxy watch","apple watch","amazfit","seiko","casio","nh35","nh36",
        "automatic","mechanical","chronograph"
    ],
    "dÃ¢y Ä‘á»“ng há»“": [
        "day dong ho","daydongho","watch band","band","strap","nato","loop",
        "bracelet","mesh","leather strap","metal strap","silicone strap"
    ],
    "case Ä‘á»“ng há»“": [
        "case dong ho","vo dong ho","bao ve dong ho","bezel protector",
        "watch case","watch bumper","watch cover","protective case"
    ],
    "kÃ­nh cÆ°á»ng lá»±c": [
        "kinh cuong luc","tempered glass","screen protector","glass protector",
        "full glass","full cover","full glue","9h","anti-scratch","privacy glass"
    ],
    "á»‘p lÆ°ng": [
        "op lung","case","cover","bumper","clear case","tpu case",
        "silicone case","shockproof case","phone case","protective case"
    ],
    "vÃ²ng tay": [
        "vong tay","bracelet","bangle","chain bracelet","cuff"
    ],
    "Ã¡o thun": [
        "ao thun","ao phong","tshirt","t-shirt","tee","tee shirt","crewneck",
        "basic tee","unisex tee","oversize tee"
    ],
    "Ã¡o phÃ´ng": [
        "ao phong","ao thun","tshirt","t-shirt","tee"
    ],

    # ===== Äá»“ ngá»t/Ä‘á»“ uá»‘ng (bá»• sung cho shop) =====
    "bÃ¡nh": [
        "banh","cake","gateau","pastry","ç”œå“","é»å¿ƒ","à¹€à¸„à¹‰à¸","à¸‚à¸™à¸¡",
        "kue","kueh","roti manis"
    ],
    "bÃ¡nh crepe": [
        "banh crepe","crepe","mille crepe","crepe cake",
        "å¯ä¸½é¥¼","å¯éº—é¤…","åƒå±‚","åƒå±¤","åƒå±‚è›‹ç³•","åƒå±¤è›‹ç³•",
        "à¹€à¸„à¸£à¸›","à¹€à¸„à¸£à¸›à¹€à¸„à¹‰à¸","kue crepe","mille crepes","kue lapis"
    ],
    "bÃ¡nh sáº§u riÃªng": [
        "banh sau rieng","durian","durian cake","durian crepe",
        "æ¦´è²","æ¦´æ§¤","æ¦´è²åƒå±‚","æ¦´æ§¤åƒå±¤","æ¦´è²åƒå±‚è›‹ç³•","æ¦´æ§¤åƒå±¤è›‹ç³•","æ¦´è²å¯ä¸½é¥¼","æ¦´æ§¤å¯éº—é¤…",
        "à¹€à¸„à¸£à¸›à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™","à¹€à¸„à¹‰à¸à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™",
        "kue durian","crepe durian","kue lapis durian"
    ],
    "trÃ  sá»¯a": [
        "tra sua","milk tea","bubble tea","boba","boba tea","pearl milk tea",
        "å¥¶èŒ¶","çç å¥¶èŒ¶","æ³¢éœ¸å¥¶èŒ¶",
        "à¸Šà¸²à¸™à¸¡","à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸",
        "teh susu","teh susu boba","minuman boba","bubble tea id"
    ],
    "milktea": [
        "milk tea","bubble tea","boba","pearl milk tea",
        "å¥¶èŒ¶","çç å¥¶èŒ¶","à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸","teh susu","boba tea"
    ],

    # ===== Chinese (ZH) â€“ nhÃ³m theo khÃ¡i niá»‡m Ä‘á»ƒ báº¯t rá»™ng hÆ¡n =====
    "æ‰‹è¡¨": ["è…•è¡¨","watch","è¡¨å¸¦","è¡¨éˆ","è¡¨åœˆ","è¡¨å£³","è¡¨æ®¼","é’¢åŒ–è†œ","é‹¼åŒ–è†œ","ä¿æŠ¤å£³","ä¿è­·æ®¼"],
    "è¡¨å¸¦": ["è¡¨å¸¶","è¡¨é“¾","è¡¨éˆ","watch band","strap","çš®è¡¨å¸¦","é‡‘å±è¡¨å¸¦","ç¡…èƒ¶è¡¨å¸¦"],
    "é’¢åŒ–è†œ": ["é‹¼åŒ–è†œ","ç»ç’ƒè†œ","è´´è†œ","è²¼è†œ","ä¿æŠ¤è†œ","ä¿è­·è†œ","tempered glass","screen protector","å…¨èƒ¶","å…¨è† ","9h"],
    "æ‰‹æœºå£³": ["æ‰‹æ©Ÿæ®¼","ä¿æŠ¤å£³","ä¿è­·æ®¼","æ‰‹æœºå¥—","phone case","case","bumper","ä¿è­·æ®¼"],
    "Tæ¤": ["Tæ¤è¡«","çŸ­è¢–","åœ†é¢†","åœ“é ˜","tee","tshirt","t-shirt"], 
    "å¥¶èŒ¶": ["çç å¥¶èŒ¶","æ³¢éœ¸å¥¶èŒ¶","å¥¶ç›–èŒ¶","milk tea","bubble tea","boba"],
    "å¯ä¸½é¥¼": ["å¯éº—é¤…","æ³•å¼è–„é¥¼","æ³•å¼è–„é¤…","åƒå±‚","åƒå±¤","åƒå±‚è›‹ç³•","åƒå±¤è›‹ç³•","crepe","mille crepe"],
    "æ¦´è²": ["æ¦´æ§¤","durian","æ¦´è²åƒå±‚","æ¦´æ§¤åƒå±¤","æ¦´è²å¯ä¸½é¥¼","æ¦´æ§¤å¯éº—é¤…","æ¦´è²è›‹ç³•","æ¦´æ§¤è›‹ç³•"],

    # ===== Thai (TH) =====
    "à¸™à¸²à¸¬à¸´à¸à¸²": ["watch","à¸ªà¸²à¸¢à¸™à¸²à¸¬à¸´à¸à¸²","à¸Ÿà¸´à¸¥à¹Œà¸¡à¸à¸£à¸°à¸ˆà¸","à¸à¸£à¸­à¸šà¸™à¸²à¸¬à¸´à¸à¸²","à¹€à¸„à¸ªà¸™à¸²à¸¬à¸´à¸à¸²"],
    "à¸ªà¸²à¸¢à¸™à¸²à¸¬à¸´à¸à¸²": ["watch band","strap","à¸ªà¸²à¸¢à¸«à¸™à¸±à¸‡","à¸ªà¸²à¸¢à¹‚à¸¥à¸«à¸°","à¸ªà¸²à¸¢à¸‹à¸´à¸¥à¸´à¹‚à¸„à¸™","à¸™à¸²à¹‚à¸•à¹‰"],
    "à¸Ÿà¸´à¸¥à¹Œà¸¡à¸à¸£à¸°à¸ˆà¸": ["tempered glass","à¸à¸£à¸°à¸ˆà¸à¸à¸±à¸™à¸£à¸­à¸¢","full glue","9h","screen protector"],
    "à¹€à¸„à¸ªà¹‚à¸—à¸£à¸¨à¸±à¸à¸—à¹Œ": ["à¹€à¸„à¸ª","à¸‹à¸­à¸‡à¸¡à¸·à¸­à¸–à¸·à¸­","bumper","phone case","protective case"],
    "à¹€à¸ªà¸·à¹‰à¸­à¸¢à¸·à¸”": ["tshirt","t-shirt","tee","à¸„à¸­à¸à¸¥à¸¡","à¹‚à¸­à¹€à¸§à¸­à¸£à¹Œà¹„à¸‹à¸‹à¹Œ"],
    "à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸": ["à¸Šà¸²à¸™à¸¡","bubble tea","boba","milk tea"],
    "à¹€à¸„à¸£à¸›": ["à¹€à¸„à¸£à¸›à¹€à¸„à¹‰à¸","crepe","mille crepe"],
    "à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™": ["durian","à¹€à¸„à¸£à¸›à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™","à¹€à¸„à¹‰à¸à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™"],

    # ===== Indonesian (ID) =====
    "jam tangan": ["watch","tali jam","pelindung layar","casing jam","bezel","case jam"],
    "tali jam": ["watch band","strap","nato","kulit","logam","silikon"],
    "pelindung layar": ["tempered glass","screen protector","kaca tempered","9h","full glue"],
    "casing hp": ["case","casing","sarung hp","bumper","phone case"],
    "kaos": ["tshirt","t-shirt","tee","kaos oversize","kaos unisex"],
    "teh susu": ["bubble tea","boba","milk tea","minuman boba"],
    "crepe": ["kue crepe","mille crepe","kue lapis","crepe cake"],
    "durian": ["kue durian","crepe durian","kue lapis durian"]
}
def add_syn(key, words):
    arr = VN_SYNONYMS.setdefault(key, [])
    # ná»‘i vÃ  khá»­ trÃ¹ng láº·p, giá»¯ thá»© tá»±
    VN_SYNONYMS[key] = list(dict.fromkeys(arr + words))

add_syn("æ¦´æ§¤", ["æ¦´è²","durian","æ¦´æ§¤åƒå±¤","æ¦´æ§¤å¯éº—é¤…","æ¦´æ§¤è›‹ç³•","æ¦´æ§¤åƒå±¤è›‹ç³•"])
add_syn("å¯éº—é¤…", ["å¯ä¸½é¥¼","crepe","mille crepe","crepe cake","åƒå±¤","åƒå±¤è›‹ç³•","æ³•å¼å¯éº—é¤…"])
add_syn("å¥¶èŒ¶", ["çç å¥¶èŒ¶","æ³¢éœ¸å¥¶èŒ¶","å¥¶è“‹èŒ¶","milk tea","bubble tea","boba"])
add_syn("å¸‚é›†", ["market","marketplace","é›†å¸‚","bazaar"])
add_syn("å°ç£", ["è‡ºç£","å°æ¹¾","taiwan","tw"])
add_syn("è‡ºç£", ["å°ç£","å°æ¹¾","taiwan","tw"])
add_syn("æ¾³æ´²", ["australia","Ãºc","au"])
add_syn("è¶Šå—", ["vietnam","viá»‡t nam","vn"])
add_syn("æ‰‹éŒ¶", ["æ‰‹è¡¨","è…•éŒ¶","è…•è¡¨"])
add_syn("éŒ¶å¸¶", ["è¡¨å¸¦","è¡¨å¸¶","éŒ¶éˆ","è¡¨éˆ","é‡‘å±¬éŒ¶å¸¶","é‡‘å±è¡¨å¸¦"])
add_syn("é‹¼åŒ–è†œ", ["é’¢åŒ–è†œ","ç»ç’ƒè²¼","ç»ç’ƒè´´","ä¿è­·è²¼","ä¿æŠ¤è´´","æ»¿ç‰ˆç»ç’ƒ","æ»¡ç‰ˆç»ç’ƒ","å…¨è† ","å…¨èƒ¶"])
add_syn("æ‰‹æ©Ÿæ®¼", ["æ‰‹æœºå£³","ä¿è­·æ®¼","ä¿æŠ¤å£³","æ‰‹æ©Ÿå¥—","æ‰‹æœºå¥—","èƒŒè“‹","èƒŒç›–"])

# --- Bá»• sung Ä‘á»“ng nghÄ©a cho shop (TW/ç¹é«”) ---
def _query_tokens(q: str, lang: str = "vi") -> set:
    n1, n2 = _norm_both(q)
    w1 = [w for w in re.split(r"\s+", n1) if len(w) > 1]
    w2 = [w for w in re.split(r"\s+", n2) if len(w) > 1]

    tokens: set[str] = set(w1) | set(w2)

    # Bigrams theo Tá»ª (cÃ³ & khÃ´ng dáº¥u) â€“ chá»‰ lÃ m 1 láº§n
    for words in (w1, w2):
        for i in range(len(words) - 1):
            a, b = words[i], words[i + 1]
            tokens.add(f"{a} {b}")
            tokens.add(f"{a}{b}")  # biáº¿n thá»ƒ khÃ´ng space

    # n-gram theo KÃ Tá»° cho CJK (2 & 3), kÃ¨m TWâ†’CN
    if _any_cjk(q):
        base  = re.sub(r"\s+", "", _normalize_text(q))
        basec = _zh_compat(base)
        for s in (base, basec):
            L = len(s)
            if L >= 2:
                tokens.update(s[i:i+2] for i in range(L - 1))
            if L >= 3:
                tokens.update(s[i:i+3] for i in range(L - 2))

    # Cá»¥m phrase Ä‘áº·c thÃ¹ theo ngÃ´n ngá»¯
    combo_phrases = {
        "vi": ["Ä‘á»“ng há»“","dÃ¢y Ä‘á»“ng há»“","kÃ­nh cÆ°á»ng lá»±c","á»‘p lÆ°ng","Ã¡o thun","Ã¡o phÃ´ng","bÃ¡nh crepe","bÃ¡nh sáº§u riÃªng","trÃ  sá»¯a"],
        "en": ["watch band","screen protector","phone case","t shirt","t-shirt","mille crepe","durian crepe","milk tea","bubble tea","boba tea"],
        "zh": ["æ‰‹è¡¨","è¡¨å¸¦","é’¢åŒ–è†œ","æ‰‹æœºå£³","Tæ¤","å¯ä¸½é¥¼","æ¦´è²åƒå±‚","å¥¶èŒ¶","çç å¥¶èŒ¶","æ‰‹éŒ¶","éŒ¶å¸¶","é‹¼åŒ–è†œ","æ‰‹æ©Ÿæ®¼","å¯éº—é¤…","æ¦´æ§¤åƒå±¤","ç»ç’ƒè²¼","ä¿è­·è²¼","æ»¿ç‰ˆç»ç’ƒ"],
        "th": ["à¸™à¸²à¸¬à¸´à¸à¸²","à¸ªà¸²à¸¢à¸™à¸²à¸¬à¸´à¸à¸²","à¸Ÿà¸´à¸¥à¹Œà¸¡à¸à¸£à¸°à¸ˆà¸","à¹€à¸„à¸ªà¹‚à¸—à¸£à¸¨à¸±à¸à¸—à¹Œ","à¹€à¸ªà¸·à¹‰à¸­à¸¢à¸·à¸”","à¹€à¸„à¸£à¸›","à¹€à¸„à¸£à¸›à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™","à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸"],
        "id": ["jam tangan","tali jam","pelindung layar","casing hp","kaos","kue crepe","crepe durian","teh susu","bubble tea","boba"],
    }

    joined_n1 = " ".join(w1)
    for phrase in combo_phrases.get(lang, []):
        if phrase in joined_n1:
            p1, p2 = _norm_both(phrase)
            variants = {p1, p2, p1.replace(" ", ""), p2.replace(" ", "")}
            # náº¿u lÃ  CJK thÃ¬ thÃªm báº£n TWâ†’CN
            if _any_cjk(phrase):
                c1, c2 = _zh_compat(p1), _zh_compat(p2)
                variants |= {c1, c2, c1.replace(" ", ""), c2.replace(" ", "")}
            tokens.update(variants)

    # Äá»“ng nghÄ©a: náº¿u phÃ¡t hiá»‡n 1 key hay 1 synonym trong cÃ¢u há»i â†’ add táº¥t cáº£ biáº¿n thá»ƒ
    n1_noacc, n2_noacc = n1, n2
    for key, syns in VN_SYNONYMS.items():
        key_n1, key_n2 = _norm_both(key)
        seen = (key_n1 in n1_noacc) or (key_n2 in n2_noacc)
        if not seen:
            for s in syns:
                s1, s2 = _norm_both(s)
                if s1 in n1_noacc or s2 in n2_noacc:
                    seen = True
                    break
        if seen:
            for s in [key] + list(syns):
                s1, s2 = _norm_both(s)
                cand = {s1, s2, s1.replace(" ", ""), s2.replace(" ", "")}
                if _any_cjk(s):
                    c1, c2 = _zh_compat(s1), _zh_compat(s2)
                    cand |= {c1, c2, c1.replace(" ", ""), c2.replace(" ", "")}
                tokens.update(cand)

    # Loáº¡i bá» rá»—ng, giá»¯ token dÃ i â‰¥ 2
    return {t.strip() for t in tokens if isinstance(t, str) and len(t.strip()) >= 2}

def filter_hits_by_query(hits, q, lang="vi"):
    if not hits:
        return []
    qtoks = _query_tokens(q, lang=lang)
    # thÃªm biáº¿n thá»ƒ TW->CN cho token
    qtoks = qtoks | {_zh_compat(t) for t in qtoks}

    kept = []
    for d in hits:
        hay_raw = " ".join([d.get("title",""), d.get("tags",""), d.get("product_type",""), d.get("variant","")])
        h1, h2 = _norm_both(hay_raw)
        h1c, h2c = _zh_compat(h1), _zh_compat(h2)

        h1_ns, h2_ns   = h1.replace(" ", ""), h2.replace(" ", "")
        h1c_ns, h2c_ns = h1c.replace(" ", ""), h2c.replace(" ", "")

        ok = any(
            (t in h1) or (t in h2) or (t in h1c) or (t in h2c) or
            (t.replace(" ","") in h1_ns) or (t.replace(" ","") in h2_ns) or
            (t.replace(" ","") in h1c_ns) or (t.replace(" ","") in h2c_ns)
            for t in qtoks
        )
        if ok:
            kept.append(d)
    return kept



def should_relax_filter(q: str, hits: list) -> bool:
    qn = _normalize_text(q)
    return len(qn.split()) <= 2 and len(hits) > 0

# ====== Compose tráº£ lá»i ======
def compose_product_reply(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    # Æ¯u tiÃªn currency trong meta; náº¿u khÃ´ng cÃ³, máº·c Ä‘á»‹nh â‚« cho VI
    currency = (hits[0].get("currency") or ("â‚«" if lang == "vi" else ""))

    items = []
    for d in hits[:2]:
        title     = d.get("title") or "Sáº£n pháº©m"
        variant   = d.get("variant")
        stock     = _stock_line(d)

        price_val = _price_value(d)
        price_str = _fmt_price(price_val, currency) if price_val is not None else None

        line = f"â€¢ {title}"
        if variant:
            line += f" ({variant})"
        if price_str:
            line += f" â€” {price_str}"
        line += f" â€” {stock}"
        items.append(line)

    raw = f"{t(lang,'suggest_hdr')}\n" + "\n".join(items) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)

def compose_product_info(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    d = hits[0]
    currency   = d.get("currency") or ("â‚«" if lang == "vi" else "")
    title      = d.get("title") or "Sáº£n pháº©m"
    stock      = _stock_line(d)

    price_val  = _price_value(d)
    price_line = f"GiÃ¡ tham kháº£o: {_fmt_price(price_val, currency)}" if price_val is not None else ""

    bullets = _extract_features_from_text(d.get("text",""))
    body    = "\n".join(bullets) if bullets else "â€¢ Thiáº¿t káº¿ tá»‘i giáº£n, dá»… phá»‘i Ä‘á»“\nâ€¢ Cháº¥t liá»‡u thoÃ¡ng, dá»… vá»‡ sinh"

    parts = [
        f"{t(lang,'highlights', title=title)}",
        body
    ]
    if price_line:
        parts.append(price_line)
    parts.extend([
        f"TÃ¬nh tráº¡ng: {stock}",
        t(lang,"product_pts")
    ])

    raw = "\n".join(parts).strip()
    return rephrase_casual(raw, intent="product", lang=lang)


def compose_contextual_answer(context, question, history, lang="vi"):
    ctx = (shop_identity(lang) + "\n" + (context or "")).strip()
    msgs = build_messages(SYSTEM_STYLE, history, ctx, question)
    _, reply = call_openai(msgs, temperature=0.6)
    return reply


def compose_price_with_suggestions(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback"), []

    main = hits[0]
    currency = main.get("currency") or ("â‚«" if lang == "vi" else "")
    main_price = _price_value(main)
    main_price_str = _fmt_price(main_price, currency) if main_price is not None else "Ä‘ang cáº­p nháº­t"

    low, high = _minmax_in_category(main)

    lines = []
    title = main.get("title") or "Sáº£n pháº©m"
    lines.append(f"VÃ¢ng áº¡, **{title}** Ä‘ang Ä‘Æ°á»£c shop bÃ¡n vá»›i **giÃ¡ cÃ´ng khai: {main_price_str}**.")
    sug = []
    if high:
        hp = _fmt_price(_price_value(high), currency)
        sug.append(f"â€¢ **CÃ¹ng dÃ²ng â€“ giÃ¡ cao nháº¥t:** {high.get('title','SP')} â€” {hp}")
    if low:
        lp = _fmt_price(_price_value(low), currency)
        sug.append(f"â€¢ **CÃ¹ng dÃ²ng â€“ giÃ¡ tháº¥p nháº¥t:** {low.get('title','SP')} â€” {lp}")

    if sug:
        lines.append("Báº¡n cÅ©ng cÃ³ thá»ƒ tham kháº£o thÃªm:")
        lines += sug
    lines.append(t(lang, "product_pts"))
    raw = "\n".join(lines)

    # ThÃªm SP chÃ­nh vÃ o button Ä‘áº§u tiÃªn
    btns = [main] + [x for x in (high, low) if x]
    return rephrase_casual(raw, intent="product", lang=lang), btns[:2]
def answer_with_rag(user_id, user_question):
    s = _get_sess(user_id)
    hist = s["hist"]

    intent = detect_intent(user_question)
    lang = detect_lang(user_question)
    print(f"ğŸ” intent={intent} | ğŸ—£ï¸ lang={lang}")

    # â€”â€”â€” QUICK ROUTES â€”â€”â€”
    if intent == "greet":
        return greet_text(lang), []
    if intent == "smalltalk":
        return handle_smalltalk(user_question, lang=lang), []
    if intent == "browse":
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return t(lang, "browse", url=url), []
    if intent == "new_items":
        items = get_new_arrivals(days=30, topk=4)
        return compose_new_arrivals(lang=lang, items=items), items[:2]

    # â€”â€”â€” PRODUCT SEARCH â€”â€”â€”
    prod_hits, prod_scores = search_products_with_scores(user_question, topk=8)
    best = max(prod_scores or [0.0])

    ok_by_score = _score_gate(user_question, prod_hits, best)


    filtered_hits = filter_hits_by_query(prod_hits, user_question, lang=lang) if STRICT_MATCH else prod_hits
    # náº¿u STRICT_MATCH lÃ m rá»—ng mÃ  catalog lÃ  ZH â†’ ná»›i lá»c
    if STRICT_MATCH and not filtered_hits and (_any_cjk(user_question) or _cjk_in_hits(prod_hits)):
        filtered_hits = prod_hits

    title_ok = _has_title_overlap(user_question, prod_hits)

    if intent == "other" and (filtered_hits or title_ok):
        intent = "product"

    if title_ok and not filtered_hits:
        filtered_hits = prod_hits

    print(f"ğŸ“ˆ best_score={best:.3f}, hits={len(prod_hits)}, kept_after_filter={len(filtered_hits)}, title_ok={title_ok}")

        # --- CONTEXT/POLICY ---
    context = retrieve_context(user_question, topk=6)
    if intent == "policy" and context:
        ans = compose_contextual_answer(context, user_question, hist, lang=lang)
        ans = f"{t(lang,'policy_hint')} {ans}"
        return rephrase_casual(ans, intent="policy", temperature=0.5, lang=lang), []

    # --- Æ¯U TIÃŠN Há»I GIÃ ---
    if is_price_question(user_question, lang) and (filtered_hits or title_ok):
        print("â¡ï¸ route=price_questionâ†’price_with_suggestions")
        chosen = filtered_hits if filtered_hits else prod_hits
        reply, sug_hits = compose_price_with_suggestions(chosen, lang=lang)
        return reply, sug_hits

    # --- PRODUCT BRANCHES ---
    # (náº¿u báº¡n Ä‘Ã£ thÃªm ok_by_score theo patch trÆ°á»›c, dÃ¹ng nÃ³; chÆ°a cÃ³ thÃ¬ thay ok_by_score báº±ng (best >= SCORE_MIN))
    not_enough = (not filtered_hits) or (not ok_by_score and not title_ok)

    if intent in {"product", "product_info"} and not_enough:
        # 1) CÃ³ context â†’ dÃ¹ng LLM + context
        if context:
            print("â¡ï¸ route=ctx_fallback_from_product")
            ans = compose_contextual_answer(context, user_question, hist, lang=lang)
            return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []
        # 2) KhÃ´ng cÃ³ context â†’ LLM trÆ¡n (shop_identity váº«n Ä‘Æ°á»£c chÃ¨n trong compose_contextual_answer)
        if ALWAYS_ANSWER:
            print("â¡ï¸ route=llm_fallback_from_product")
            ans = compose_contextual_answer("", user_question, hist, lang=lang)
            return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []
        # 3) Cuá»‘i cÃ¹ng má»›i rÆ¡i vá» OOS
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        print("â¡ï¸ route=oos_hint")
        return t(lang, "oos", url=url), []

    if intent == "product_info":
        print("â¡ï¸ route=product_info")
        return compose_product_info(filtered_hits, lang=lang), filtered_hits[:1]

    if intent in {"product", "other"} and filtered_hits and (ok_by_score or title_ok):
        print("â¡ï¸ route=product_reply")
        return compose_product_reply(filtered_hits, lang=lang), filtered_hits[:2]

    # --- CONTEXT FALLBACK CHUNG ---
    if context:
        print("â¡ï¸ route=ctx_fallback")
        ans = compose_contextual_answer(context, user_question, hist, lang=lang)
        return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []

    print("â¡ï¸ route=fallback")
    if ALWAYS_ANSWER:
        ans = compose_contextual_answer("", user_question, hist, lang=lang)
        return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []
    return t(lang, "fallback"), []


# ========= WEBHOOK =========
@app.route("/webhook", methods=["GET", "POST"])
def webhook():
    # --- Verify (GET)
    if request.method == "GET":
        token = request.args.get("hub.verify_token")
        challenge = request.args.get("hub.challenge")
        print(f"[Webhook][GET] verify_token={token}, challenge={challenge}")
        return (challenge, 200) if token == VERIFY_TOKEN else ("Invalid verification token", 403)

    # --- Events (POST)
    if not _verify_fb_sig(request):
        ua = request.headers.get("User-Agent", "?")
        print(f"[Webhook][POST] âŒ Invalid signature (UA={ua})")
        return "Invalid signature", 403

    payload = request.json or {}
    print("[Webhook][POST] ğŸ”” incoming:", json.dumps(payload)[:500])

    for entry in payload.get("entry", []):
        owner_id = str(entry.get("id"))
        page_token = TOKEN_MAP.get(owner_id)
        if not page_token:
            print(f"[Webhook] âš ï¸ No token mapped for owner_id={owner_id}. TOKEN_MAP size={len(TOKEN_MAP)}")
            continue

        for event in entry.get("messaging", []):
            try:
                # 1) Bá» qua cÃ¡c event khÃ´ng pháº£i user nháº¯n tin
                if event.get("message", {}).get("is_echo"):
                    continue
                if "delivery" in event or "read" in event or "reaction" in event:
                    continue
                if not (event.get("message") or event.get("postback")):
                    continue

                psid = event.get("sender", {}).get("id")
                if not psid:
                    continue

                msg = event.get("message", {}) or {}
                pb  = event.get("postback", {}) or {}

                # 2) Láº¥y text há»£p lá»‡
                text = None
                if "text" in msg:
                    text = msg["text"]
                elif msg.get("quick_reply", {}).get("payload"):
                    text = msg["quick_reply"]["payload"]
                elif pb.get("payload"):
                    text = pb["payload"]
                elif pb.get("title"):
                    text = pb["title"]

                # 3) Náº¿u chá»‰ lÃ  attachments â†’ nháº¯n 1 cÃ¢u rá»“i thÃ´i
                if not text:
                    if msg.get("attachments"):
                        fb_send_text(psid, "MÃ¬nh Ä‘Ã£ nháº­n áº£nh/file báº¡n gá»­i. MÃ´ táº£ thÃªm Ä‘á»ƒ mÃ¬nh tÆ° váº¥n nhÃ© ğŸ˜Š", page_token)
                    continue

                # 4) Chá»‘ng trÃ¹ng theo MID (khÃ´ng dÃ¹ng timestamp)
                mid = msg.get("mid") or pb.get("mid")
                if mid:
                    sess = _get_sess(psid)
                    if sess.get("last_mid") == mid:
                        continue
                    sess["last_mid"] = mid

                fb_mark_seen(psid, page_token)
                fb_typing_on(psid, page_token)

                _remember(psid, "user", text)
                reply, btn_hits = answer_with_rag(psid, text)
                lang = detect_lang(text)
                _remember(psid, "assistant", reply)

                fb_send_text(psid, reply, page_token)

                if btn_hits:
                    buttons = []
                    for h in btn_hits[:3]:
                        if h.get("url"):
                            buttons.append({
                                "type": "web_url",
                                "url": h["url"],
                                "title": (h.get("title") or t(lang, "btn_view"))[:20]
                            })
                    if buttons:
                        fb_send_buttons(psid, t(lang, "quick_view"), buttons, page_token)

            except Exception as e:
                print("[Webhook][POST] âš ï¸ handle event error:", repr(e))
                continue

    # LuÃ´n 200 sau khi xá»­ lÃ½ xong
    return "ok", 200



# ========= API =========
@app.route("/api/chat", methods=["POST"])
def chat():
    data = request.json or {}
    messages = data.get("messages", [])
    print("ğŸŒ /api/chat len =", len(messages))
    openai_raw, _ = call_openai(messages)
    return jsonify(openai_raw)

@app.route("/api/chat_rag", methods=["POST"])
def chat_rag():
    data = request.json or {}
    q = data.get("question", "")
    print("ğŸŒ /api/chat_rag question:", q)
    if not q:
        return jsonify({"error": "Missing 'question'"}), 400
    reply, _ = answer_with_rag("anonymous", q)
    return jsonify({"reply": reply})

@app.route("/api/product_search")
def api_product_search():
    q = (request.args.get("q") or "").strip()
    if not q:
        return jsonify({"ok": False, "msg": "missing q"}), 400
    lang = detect_lang(q)
    hits, scores = search_products_with_scores(q, topk=8)
    best = max(scores or [0.0])

    kept = filter_hits_by_query(hits, q, lang=lang) if STRICT_MATCH else hits
    if STRICT_MATCH and not kept and should_relax_filter(q, hits):
        kept = hits

    ok_by_score = _score_gate(q, hits, best)
    title_ok = _has_title_overlap(q, hits)

    if not kept or (not ok_by_score and not title_ok):
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return jsonify({"ok": True, "reply": t(lang, "oos", url=url), "items": []})

    reply = compose_product_reply(kept, lang=lang)
    return jsonify({"ok": True, "reply": reply, "items": kept[:2]})


# ========= IG OAuth callback & policy pages =========
@app.route("/auth/callback")
def auth_callback():
    code = request.args.get("code")
    print("ğŸ” /auth/callback code:", code)
    return f"Auth success! Code: {code}"

@app.route("/privacy")
def privacy():
    return """
    <h1>Privacy Policy - Aloha Bot</h1>
    <p>ChÃºng tÃ´i chá»‰ xá»­ lÃ½ ná»™i dung tin nháº¯n mÃ  ngÆ°á»i dÃ¹ng gá»­i tá»›i Fanpage Ä‘á»ƒ tráº£ lá»i.
    KhÃ´ng bÃ¡n/chia sáº» dá»¯ liá»‡u cÃ¡ nhÃ¢n. Dá»¯ liá»‡u phiÃªn trÃ² chuyá»‡n (session) chá»‰ lÆ°u táº¡m thá»i
    tá»‘i Ä‘a 30 phÃºt phá»¥c vá»¥ tráº£ lá»i vÃ  sáº½ tá»± xoÃ¡ sau Ä‘Ã³. Chá»‰ sá»‘ sáº£n pháº©m (vectors) lÃ  dá»¯ liá»‡u cÃ´ng khai tá»« cá»­a hÃ ng.</p>
    <p>LiÃªn há»‡ xoÃ¡ dá»¯ liá»‡u: gá»­i tin nháº¯n 'delete my data' tá»›i Fanpage hoáº·c email: <b>hoclac1225@email.com</b>.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

@app.route("/data_deletion")
def data_deletion():
    return """
    <h1>Data Deletion Instructions</h1>
    <p>Äá»ƒ yÃªu cáº§u xoÃ¡ dá»¯ liá»‡u: (1) nháº¯n 'delete my data' tá»›i Fanpage, hoáº·c (2) gá»­i email tá»›i <b>hoclac1225@email.com</b>
    kÃ¨m ID cuá»™c trÃ² chuyá»‡n. ChÃºng tÃ´i sáº½ xá»­ lÃ½ trong thá»i gian sá»›m nháº¥t.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

# ========= Debug & Health =========
@app.route("/debug/rag_status")
def rag_status():
    return jsonify({
        "vector_dir": os.path.abspath(VECTOR_DIR),
        "products_index": bool(IDX_PROD),
        "products_chunks": len(META_PROD) if META_PROD else 0,
        "policies_index": bool(IDX_POL),
        "policies_chunks": len(META_POL) if META_POL else 0,
        "sessions": len(SESS),
    })

@app.route("/health")
def health():
    return jsonify({
        "ok": True,
        "products": len(META_PROD) if META_PROD else 0,
        "policies": len(META_POL) if META_POL else 0
    })

# ========= Watcher: tá»± reload khi vector Ä‘á»•i =========


_last_vec_mtime = 0
def _mtime(path):
    try:
        return os.path.getmtime(path)
    except Exception:
        return 0

def _watch_vectors():
    global _last_vec_mtime
    idx_p = os.path.join(VECTOR_DIR, "products.index")
    meta_p = os.path.join(VECTOR_DIR, "products.meta.json")
    idx_k = os.path.join(VECTOR_DIR, "policies.index")
    meta_k = os.path.join(VECTOR_DIR, "policies.meta.json")

    newest = max(_mtime(idx_p), _mtime(meta_p), _mtime(idx_k), _mtime(meta_k))
    if newest and newest != _last_vec_mtime:
        print("ğŸ•µï¸ Detected vector change â†’ reload")
        if _reload_vectors():
            _last_vec_mtime = newest

def _start_vector_watcher():
    try:
        from apscheduler.schedulers.background import BackgroundScheduler
        sch = BackgroundScheduler(timezone="Asia/Ho_Chi_Minh")
        sch.add_job(_watch_vectors, "interval", seconds=30, id="watch_vectors")
        sch.add_job(lambda: (_purge_sessions()), "interval", minutes=10, id="purge_sessions")
        sch.start()
        print("â±ï¸ Vector watcher started (30s)")
    except Exception as e:
        print("âš ï¸ Scheduler error:", repr(e))



# ======== MAIN ========
if __name__ == "__main__":
    if os.getenv("ENABLE_VECTOR_WATCHER", "true").lower() == "true":
        _start_vector_watcher()
    port = int(os.getenv("PORT", 3000))
    print(f"ğŸš€ Starting app on 0.0.0.0:{port}")
    # app.run(host="0.0.0.0", port=port, debug=False)
