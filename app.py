
# -*- coding: utf-8 -*-
import unicodedata
import os, json, time, re, requests, numpy as np, faiss, threading, random
from collections import deque
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from openai import OpenAI
import hmac, hashlib, base64
from typing import Optional
from ingest_products import fetch_all_products, build_docs, dedup_docs, save_faiss


# --- Flask & CORS ---
app = Flask(__name__)
CORS(app, resources={
    r"/api/*": {
        "origins": [
            "https://shop.aloha.id.vn",   # <- TH√äM D√íNG N√ÄY
            "https://aloha.id.vn",
            "https://www.aloha.id.vn",
            "https://9mn9fa-6p.myshopify.com",
        ],
        "supports_credentials": True,
        "allow_headers": ["Content-Type", "content-type", "Authorization", "X-Admin-Token"],
        "methods": ["GET", "POST", "OPTIONS"],
    }
})


# Load .env TR∆Ø·ªöC khi ƒë·ªçc os.getenv
load_dotenv()


# --- text normalize helpers (c√≥ & kh√¥ng d·∫•u)
def _strip_accents(s: str) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if not unicodedata.combining(ch))

def _normalize_text(s: str) -> str:
    # gi·ªØ l·∫°i ch·ªØ, s·ªë, kho·∫£ng tr·∫Øng, c√°c b·∫£ng ch·ªØ c√°i m·ªü r·ªông
    return re.sub(r"[^0-9a-z\u00c0-\u024f\u1e00-\u1eff\u4e00-\u9fff\u0E00-\u0E7F ]", " ", (s or "").lower()).strip()

def _norm_both(s: str):
    """Tr·∫£ v·ªÅ tuple (c√≥_d·∫•u, kh√¥ng_d·∫•u) ƒë√£ normalize & lower."""
    n1 = _normalize_text(s)
    n2 = _normalize_text(_strip_accents(s))
    return n1, n2

def _char_ngrams(s: str, n=2):
    s = _normalize_text(s)
    s = re.sub(r"\s+", "", s)
    if not s:
        return set()
    if len(s) < n:
        return {s}
    return {s[i:i+n] for i in range(len(s)-n+1)}

# --- Cross-language helpers ---
CJK_RE = re.compile(r"[\u4e00-\u9fff]")

def _any_cjk(s: str) -> bool:
    return bool(CJK_RE.search(s or ""))

def _cjk_in_hits(hits, k=None) -> bool:
    if k is None:
        k = TITLE_MAX_CHECK
    for d in hits[:k]:
        if _any_cjk(d.get("title")) or _any_cjk(d.get("tags")) or _any_cjk(d.get("product_type")):
            return True
    return False

def _score_gate(q: str, hits: list, best: float) -> bool:
    """
    Tr·∫£ v·ªÅ True n·∫øu best score ƒë·ªß t·ªët sau khi √°p d·ª•ng ng∆∞·ª°ng ƒë·ªông.
    - C√¢u ng·∫Øn: h·∫° 0.05
    - Danh m·ª•c/ti√™u ƒë·ªÅ ZH (ch√©o ng√¥n ng·ªØ): h·∫° 0.08
    - S√†n t·ªëi thi·ªÉu: 0.18
    """
    th = SCORE_MIN
    if len(_normalize_text(q).split()) <= 3:
        th -= 0.05
    if _any_cjk(q) or _cjk_in_hits(hits):
        th -= 0.08
    th = max(0.18, th)
    return best >= th


# --- Title overlap config (ƒë·∫∑t ·ªü c·∫•p module, sau load_dotenv) ---
TITLE_MIN_WORDS = int(os.getenv("TITLE_MIN_WORDS", "3"))
TITLE_CJK_MIN_COVER = float(os.getenv("TITLE_CJK_MIN_COVER", "0.30"))
TITLE_MAX_CHECK = int(os.getenv("TITLE_MAX_CHECK", "5"))
# ==== Title normalization & similarity (ƒëa ng√¥n ng·ªØ) ====
# b·ªè emoji/k√≠ hi·ªáu Surrogate Plane (kh√¥ng ·∫£nh h∆∞·ªüng ch·ªØ VN/CJK)
EMOJI_RE = re.compile(r"[\U00010000-\U0010FFFF]")

def _strip_emoji_symbols(s: str) -> str:
    return EMOJI_RE.sub(" ", s or "")

def _norm_title(s: str) -> str:
    # chu·∫©n ho√° width (ÔΩú -> |), h·∫° k√≠ t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ s·ªë + latin + CJK
    s = unicodedata.normalize("NFKC", s or "")
    s = s.replace("ÔΩú", " ").replace("|", " ")
    s = _strip_emoji_symbols(s)
    return _normalize_text(s)

def _cjk_cover(qn: str, tn: str) -> float:
    # t·ªâ l·ªá ph·ªß bigram k√Ω t·ª± (d√πng cho CJK/ti√™u ƒë·ªÅ kh√¥ng c√≥ kho·∫£ng tr·∫Øng)
    qg = _char_ngrams(qn, 2)
    tg = _char_ngrams(tn, 2)
    if not qg or not tg:
        return 0.0
    return len(qg & tg) / max(1, len(qg))

def _title_similarity(q: str, title: str) -> float:
    qn = _norm_title(q)
    tn = _norm_title(title)
    # n·∫øu c√≥ CJK ‚Üí d√πng cover bigram; ng∆∞·ª£c l·∫°i d√πng tr√πng t·ª´
    if _any_cjk(qn) or _any_cjk(tn):
        return _cjk_cover(qn, tn)          # 0.0 ‚Äì 1.0
    qwords = [w for w in qn.split() if len(w) > 1]
    twords = set(tn.split())
    if not qwords or not twords:
        return 0.0
    matched = sum(1 for w in qwords if w in twords)
    return matched / len(qwords)            # 0.0 ‚Äì 1.0

def _rerank_by_title(q: str, hits: list, scores: list) -> list:
    """Tr·ªôn score vector + ƒë·ªô gi·ªëng ti√™u ƒë·ªÅ ƒë·ªÉ ∆∞u ti√™n ƒë√∫ng s·∫£n ph·∫©m."""
    out = []
    for i, d in enumerate(hits):
        dd = dict(d)  # kh√¥ng l√†m h·ªèng c·∫•u tr√∫c c≈©
        dd["_title_sim"] = _title_similarity(q, d.get("title", ""))
        dd["score"] = float(scores[i]) if i < len(scores) else 0.0
        out.append(dd)
    # ∆∞u ti√™n theo ƒë·ªô gi·ªëng ti√™u ƒë·ªÅ, sau ƒë√≥ t·ªõi ƒëi·ªÉm vector
    out.sort(key=lambda x: (x.get("_title_sim", 0.0), x.get("score", 0.0)), reverse=True)
    return out


def _has_title_overlap(
    q: str,
    hits: list,
    min_words: Optional[int] = None,   # n·∫øu d√πng Python 3.10+ c√≥ th·ªÉ d√πng: int | None
    min_cover: float = 0.6
) -> bool:
    """
    Ki·ªÉm tra m·ª©c tr√πng kh·ªõp gi·ªØa c√¢u h·ªèi v√† title c√°c hit.
    - Ng√¥n ng·ªØ c√≥ kho·∫£ng tr·∫Øng: y√™u c·∫ßu s·ªë t·ª´ tr√πng t·ªëi thi·ªÉu (TITLE_MIN_WORDS) ho·∫∑c t·ªâ l·ªá ph·ªß >= min_cover.
    - CJK/kh√¥ng c√≥ kho·∫£ng tr·∫Øng: d√πng bigram k√Ω t·ª± v·ªõi ng∆∞·ª°ng TITLE_CJK_MIN_COVER.
    """
    if not q or not hits:
        return False

    if min_words is None:
        min_words = TITLE_MIN_WORDS

    qn1, qn2 = _norm_both(q)
    qtok = [w for w in qn1.split() if len(w) > 1]

    # CJK/kh√¥ng c√≥ kho·∫£ng tr·∫Øng ‚Üí so tr√πng bigram k√Ω t·ª±
    if not qtok or re.search(r"[\u4e00-\u9fff]", q):
        qgrams = _char_ngrams(qn1, 2) | _char_ngrams(qn2, 2)
        if not qgrams:
            return False
        for d in hits[:TITLE_MAX_CHECK]:
            t1, t2 = _norm_both(d.get("title", ""))
            tgrams = _char_ngrams(t1, 2) | _char_ngrams(t2, 2)
            cover = len(qgrams & tgrams) / max(1, len(qgrams))
            if cover >= TITLE_CJK_MIN_COVER:
                return True
        return False

    # Ng√¥n ng·ªØ c√≥ kho·∫£ng tr·∫Øng ‚Üí so theo t·ª´
    for d in hits[:TITLE_MAX_CHECK]:
        t1, t2 = _norm_both(d.get("title", ""))
        matched = sum(1 for w in qtok if (w in t1) or (w in t2))
        cond_min_words = (len(qtok) >= min_words and matched >= min_words)
        cond_cover = (matched / max(1, len(qtok))) >= min_cover
        if cond_min_words or cond_cover:
            return True
    return False

# (tu·ª≥ ch·ªçn) Alias ƒë·ªÉ t∆∞∆°ng th√≠ch n·∫øu tr∆∞·ªõc ƒë√¢y g·ªçi t√™n h√†m l√† "_"
_ = _has_title_overlap

# ========= BOOTSTRAP =========

APP_SECRET = os.getenv("FB_APP_SECRET", "")

# Cho ph√©p t·∫°m t·∫Øt verify ch·ªØ k√Ω khi test n·ªôi b·ªô (ƒë·∫∑t trong .env: DISABLE_FB_SIG_VERIFY=true)
DISABLE_FB_SIG_VERIFY = os.getenv("DISABLE_FB_SIG_VERIFY", "false").lower() == "true"


def _verify_fb_sig(req) -> bool:
    # N·∫øu t·∫Øt verify ho·∫∑c ch∆∞a c·∫•u h√¨nh APP_SECRET ‚Üí cho qua (ch·ªâ n√™n d√πng khi test)
    if DISABLE_FB_SIG_VERIFY or not APP_SECRET:
        return True

    sig256 = req.headers.get("X-Hub-Signature-256", "")
    sig1   = req.headers.get("X-Hub-Signature", "")
    raw = req.get_data()

    if sig256.startswith("sha256="):
        digest = hmac.new(APP_SECRET.encode(), raw, hashlib.sha256).hexdigest()
        return hmac.compare_digest("sha256=" + digest, sig256)
    if sig1.startswith("sha1="):
        digest = hmac.new(APP_SECRET.encode(), raw, hashlib.sha1).hexdigest()
        return hmac.compare_digest("sha1=" + digest, sig1)
    return False
#////////////////////
OPENAI_API_KEY   = os.getenv("OPENAI_API_KEY", "")
VERIFY_TOKEN     = os.getenv("VERIFY_TOKEN", "aloha_verify_123")
# ---- Multi-page map ----
# ---- Multi-id -> token (FB Page & IG) ----
TOKEN_MAP = {}

def _add_map(key, token):
    if key and token:
        TOKEN_MAP[str(key)] = token

for i in range(1, 11):
    pid = os.getenv(f"FB_PAGE_ID_{i}")
    ptk = os.getenv(f"FB_PAGE_TOKEN_{i}")
    _add_map(pid, ptk)  # map Page ID -> Page token

    igid = os.getenv(f"IG_ACCOUNT_ID_{i}")
    _add_map(igid, ptk) # map IG Account ID -> d√πng CHUNG Page token ƒë√£ li√™n k·∫øt IG

VECTOR_DIR       = os.getenv("VECTOR_DIR", "./vectors")
# Shopify
SHOPIFY_SHOP = os.getenv("SHOPIFY_STORE", "")  # domain *.myshopify.com (tham chi·∫øu)
# Link shop m·∫∑c ƒë·ªãnh (fallback)
SHOP_URL         = os.getenv("SHOP_URL", "https://shop.aloha.id.vn/zh")
# ƒêa ng√¥n ng·ªØ
SUPPORTED_LANGS  = [s.strip() for s in os.getenv("SUPPORTED_LANGS", "vi,en,zh,th,id,ko,ja").split(",")]

DEFAULT_LANG     = os.getenv("DEFAULT_LANG", "vi")
SHOP_URL_MAP = {
    "vi": os.getenv("SHOP_URL_VI", SHOP_URL),
    "en": os.getenv("SHOP_URL_EN", SHOP_URL),
    "zh": os.getenv("SHOP_URL_ZH", SHOP_URL),
    "th": os.getenv("SHOP_URL_TH", SHOP_URL),
    "id": os.getenv("SHOP_URL_ID", SHOP_URL),
    "ko": os.getenv("SHOP_URL_KO", SHOP_URL),
    "ja": os.getenv("SHOP_URL_JA", SHOP_URL),
}

# --- Always-answer & shop identity ---
ALWAYS_ANSWER = os.getenv("ALWAYS_ANSWER", "true").lower() == "true"
SHOP_NAME = os.getenv("SHOP_NAME", "Aloha")
SHOP_BRAND_TAGLINE = os.getenv("SHOP_BRAND_TAGLINE", "C·ª≠a h√†ng ph·ª• ki·ªán & lifestyle")

def shop_identity(lang: str):
    url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
    return (
        f"SHOP_IDENTITY:\n"
        f"- T√™n/Brand: {SHOP_NAME}\n"
        f"- Tagline: {SHOP_BRAND_TAGLINE}\n"
        f"- Website: {url}\n"
        f"- Ng√¥n ng·ªØ h·ªó tr·ª£: {', '.join(SUPPORTED_LANGS)}\n"
        f"- L∆∞u √Ω: Ch·ªâ n√™u GI√Å/T·ªíN KHO khi c√≥ trong CONTEXT; n·∫øu kh√¥ng c√≥ d·ªØ ki·ªán th√¨ xin th√™m th√¥ng tin ho·∫∑c d·∫´n link.\n"
    )


REPHRASE_ENABLED = os.getenv("REPHRASE_ENABLED", "true").lower() == "true"
EMOJI_MODE       = os.getenv("EMOJI_MODE", "cute")  # "cute" | "none"

# L·ªçc & ng∆∞·ª°ng ƒëi·ªÉm
SCORE_MIN = float(os.getenv("PRODUCT_SCORE_MIN", "0.34"))
STRICT_MATCH = os.getenv("STRICT_MATCH", "true").lower() == "true"

# ...
print("=== BOOT ===")
print("VECTOR_DIR:", os.path.abspath(VECTOR_DIR))
print("TOKEN_MAP size:", len(TOKEN_MAP))
print("OPENAI key set:", bool(OPENAI_API_KEY))
print("SUPPORTED_LANGS:", SUPPORTED_LANGS)

# OpenAI
OPENAI_URL   = "https://api.openai.com/v1/responses"
OPENAI_MODEL = "gpt-4o-mini"
EMBED_MODEL  = os.getenv("EMBED_MODEL", "text-embedding-3-small")

oai_client   = OpenAI(api_key=OPENAI_API_KEY)

# ========= SMALL IN-MEMORY SESSION =========
SESS = {}
SESS_LOCK = threading.RLock()
SESSION_TTL = 60 * 30  # 30 ph√∫t
SESS_MAX = int(os.getenv("SESS_MAX", "2000"))

def _purge_sessions():
    now = time.time()
    # xo√° h·∫øt session h·∫øt h·∫°n
    expired = [k for k,v in SESS.items() if now - v.get("ts",0) > SESSION_TTL]
    for k in expired:
        SESS.pop(k, None)
    # n·∫øu v·∫´n v∆∞·ª£t qu√° SESS_MAX ‚Üí LRU trim
    if len(SESS) > SESS_MAX:
        extra = len(SESS) - SESS_MAX
        for k,_ in sorted(SESS.items(), key=lambda kv: kv[1].get("ts",0))[:extra]:
            SESS.pop(k, None)

def _get_sess(user_id):
    now = time.time()
    with SESS_LOCK:
        s = SESS.get(user_id)
        if not s or now - s["ts"] > SESSION_TTL:
            s = {"hist": deque(maxlen=8), "last_mid": None, "ts": now}
            SESS[user_id] = s
        s["ts"] = now
        # ch·ªâ purge khi c·∫ßn ƒë·ªÉ ti·∫øt ki·ªám CPU
        if len(SESS) > SESS_MAX:
            _purge_sessions()
        return s
    
def _remember(user_id, role, text):
    with SESS_LOCK:
        s = _get_sess(user_id)
        s["hist"].append({"role": role, "content": text})




# ========= OPENAI WRAPPER =========
def _to_chat_messages(messages):
    """Chuy·ªÉn format responses -> chat.completions ƒë·ªÉ fallback."""
    chat_msgs = []
    ALLOWED = {"input_text", "output_text", "text"}  # <-- th√™m output_text
    for m in messages:
        role = m.get("role", "user")
        parts = m.get("content", [])
        text = "\n".join([p.get("text","") for p in parts if p.get("type") in ALLOWED]).strip()
        chat_msgs.append({"role": role, "content": text})
    return chat_msgs


def call_openai(messages, temperature=0.7):
    """
    ∆Øu ti√™n /v1/responses; n·∫øu l·ªói -> fallback /v1/chat/completions.
    messages: [{"role":..., "content":[{"type":"input_text","text":"..."}]}]
    """
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": OPENAI_MODEL, "input": messages, "temperature": temperature}
    try:
        t0 = time.time()
        r = requests.post(OPENAI_URL, headers=headers, json=payload, timeout=40)
        dt = (time.time() - t0) * 1000
        print(f"üîÅ OpenAI responses status={r.status_code} in {dt:.0f}ms")
        if r.status_code == 200:
            data = r.json()
            try:
                reply = data["output"][0]["content"][0]["text"]
            except Exception:
                reply = data.get("output_text") or "M√¨nh ƒëang ·ªü ƒë√¢y, s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!"
            return data, reply

        print(f"‚ùå responses body: {r.text[:800]}")
        # Fallback sang chat.completions
        chat_msgs = _to_chat_messages(messages)
        rc = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": chat_msgs, "temperature": temperature},
            timeout=40
        )
        print(f"üîÅ OpenAI chat.status={rc.status_code}")
        if rc.status_code == 200:
            data = rc.json()
            reply = (data.get("choices") or [{}])[0].get("message", {}).get("content") or "..."
            return data, reply

        print(f"‚ùå chat body: {rc.text[:800]}")
        return {}, "Xin l·ªói, hi·ªán m√¨nh g·∫∑p ch√∫t tr·ª•c tr·∫∑c. B·∫°n nh·∫Øn l·∫°i gi√∫p m√¨nh nh√©!"
    except Exception as e:
        print("‚ùå OpenAI error:", repr(e))
        return {}, "Xin l·ªói, hi·ªán m√¨nh g·∫∑p ch√∫t tr·ª•c tr·∫∑c. B·∫°n nh·∫Øn l·∫°i gi√∫p m√¨nh nh√©!"

# === Rephrase m·ªÅm + emoji cute ===
EMOJI_SETS = {
    "generic": ["‚ú®","üôÇ","üòä","üåü","üí´"],
    "greet":   ["üëã","üòä","üôÇ","‚ú®"],
    "browse":  ["üõçÔ∏è","üß≠","üîé","‚ú®"],
    "product": ["üõçÔ∏è","‚ú®","üëç","üíñ"],
    "oos":     ["üôè","‚õî","üòÖ","üõí"],
    "policy":  ["‚ÑπÔ∏è","üì¶","üõ°Ô∏è","‚úÖ"]
}
def em(intent="generic", n=1):
    if EMOJI_MODE == "none": return ""
    arr = EMOJI_SETS.get(intent, EMOJI_SETS["generic"])
    return " " + " ".join(random.choice(arr) for _ in range(max(1, n))).strip()

def rephrase_casual(text: str, intent="generic", temperature=0.7, lang: str = None) -> str:
    """L√†m m·ªÅm c√¢u + th√™m 1‚Äì2 emoji nh·∫π nh√†ng, ƒë√∫ng ng√¥n ng·ªØ lang."""
    if not REPHRASE_ENABLED:
        return text + (em(intent,1) if intent!="generic" else "")
    try:
        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
        msgs = [
            {"role":"system","content":f"B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng, vi·∫øt {lang or 'vi'} t·ª± nhi√™n, th√¢n thi·ªán, ng·∫Øn g·ªçn; th√™m 1‚Äì2 emoji ph√π h·ª£p (kh√¥ng l·∫°m d·ª•ng). Gi·ªØ nguy√™n d·ªØ ki·ªán/gi√°, kh√¥ng b·ªãa."},
            {"role":"user","content": f"Vi·∫øt l·∫°i ƒëo·∫°n sau b·∫±ng {lang or 'vi'} theo gi·ªçng th√¢n thi·ªán, k·∫øt th√∫c b·∫±ng 1 c√¢u ch·ªët h√†nh ƒë·ªông.\n---\n{text}\n---\n{em(intent,2)}"}
        ]
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": msgs, "temperature": temperature, "max_tokens": 220},
            timeout=20
        )
        if r.status_code == 200:
            data = r.json()
            out = (data.get("choices") or [{}])[0].get("message", {}).get("content")
            return out.strip() if out else text
        else:
            print("‚ö†Ô∏è rephrase status=", r.status_code, r.text[:200])
            return text + em(intent,1)
    except Exception as e:
        print("‚ö†Ô∏è rephrase error:", repr(e))
        return text + em(intent,1)
def handle_smalltalk(text: str, lang: str = "vi") -> str:
    # tr·∫£ l·ªùi ng·∫Øn g·ªçn, kh√¥ng g·ªçi rephrase ƒë·ªÉ tr√°nh th√™m CTA b√°n h√†ng
    raw = f"{t(lang, 'smalltalk_hi')} {t(lang, 'smalltalk_askback')}".strip()
    return raw


# ========= FACEBOOK SENDER =========
def fb_call(path, payload=None, method="POST", params=None, page_token=None):
    if not page_token:
        print("‚ùå missing page_token for fb_call")
        return None
    url = f"https://graph.facebook.com/v19.0{path}"
    params = params or {}
    params["access_token"] = page_token

    # appsecret_proof (recommended / required n·∫øu b·∫≠t)
    if APP_SECRET:
        try:
            proof = hmac.new(APP_SECRET.encode(), page_token.encode(), hashlib.sha256).hexdigest()
            params["appsecret_proof"] = proof
        except Exception as e:
            print("‚ö†Ô∏è cannot compute appsecret_proof:", repr(e))

    try:
        r = requests.request(method, url, params=params, json=payload, timeout=15)
        return r
    except Exception as e:
        print("‚ö†Ô∏è FB API error:", repr(e))
        return None

def fb_mark_seen(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"mark_seen"}, page_token=page_token)

def fb_typing_on(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"typing_on"}, page_token=page_token)

def fb_send_text(user_id, text, page_token):
    if not page_token:
        print("‚ùå missing page_token for fb_send_text")
        return
    msg = (text or "").strip()
    if len(msg) > 1900:  # Messenger khuy·∫øn ngh·ªã <= ~2000 k√Ω t·ª±
        msg = msg[:1900] + "‚Ä¶"
    payload = {
        "recipient": {"id": user_id},
        "messaging_type": "RESPONSE",
        "message": {"text": msg}
    }
    r = fb_call("/me/messages", payload, page_token=page_token)
    print(f"üì© Send text status={getattr(r,'status_code',None)}")

def fb_send_buttons(user_id, text, buttons, page_token):
    if not buttons: return
    payload = {
        "recipient": {"id": user_id},
        "messaging_type": "RESPONSE",
        "message": {
            "attachment": {
                "type": "template",
                "payload": {"template_type": "button", "text": text, "buttons": buttons[:3]}  # ‚Üê cho ph√©p 3 n√∫t
            }
        }
    }
    r = fb_call("/me/messages", payload, page_token=page_token)
    print(f"üîò ButtonAPI status={getattr(r,'status_code',None)} body={getattr(r,'text','')[:400]}")


# === Reload vectors (FAISS) ===
# === Reload vectors (FAISS) ===
CANONICAL_DOMAIN = os.getenv("CANONICAL_DOMAIN", SHOP_URL).rstrip("/")
ALIAS_DOMAINS = [d.strip().rstrip("/") for d in os.getenv("ALIAS_DOMAINS","").split(",") if d.strip()]

print("CANONICAL_DOMAIN:", CANONICAL_DOMAIN, "| ALIAS_DOMAINS:", ALIAS_DOMAINS)  # <-- move here


def _canon_url(u: str) -> str:
    if not u: return u
    uu = u.strip()
    for dom in ALIAS_DOMAINS:
        if uu.startswith(dom + "/") or uu == dom:
            return CANONICAL_DOMAIN + uu[len(dom):]
    return uu

def _apply_canonical_urls(meta):
    if not meta: return meta
    for d in meta:
        if d.get("url"):
            d["url"] = _canon_url(d["url"])
    return meta

# ========= RAG (FAISS) =========

def _safe_read_index(prefix):
    try:
        idx_path  = os.path.join(VECTOR_DIR, f"{prefix}.index")
        meta_path = os.path.join(VECTOR_DIR, f"{prefix}.meta.json")
        if not (os.path.exists(idx_path) and os.path.exists(meta_path)):
            print(f"‚ö†Ô∏è Missing index/meta for '{prefix}'")
            return None, None
        idx  = faiss.read_index(idx_path)
        meta = json.load(open(meta_path, encoding="utf-8"))

        # --- √Åp canonical domain cho m·ªçi URL trong meta ---
        meta = _apply_canonical_urls(meta)

        print(f"‚úÖ {prefix} loaded: {len(meta)} chunks")
        return idx, meta
    except Exception as e:
        print(f"‚ùå Load index '{prefix}':", repr(e))
        return None, None
    
IDX_PROD, META_PROD = _safe_read_index("products")
IDX_POL,  META_POL  = _safe_read_index("policies")

def _reload_vectors():
    global IDX_PROD, META_PROD, IDX_POL, META_POL
    try:
        IDX_PROD, META_PROD = _safe_read_index("products")
        IDX_POL,  META_POL  = _safe_read_index("policies")
        ok = (IDX_PROD is not None or IDX_POL is not None)
        print("üîÑ Reload vectors:", ok,
              "| prod_chunks=", (len(META_PROD) if META_PROD else 0),
              "| policy_chunks=", (len(META_POL) if META_POL else 0))
        return ok
    except Exception as e:
        print("‚ùå reload vectors:", repr(e))
        return False
# ==== Diagnostics: sizes & memory ====
import psutil, shutil

def _file_size_mb(path):
    try:
        return round(os.path.getsize(path) / 1024 / 1024, 2)
    except Exception:
        return 0.0

@app.route("/debug/rag_status")
def rag_status():
    products_index_path = os.path.join(VECTOR_DIR, "products.index")
    policies_index_path = os.path.join(VECTOR_DIR, "policies.index")
    return jsonify({
        "vector_dir": os.path.abspath(VECTOR_DIR),
        "products_index": bool(IDX_PROD),
        "products_chunks": len(META_PROD) if META_PROD else 0,
        "products_index_ntotal": int(getattr(IDX_PROD, "ntotal", 0)) if IDX_PROD else 0,
        "products_index_size_mb": _file_size_mb(products_index_path),
        "policies_index": bool(IDX_POL),
        "policies_chunks": len(META_POL) if META_POL else 0,
        "policies_index_ntotal": int(getattr(IDX_POL, "ntotal", 0)) if IDX_POL else 0,
        "policies_index_size_mb": _file_size_mb(policies_index_path),
        "sessions": len(SESS),
    })

@app.route("/debug/mem_status")
def mem_status():
    p = psutil.Process(os.getpid())
    rss_mb = p.memory_info().rss / 1024 / 1024
    vms_mb = p.memory_info().vms / 1024 / 1024
    return jsonify({
        "pid": p.pid,
        "rss_mb": round(rss_mb, 2),
        "vms_mb": round(vms_mb, 2),
    })

@app.route("/debug/disk_status")
def disk_status():
    total, used, free = shutil.disk_usage("/")
    to_mb = lambda x: round(x / 1024 / 1024, 2)
    return jsonify({
        "disk_total_mb": to_mb(total),
        "disk_used_mb": to_mb(used),
        "disk_free_mb": to_mb(free),
    })

# --- Admin auth (strip + ƒëa k√™nh + t·∫Øt t·∫°m th·ªùi) ---
ADMIN_TOKEN = (os.getenv("ADMIN_TOKEN", "") or "").strip()
DISABLE_ADMIN_AUTH = os.getenv("DISABLE_ADMIN_AUTH", "false").lower() == "true"

def _admin_ok(req):
    if DISABLE_ADMIN_AUTH:
        return True
    hdr = (req.headers.get("X-Admin-Token") or "").strip()
    qp  = (req.args.get("token") or "").strip()  # cho ph√©p ?token=...
    token = hdr or qp
    return (not ADMIN_TOKEN) or (token == ADMIN_TOKEN)

@app.route("/debug/product_coverage")
def product_coverage():
    pids, handles, vids = set(), set(), set()
    for d in (META_PROD or []):
        if d.get("id"):         pids.add(str(d["id"]))
        if d.get("handle"):     handles.add(d["handle"])
        if d.get("variant_id"): vids.add(str(d["variant_id"]))
    return jsonify({
        "chunks": len(META_PROD or []),
        "unique_products_by_id": len(pids),
        "unique_handles": len(handles),
        "unique_variants": len(vids),
    })


@app.post("/admin/reload_vectors")
def admin_reload_vectors():
    if not _admin_ok(request):
        return jsonify({"ok": False, "error": "unauthorized"}), 401
    ok = _reload_vectors()
    return jsonify({"ok": ok})

@app.post("/admin/rebuild_vectors_now")
def admin_rebuild_vectors_now():
    if not _admin_ok(request):
        return jsonify({"ok": False, "error": "unauthorized"}), 401

    try:
        t0 = time.time()
        products = fetch_all_products()
        docs = dedup_docs(build_docs(products))
        os.makedirs(VECTOR_DIR, exist_ok=True)

        idx_path  = os.path.join(VECTOR_DIR, "products.index")
        meta_path = os.path.join(VECTOR_DIR, "products.meta.json")
        idx_tmp   = idx_path  + ".tmp"
        meta_tmp  = meta_path + ".tmp"

        # Ghi file t·∫°m
        save_faiss(docs, idx_tmp, meta_tmp)

        # üëâ ƒê·∫æM S·ªê ‚ÄúTH·ª∞C S·ª∞‚Äù ƒê√É GHI
        try:
            saved_ntotal = int(getattr(faiss.read_index(idx_tmp), "ntotal", 0))
        except Exception:
            saved_ntotal = 0
        try:
            saved_meta = len(json.load(open(meta_tmp, encoding="utf-8")))
        except Exception:
            saved_meta = 0

        # Atomic swap
        os.replace(idx_tmp,  idx_path)
        os.replace(meta_tmp, meta_path)

        # Reload v√†o RAM
        ok = _reload_vectors()

        return jsonify({
            "ok": ok,
            "docs_after_dedup": len(docs),        # s·ªë doc sau build+dedup
            "saved_to_index": saved_ntotal,       # s·ªë vector th·ª±c ƒë√£ ghi
            "saved_meta": saved_meta,             # s·ªë item trong meta (n√™n = saved_to_index)
            "skipped": max(0, len(docs) - saved_ntotal),
            "t": round(time.time() - t0, 1)
        })
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

def _embed_query(q: str) -> Optional[np.ndarray]:
    try:
        t0 = time.time()
        resp = oai_client.embeddings.create(model=EMBED_MODEL, input=[q])
        v = np.array(resp.data[0].embedding, dtype="float32")[None, :]
        faiss.normalize_L2(v)
        print(f"üß© Embedding in {(time.time()-t0)*1000:.0f}ms")
        return v
    except Exception as e:
        print("‚ö†Ô∏è _embed_query error:", repr(e))
        return None



def search_products_with_scores(query, topk=8):
    if IDX_PROD is None:
        return [], []

    v = _embed_query(query)
    if v is None:  # >>> th√™m d√≤ng an to√†n
        return [], []

    try:
        D, I = IDX_PROD.search(v, topk)
        hits, scores, seen = [], [], set()
        for idx, score in zip(I[0], D[0]):
            if 0 <= idx < len(META_PROD):
                d = META_PROD[idx]
                key = (
                    d.get("url"),
                    (d.get("title") or "").lower().strip(),
                    str(d.get("variant_id") or d.get("sku") or d.get("variant") or "")
                    )
                if key in seen:
                    continue
                seen.add(key)
                hits.append(d)
                scores.append(float(score))
        print(f"üìö product hits: {len(hits)}")
        return hits, scores
    except Exception as e:
        print("‚ö†Ô∏è search_products_with_scores:", repr(e))
        return [], []
def retrieve_context(question, topk=6):
    if IDX_PROD is None and IDX_POL is None:
        return ""

    v = _embed_query(question)
    if v is None:  # >>> th√™m d√≤ng an to√†n
        return ""

    ctx = []
    if IDX_PROD is not None:
        try:
            _, Ip = IDX_PROD.search(v, topk)
            ctx += [META_PROD[i]["text"] for i in Ip[0] if i >= 0]
        except Exception as e:
            print("‚ö†Ô∏è search products:", repr(e))
    if IDX_POL is not None:
        try:
            _, Ik = IDX_POL.search(v, topk)
            ctx += [META_POL[i]["text"] for i in Ik[0] if i >= 0]
        except Exception as e:
            print("‚ö†Ô∏è search policies:", repr(e))
    print("üß† ctx pieces:", len(ctx))
    return "\n\n".join(ctx[:topk]) if ctx else ""
def _parse_ts(s):
    try:
        s = (s or "").replace("Z", "").replace("T", " ")
        return time.mktime(time.strptime(s[:19], "%Y-%m-%d %H:%M:%S"))
    except Exception:
        return 0

def get_new_arrivals(days=30, topk=4):
    """T√¨m sp m·ªõi theo timestamp/tags 'new|m·ªõi|v·ª´a v·ªÅ'; fallback FAISS n·∫øu tr·ªëng."""
    if not META_PROD:
        return []
    now = time.time()
    cutoff = now - days*86400
    new_items = []
    for d in META_PROD:
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            if d.get(k):
                ts = max(ts, _parse_ts(d.get(k)))
        tags = (d.get("tags","") or "").lower()
        flag_new = any(x in tags for x in ["new","m·ªõi","v·ª´a v·ªÅ","new arrivals"])
        if flag_new or (ts and ts >= cutoff):
            new_items.append(d)

    if not new_items and IDX_PROD is not None:
        hits, _ = search_products_with_scores("new arrivals h√†ng m·ªõi v·ª´a v·ªÅ", topk=topk*2)
        new_items = hits

    def _key(d):
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            ts = max(ts, _parse_ts(d.get(k)))
        return ts
    new_items.sort(key=_key, reverse=True)
    return new_items[:topk]

def compose_new_arrivals(lang: str = "vi", items=None):
    items = items or []
    if not items:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return rephrase_casual(t(lang, "browse", url=url), intent="browse", lang=lang)

    lines = []
    for d in items[:2]:
        title = d.get("title") or "S·∫£n ph·∫©m"
        stock = _stock_line(d)
        pval  = _price_value(d)
        currency = d.get("currency") or ("‚Ç´" if lang == "vi" else "")
        pstr  = _fmt_price(pval, currency) if pval is not None else None

        line = f"‚Ä¢ {title}"
        if pstr: line += f" ‚Äî {pstr}"
        line += f" ‚Äî {stock}"
        lines.append(line)

    raw = f"{t(lang,'new_hdr')}\n" + "\n".join(lines) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)



# ========= INTENT, PERSONA, FEW-SHOT & NATURAL REPLY =========
GREETS = {"hi","hello","hey","helo","heloo","h√≠","h√¨","ch√†o","xin ch√†o","alo","aloha","hello bot","hi bot"}
def is_greeting(text: str) -> bool:
    t = re.sub(r"\s+", " ", (text or "").lower()).strip()
    return any(w in t for w in GREETS) and len(t) <= 40

# ‚Äî‚Äî‚Äî Ng√¥n ng·ªØ: detect & c√¢u ch·ªØ
def detect_lang(text: str) -> str:
    txt = (text or "").strip()
    if not txt: return DEFAULT_LANG
    if re.search(r"[\u4e00-\u9fff]", txt):  # CJK
        return "zh" if "zh" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[\u0E00-\u0E7F]", txt):  # Thai
        return "th" if "th" in SUPPORTED_LANGS else DEFAULT_LANG
        # Korean Hangul
    if re.search(r"[\uac00-\ud7af]", txt):  # Hangul syllables
        return "ko" if "ko" in SUPPORTED_LANGS else DEFAULT_LANG
    # Japanese (Hiragana + Katakana)
    if re.search(r"[\u3040-\u30ff\u31f0-\u31ff]", txt):
        return "ja" if "ja" in SUPPORTED_LANGS else DEFAULT_LANG

    if re.search(r"[ƒÉ√¢√™√¥∆°∆∞ƒë√°√†·∫£√£·∫°·∫Ø·∫±·∫≥·∫µ·∫∑·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç·ªë·ªì·ªï·ªó·ªô∆°√≥·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ]", txt, flags=re.I):
        return "vi" if "vi" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"\b(yang|dan|tidak|saja|terima|kasih)\b", txt.lower()):
        return "id" if "id" in SUPPORTED_LANGS else DEFAULT_LANG
    return "en" if "en" in SUPPORTED_LANGS else DEFAULT_LANG


# ====== MULTI-LANG PATTERNS (smalltalk & new arrivals) ======
# M·ªói ng√¥n ng·ªØ l√† 1 list regex. C√≥ th·ªÉ b·ªï sung d·∫ßn m√† kh√¥ng ƒë·ª•ng ch·ªó kh√°c.
# ==== Smalltalk & New arrivals (multi-lang) ====

# ========= I18N STRINGS & HELPERS =========
LANG_STRINGS = {
    "vi": {
        "greet": "Xin ch√†o üëã R·∫•t vui ƒë∆∞·ª£c ph·ª•c v·ª• b·∫°n! B·∫°n mu·ªën m√¨nh gi√∫p g√¨ kh√¥ng n√®? üôÇ",
        "browse": "M·ªùi b·∫°n v√†o web tham quan ·∫° üõçÔ∏è üëâ {url}",
        "oos": "Xin l·ªói üôè s·∫£n ph·∫©m ƒë√≥ hi·ªán **ƒëang h·∫øt h√†ng** t·∫°i shop. B·∫°n th·ª≠ xem c√°c m·∫´u t∆∞∆°ng t·ª± tr√™n web nh√© üëâ {url} ‚ú®",
        "fallback": "M√¨nh ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ch·∫Øc ch·∫Øn ü§î. B·∫°n m√¥ t·∫£ th√™m m·∫´u/ki·ªÉu d√°ng/ch·∫•t li·ªáu ƒë·ªÉ m√¨nh t∆∞ v·∫•n chu·∫©n h∆°n nha ‚ú®",
        "suggest_hdr": "M√¨nh ƒë·ªÅ xu·∫•t v√†i l·ª±a ch·ªçn ph√π h·ª£p",
        "product_pts": "B·∫°n th√≠ch ki·ªÉu m·∫£nh hay th·ªÉ thao? M√¨nh l·ªçc th√™m m√†u & size gi√∫p b·∫°n nh√©.",
        "highlights": "{title} c√≥ v√†i ƒëi·ªÉm n·ªïi b·∫≠t n√®",
        "policy_hint": "Theo ch√≠nh s√°ch shop:",
        "smalltalk_hi": "Hi üëã M√¨nh kh·ªèe n√® üòÑ",
        "smalltalk_askback": "H√¥m nay c·ªßa b·∫°n th·∫ø n√†o?",
        "new_hdr": "H√†ng m·ªõi v·ªÅ n√® ‚ú®",
        "btn_view": "Xem s·∫£n ph·∫©m",
        "quick_view": "Xem nhanh:",

    },
    "en": {
        "greet": "Hello üëã Happy to help! How can I assist you today? üôÇ",
        "browse": "Feel free to explore our store üõçÔ∏è üëâ {url}",
        "oos": "Sorry üôè that item is **out of stock** right now. Check similar picks here üëâ {url} ‚ú®",
        "fallback": "I‚Äôm missing a bit of info ü§î. Share style/material/size and I‚Äôll refine the picks ‚ú®",
        "suggest_hdr": "Here are a few good options",
        "product_pts": "Prefer a slim or sporty style? I can filter color & size for you.",
        "highlights": "{title} highlights",
        "policy_hint": "Store policy:",
         "smalltalk_hi": "Hi üëã I'm good! üòÑ",
        "smalltalk_askback": "How's your day going?",
        "new_hdr": "New arrivals ‚ú®",
        "btn_view": "View product",
        "quick_view": "Quick view:",
    },
    "zh": {
        "greet": "‰Ω†Â•Ω üëã ÂæàÈ´òÂÖ¥‰∏∫‰Ω†ÊúçÂä°ÔºÅÈúÄË¶ÅÊàëÂ∏Æ‰Ω†ÂÅö‰ªÄ‰πàÂë¢ÔºüüôÇ",
        "browse": "Ê¨¢ËøéÈÄõÈÄõÊàë‰ª¨ÁöÑÂïÜÂ∫ó üõçÔ∏è üëâ {url}",
        "oos": "Êä±Ê≠â üôè ËØ•ÂïÜÂìÅÁõÆÂâç**Áº∫Ë¥ß**„ÄÇÂèØ‰ª•ÂÖàÁúãÁúãÁ±ª‰ººÁöÑÊ¨æÂºè üëâ {url} ‚ú®",
        "fallback": "ËøòÈúÄË¶Å‰∏Ä‰∫õ‰ø°ÊÅØÂì¶ ü§î„ÄÇËØ¥‰∏ãÈ£éÊ†º/ÊùêË¥®/Â∞∫ÂØ∏ÔºåÊàëÂÜçÁ≤æÂáÜÊé®Ëçê ‚ú®",
        "suggest_hdr": "Áªô‰Ω†Âá†Ê¨æÂêàÈÄÇÁöÑÈÄâÊã©",
        "product_pts": "ÊÉ≥Ë¶ÅÁ∫§ÁªÜËøòÊòØËøêÂä®È£éÔºüÊàëÂèØ‰ª•ÊåâÈ¢úËâ≤ÂíåÂ∞∫Á†ÅÂÜçÁ≠õ‰∏ÄËΩÆ„ÄÇ",
        "highlights": "{title} ÁöÑ‰∫ÆÁÇπ",
        "policy_hint": "Â∫óÈì∫ÊîøÁ≠ñÔºö",
        "smalltalk_hi": "Âó® üëã ÊàëÂæàÂ•ΩÂñî üòÑ",
        "smalltalk_askback": "‰Ω†‰ªäÂ§©ËøáÂæóÊÄé‰πàÊ†∑Ôºü",
        "new_hdr": "Êñ∞ÂìÅ‰∏äÊû∂ ‚ú®",
        "btn_view": "Êü•ÁúãÂïÜÂìÅ",
        "quick_view": "Âø´ÈÄüÊü•ÁúãÔºö",
    },
    "th": {
        "greet": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ üëã ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á üôÇ",
        "browse": "‡πÄ‡∏ä‡∏¥‡∏ç‡∏ä‡∏°‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ üõçÔ∏è üëâ {url}",
        "oos": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ üôè ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ä‡∏¥‡πâ‡∏ô‡∏ô‡∏±‡πâ‡∏ô **‡∏´‡∏°‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß** ‡∏Ñ‡πà‡∏∞ ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà üëâ {url} ‚ú®",
        "fallback": "‡∏Ç‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î‡∏ô‡∏∞‡∏Ñ‡∏∞/‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô‡∏™‡πÑ‡∏ï‡∏•‡πå/‡∏ß‡∏±‡∏™‡∏î‡∏∏/‡∏Ç‡∏ô‡∏≤‡∏î ‚ú®",
        "suggest_hdr": "‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°",
        "product_pts": "‡∏ä‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏£‡∏µ‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï‡∏î‡∏µ? ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏±‡∏î‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÑ‡∏ã‡∏ã‡πå‡πÉ‡∏´‡πâ‡∏≠‡∏µ‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏∞/‡∏Ñ‡∏£‡∏±‡∏ö",
        "highlights": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á {title}",
        "policy_hint": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡πâ‡∏≤‡∏ô:",
         "smalltalk_hi": "‡πÑ‡∏Æ üëã ‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢‡∏ô‡∏∞ üòÑ",
        "smalltalk_askback": "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á?",
        "new_hdr": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà ‚ú®",
        "btn_view": "‡∏î‡∏π‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤",
        "quick_view": "‡∏î‡∏π‡∏î‡πà‡∏ß‡∏ô:",
    },
    "id": {
        "greet": "Halo üëã Senang membantu! Ada yang bisa saya bantu? üôÇ",
        "browse": "Silakan jelajahi toko kami üõçÔ∏è üëâ {url}",
        "oos": "Maaf üôè produk itu **sedang kosong**. Coba lihat yang mirip di sini üëâ {url} ‚ú®",
        "fallback": "Butuh info tambahan ü§î. Sebutkan gaya/bahan/ukuran ya, biar saya saringkan ‚ú®",
        "suggest_hdr": "Beberapa pilihan yang cocok",
        "product_pts": "Suka model tipis atau sporty? Saya bisa saring warna & ukuran.",
        "highlights": "Hal menarik dari {title}",
        "policy_hint": "Kebijakan toko:",
       "smalltalk_hi": "Hai üëã Aku baik-baik saja üòÑ",
        "smalltalk_askback": "Harinya kamu gimana?",
        "new_hdr": "Produk baru ‚ú®",
        "btn_view": "Lihat produk",
        "quick_view": "Lihat cepat:",
    },
        "ko": {
        "greet": "ÏïàÎÖïÌïòÏÑ∏Ïöî üëã Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî? üôÇ",
        "browse": "Ïä§ÌÜ†Ïñ¥Î•º ÎëòÎü¨Î≥¥ÏÑ∏Ïöî üõçÔ∏è üëâ {url}",
        "oos": "Ï£ÑÏÜ°Ìï¥Ïöî üôè Ìï¥Îãπ ÏÉÅÌíàÏùÄ ÌòÑÏû¨ **ÌíàÏ†à**ÏûÖÎãàÎã§. ÎπÑÏä∑Ìïú ÏÉÅÌíàÏùÑ Ïó¨Í∏∞ÏÑú ÌôïÏù∏Ìï¥ Î≥¥ÏÑ∏Ïöî üëâ {url} ‚ú®",
        "fallback": "Ï†ïÎ≥¥Í∞Ä Ï°∞Í∏à Î∂ÄÏ°±Ìï¥Ïöî ü§î. ÏõêÌïòÎäî Ïä§ÌÉÄÏùº/Ïû¨Ïßà/ÏÇ¨Ïù¥Ï¶àÎ•º ÏïåÎ†§Ï£ºÏãúÎ©¥ Îçî Ï†ïÌôïÌûà Ï∂îÏ≤úÌï¥ ÎìúÎ¶¥Í≤åÏöî ‚ú®",
        "suggest_hdr": "Ïù¥Îü∞ ÏòµÏÖòÏùÑ Ï∂îÏ≤úÎìúÎ†§Ïöî",
        "product_pts": "Ïä¨Î¶º/Ïä§Ìè¨Ìã∞ Ï§ë Ïñ¥Îñ§ Ïä§ÌÉÄÏùºÏù¥ Ï¢ãÏúºÏÑ∏Ïöî? ÏÉâÏÉÅ/ÏÇ¨Ïù¥Ï¶àÎèÑ Í≥®ÎùºÎìúÎ¶¥Í≤åÏöî.",
        "highlights": "{title} Ï£ºÏöî Ìè¨Ïù∏Ìä∏",
        "policy_hint": "Ïä§ÌÜ†Ïñ¥ Ï†ïÏ±Ö:",
        "smalltalk_hi": "ÏïàÎÖïÌïòÏÑ∏Ïöî üëã Ïûò ÏßÄÎÇ¥Í≥† ÏûàÏñ¥Ïöî üòÑ",
        "smalltalk_askback": "Ïò§Îäò ÌïòÎ£®Îäî Ïñ¥Îñ†ÏÑ∏Ïöî?",
        "new_hdr": "Ïã†ÏÉÅÌíà ‚ú®",
        "btn_view": "ÏÉÅÌíà Î≥¥Í∏∞",
        "quick_view": "Îπ†Î•∏ Î≥¥Í∏∞:",
    },
    "ja": {
        "greet": "„Åì„Çì„Å´„Å°„ÅØ üëã ‰Ωï„Çí„ÅäÊâã‰ºù„ÅÑ„Åß„Åç„Åæ„Åô„ÅãÔºü üôÇ",
        "browse": "„Çπ„Éà„Ç¢„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ üõçÔ∏è üëâ {url}",
        "oos": "„Åô„Åø„Åæ„Åõ„Çì üôè „Åù„ÅÆÂïÜÂìÅ„ÅØÁèæÂú®**Âú®Â∫´Âàá„Çå**„Åß„Åô„ÄÇ„Åì„Å°„Çâ„Åã„Çâ‰ºº„ÅüÂïÜÂìÅ„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ üëâ {url} ‚ú®",
        "fallback": "„ÇÇ„ÅÜÂ∞ë„ÅóÊÉÖÂ†±„ÅåÂøÖË¶Å„Åß„Åô ü§î„ÄÇ„Çπ„Çø„Ç§„É´/Á¥†Êùê/„Çµ„Ç§„Ç∫„ÇíÊïô„Åà„Å¶„ÅÑ„Åü„Å†„Åë„Çå„Å∞„ÄÅ„Çà„ÇäÊ≠£Á¢∫„Å´„ÅîÊèêÊ°à„Åó„Åæ„Åô ‚ú®",
        "suggest_hdr": "„Åì„Å°„Çâ„ÅÆ„Ç™„Éó„Ç∑„Éß„É≥„Åå„Åä„Åô„Åô„ÇÅ„Åß„Åô",
        "product_pts": "„Çπ„É™„É† or „Çπ„Éù„Éº„ÉÜ„Ç£„ÄÅ„Å©„Å°„Çâ„ÅåÂ•Ω„Åø„Åß„Åô„ÅãÔºüËâ≤„Éª„Çµ„Ç§„Ç∫„ÅÆÁµû„ÇäËæº„Åø„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇ",
        "highlights": "{title} „ÅÆ„Éù„Ç§„É≥„Éà",
        "policy_hint": "„Çπ„Éà„Ç¢„Éù„É™„Ç∑„ÉºÔºö",
        "smalltalk_hi": "„Åì„Çì„Å´„Å°„ÅØ üëã ÂÖÉÊ∞ó„Åß„Åô üòÑ",
        "smalltalk_askback": "‰ªäÊó•„ÅØ„Å©„Çì„Å™‰∏ÄÊó•„Åß„Åô„ÅãÔºü",
        "new_hdr": "Êñ∞ÁùÄ„Ç¢„Ç§„ÉÜ„É† ‚ú®",
        "btn_view": "ÂïÜÂìÅ„ÇíË¶ã„Çã",
        "quick_view": "„ÇØ„Ç§„ÉÉ„ÇØ„Éì„É•„ÉºÔºö",
    },

}


def t(lang: str, key: str, **kw) -> str:
    lang2 = lang if lang in LANG_STRINGS else DEFAULT_LANG
    s = (LANG_STRINGS.get(lang2, {}).get(key)
         or LANG_STRINGS.get(DEFAULT_LANG, {}).get(key)
         or "")
    try:
        return s.format(**kw)
    except Exception:
        return s


def greet_text(lang: str) -> str:
    return t(lang, "greet")

# ==== Smalltalk & New arrivals (multi-lang) ====

SMALLTALK_PATTERNS = {
    "vi": [
        # bn/b·∫°n kh·ªèe/kh·ªèe/kho·∫ª kh√¥ng/ko/h√¥ng/hem/hok
        r"\b(bn|b·∫°n)\s*(kh[o√≥√≤·ªè√µ·ªç∆°·ªõ·ªù·ªü·ª°·ª£]e|khoe|kho·∫ª|kh·ªèe)\s*(kh[o√¥]ng|ko|k|h[o∆°√¥√≥√≤√µ·ªè·ªç]ng|hong|h√¥ng|hem|hok)\b",
        r"\b(kh[o√≥√≤·ªè√µ·ªç∆°·ªõ·ªù·ªü·ª°·ª£]e|khoe|kho·∫ª|kh·ªèe)\s*(kh[o√¥]ng|ko|k|h[o∆°√¥√≥√≤√µ·ªè·ªç]ng|hong|h√¥ng|hem|hok)\b",
        # ·ªïn kh√¥ng
        r"\b(·ªïn|on)\s*(kh[o√¥]ng|ko|k|hong|h[o∆°]ng|hem|hok)\b",
        # h√¥m nay th·∫ø n√†o / nay sao
        r"\b(h[o√¥]m?\s*nay|nay)\s*(b·∫°n|bn)?\s*(th[·∫øe]\s*n[a√†]o|sao|ok\s*kh[o√¥]ng)\b",
        # ƒëang l√†m g√¨ / d·∫°o n√†y
        r"\b(ƒëang\s*l√†m\s*g√¨|l√†m\s*g√¨( v·∫≠y| ƒë√≥)?|l√†m\s*chi|lam\s*gi)\b",
        r"\b(d·∫°o\s*n√†y|dao\s*nay)\b",
        # ƒÉn c∆°m ch∆∞a / ng·ªß ch∆∞a
        r"\b(ƒÉn\s*c∆°m\s*ch∆∞a|ƒÉn\s*ch∆∞a|an\s*chua|u·ªëng\s*ch∆∞a|ng[u∆∞]\s*ch[aƒÉ]u?)\b",
        # c·∫£m ∆°n / thanks
        r"\b(c[·∫£a]m\s?∆°n|c[√°a]m\s?∆°n|thanks?|thank you|ty|tks|thx)\b",
        # c∆∞·ªùi/emoji
        r"\b(haha+|hihi+|hehe+|kkk+|=D|:d|:v|:3)\b|[üòÇü§£üòÜ]",
    ],
    "en": [
        r"\b(how('?s)?\s*it\s*going|how\s*are\s*(you|u)|how\s*r\s*u|how\s*u\s*doin?g?)\b",
        r"\b(what('?s)?\s*up|wass?up|sup|wyd)\b",
        r"\b(have\s*you\s*eaten|had\s*(lunch|dinner)|grabbed\s*(lunch|food))\b",
        r"\b(thanks?|thank\s*(you|u)|ty|thx|tysm|tks)\b",
        r"\b(lol|lmao|rofl|haha+|hehe+|:d)\b|[üòÇü§£üòÜ]",
    ],
    "zh": [
        r"(‰Ω†Â•ΩÂêó|Â¶≥Â•ΩÂêó|ÊúÄËøëÊÄé‰πàÊ†∑|ÊúÄËøëÂ¶Ç‰Ωï|ÊúÄËøëËøòÂ•Ω|ËøòÂ•ΩÂêó|ÂøÉÊÉÖÂ¶Ç‰Ωï|ÂºÄÂøÉÂêó|ÈÅéÂæóÊÄéÊ®£|ËøáÂæóÊÄéÊ†∑)",
        r"(ÂêÉÈ•≠‰∫ÜÂêó|ÂêÉËøáÈ•≠Ê≤°|ÂêÉ‰∫ÜÊ≤°|ÂêÉ‰∫ÜÂêó)",
        r"(Ë∞¢Ë∞¢|Â§öË∞¢|Ë¨ùË¨ù|ÊÑüË¨ù|ÊÑüË∞¢|Ë¨ùÂï¶|Ë∞¢Ë∞¢Âï¶|Ë∞¢Âï¶)",
        r"(ÂìàÂìà+|ÂòøÂòø+|ÂëµÂëµ+|Âó®Âó®+)|[üòÇü§£üòÜ]",
    ],
    "th": [
        r"(‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ(‡πÑ‡∏´‡∏°|‡∏°‡∏±‡πâ‡∏¢|‡∏õ‡πà‡∏≤‡∏ß)|‡πÄ‡∏õ‡πá‡∏ô(‡πÑ‡∏á|‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£)‡∏ö‡πâ‡∏≤‡∏á|‡πÇ‡∏≠‡πÄ‡∏Ñ(‡πÑ‡∏´‡∏°|‡∏°‡∏±‡πâ‡∏¢))",
        r"(‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà|‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£|‡∏ó‡∏≥‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà)",
        r"(‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß(‡∏´‡∏£‡∏∑‡∏≠)?‡∏¢‡∏±‡∏á|‡∏ó‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß(‡∏´‡∏£‡∏∑‡∏≠)?‡∏¢‡∏±‡∏á)",
        r"(‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì(‡∏Ñ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏∞)?|‡∏Ç‡∏≠‡∏ö‡πÉ‡∏à|thanks?|thank you|ty)",
        r"(‡∏Æ‡πà‡∏≤+‡πÜ+|555+)|[üòÇü§£üòÜ]",
    ],
    "id": [
        r"(apa\s*kabar|gimana\s*kabarnya|gmn\s*kabar|kabarnya\s*gimana)",
        r"(lagi\s*apa|lg\s*apa|sedang\s*apa|ngapain(\s*nih)?)",
        r"(sudah|udah)\s*makan\s*(belum|blm)",
        r"(terima\s*kasih|terimakasih|trimakasih|makasih|makasi|thanks?|thank you|thx|ty)",
        r"(wkwk+|wk+|haha+|hehe+|:d)|[üòÇü§£üòÜ]",
    ],
        "ko": [
        r"(ÏïàÎÖï|ÏïàÎÖïÌïòÏÑ∏Ïöî|ÌïòÏù¥|Ìó¨Î°ú)",
        r"(ÏöîÏ¶ò Ïñ¥Îïå|Ïûò ÏßÄÎÇ¥|Í∏∞Î∂Ñ Ïñ¥Îïå)",
        r"(Í≥†ÎßàÏõå|Í∞êÏÇ¨|Îï°ÌÅê|thanks?|thank you|thx|ty)",
        r"(„Öã„Öã+|„Öé„Öé+|ÌïòÌïò+)|[üòÇü§£üòÜ]",
    ],
    "ja": [
        r"(„Åì„Çì„Å´„Å°„ÅØ|„Åì„Çì„Å°„ÅØ|„ÇÇ„Åó„ÇÇ„Åó|„ÇÑ„ÅÇ|„Éè„É≠„Éº)",
        r"(ÂÖÉÊ∞ó„Åß„Åô„Åã|Ë™øÂ≠ê„Å©„ÅÜ|ÊúÄËøë„Å©„ÅÜ)",
        r"(„ÅÇ„Çä„Åå„Å®„ÅÜ|ÊúâÈõ£„ÅÜ|„Çµ„É≥„Ç≠„É•„Éº|thanks?|thank you|thx|ty)",
        r"(Á¨ë|ÔΩóÔΩó+|„ÅØ„ÅØ+)|[üòÇü§£üòÜ]",
    ],

}

NEW_ITEMS_PATTERNS = {
    "vi": [
        r"(h√†ng|sp|m·∫´u|s·∫£n\s*ph·∫©m).*(m·ªõi|v·ª´a\s*v·ªÅ|new\s*arrivals)",
        r"(c√≥|ƒë√£).*(m·∫´u|s·∫£n\s*ph·∫©m).*(m·ªõi|v·ª´a\s*v·ªÅ)",
        r"\b(new|m·ªõi|v·ª´a v·ªÅ|new arrivals)\b",
    ],
    "en": [
        r"(new\s*arrivals?|new\s*products?|what's\s*new)",
        r"(any|have).*(new\s*items?)",
    ],
    "zh": [
        r"(Êñ∞ÂìÅ|Êñ∞Âà∞|Êñ∞Ë≤®|Êñ∞Ë¥ß)",
        r"(Êúâ.*Êñ∞(ÂìÅ|Ë¥ß|Ë≤®)|‰æÜ‰∫Ü.*Êñ∞|Êù•‰∫Ü.*Êñ∞)",
    ],
    "th": [
        r"(‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà|‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà|‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà|‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà)",
        r"(‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡∏°‡πà|‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏´‡∏°)",
    ],
    "id": [
        r"(produk baru|barang baru|baru datang)",
        r"(ada yang baru|ada produk baru)",
    ],
}

def _pat(pats: dict, lang: str):
    """L·∫•y list pattern theo ng√¥n ng·ªØ, fallback v·ªÅ DEFAULT_LANG n·∫øu kh√¥ng c√≥."""
    return pats.get(lang) or pats.get(DEFAULT_LANG, [])
# ===== Gi√° / Price questions (multi-lang) =====
PRICE_PATTERNS = {
    "vi": [r"\bgi√°\b", r"bao nhi√™u", r"nhi√™u ti·ªÅn", r"\bgi√° bao nhi√™u\b", r"\bbao nhieu\b"],
    "en": [r"\bprice\b", r"how much", r"\bcost\b"],
    "zh": [r"(‰ª∑Ê†º|ÂπæÈå¢|Â§öÂ∞ëÈí±|Â§öÂ∞ëÈå¢)"],
    "th": [r"(‡∏£‡∏≤‡∏Ñ‡∏≤|‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà|‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£)"],
    "id": [r"(harga|berapa)"],
}
def is_price_question(text: str, lang: str) -> bool:
    raw = (text or "")
    return any(re.search(p, raw, flags=re.I) for p in _pat(PRICE_PATTERNS, lang))


SYSTEM_STYLE = (
    "B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng Aloha t√™n l√† Aloha Bot. T√¥ng gi·ªçng: th√¢n thi·ªán, ch·ªß ƒë·ªông, "
    "tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ ng∆∞·ªùi th·∫≠t; d√πng 1‚Äì3 emoji h·ª£p ng·ªØ c·∫£nh (kh√¥ng l·∫°m d·ª•ng). "
    "Lu√¥n d·ª±a v√†o CONTEXT (n·ªôi dung RAG). Kh√¥ng b·ªãa. "
    "KH√îNG ƒë∆∞·ª£c n√™u gi√°/t·ªìn kho/thu·ªôc t√≠nh c·ª• th·ªÉ n·∫øu CONTEXT kh√¥ng c√≥ d·ªØ ki·ªán; "
    "khi thi·∫øu d·ªØ ki·ªán th√¨ h·ªèi l·∫°i 1 c√¢u l√†m r√µ ho·∫∑c m·ªùi xem link c·ª≠a h√†ng. "
    "Tr√¨nh b√†y d·ªÖ ƒë·ªçc: g·∫°ch ƒë·∫ßu d√≤ng khi li·ªát k√™; 1 c√¢u ch·ªët h√†nh ƒë·ªông."
)
# FEW_SHOT_EXAMPLES
FEW_SHOT_EXAMPLES = [
    {"role":"user","content":[{"type":"input_text","text":"helo"}]},
    {"role":"assistant","content":[{"type":"output_text","text":"Xin ch√†o üëã R·∫•t vui ƒë∆∞·ª£c ph·ª•c v·ª• b·∫°n! B·∫°n mu·ªën m√¨nh gi√∫p g√¨ kh√¥ng n√®? üôÇ"}]},
    {"role":"user","content":[{"type":"input_text","text":"shop b·∫°n c√≥ nh·ªØng g√¨"}]},
    {"role":"assistant","content":[{"type":"output_text","text":f"M·ªùi b·∫°n tham quan c·ª≠a h√†ng t·∫°i ƒë√¢y ·∫° üõçÔ∏è üëâ {SHOP_URL_MAP.get('vi', SHOP_URL)}"}]},
]
# ---- Intent routing ----
POLICY_KEYWORDS  = {"ch√≠nh s√°ch","ƒë·ªïi tr·∫£","b·∫£o h√†nh","ship","v·∫≠n chuy·ªÉn","giao h√†ng","tr·∫£ h√†ng","refund"}
PRODUCT_KEYWORDS = {
    "mua","b√°n","gi√°","size","k√≠ch th∆∞·ªõc","ch·∫•t li·ªáu","m√†u","h·ª£p","ph√π h·ª£p",
    "d√¢y","ƒë·ªìng h·ªì","v√≤ng","case","√°o","qu·∫ßn","√°o ph√¥ng","tshirt","t-shirt","√°o thun",
    "s·∫£n ph·∫©m","sp","b√°nh","crepe","b√°nh crepe","b√°nh s·∫ßu ri√™ng","milktea","tr√† s·ªØa"
}
BROWSE_KEYWORDS  = {"c√≥ nh·ªØng g√¨","b√°n g√¨","c√≥ g√¨","danh m·ª•c","catalog","xem h√†ng","tham quan","xem shop","xem s·∫£n ph·∫©m","shop c√≥ g√¨","nh·ªØng s·∫£n ph·∫©m g√¨"}
_BROWSE_PATTERNS = [
    r"(shop|b√™n b·∫°n|b√™n m√¨nh).*(b√°n|c√≥).*(g√¨|nh·ªØng g√¨|nh·ªØng s·∫£n ph·∫©m g√¨)",
    r"(b√°n|c√≥).*(nh·ªØng\s+)?s·∫£n ph·∫©m g√¨",
]
# ==== Smalltalk & New arrivals ====



def detect_intent(text: str):
    raw = (text or "")
    t0  = re.sub(r"\s+", " ", raw.lower()).strip()
    lang = detect_lang(raw)

    if any(k in t0 for k in POLICY_KEYWORDS):  return "policy"
    if is_greeting(raw):                       return "greet"

    # smalltalk ƒëa ng√¥n ng·ªØ
    if any(re.search(p, raw, flags=re.I) for p in _pat(SMALLTALK_PATTERNS, lang)):
        return "smalltalk"

    # browse: t·ª´ kh√≥a + pattern chung
    if any(k in t0 for k in BROWSE_KEYWORDS):  return "browse"
    if any(re.search(p, t0, flags=re.I) for p in _BROWSE_PATTERNS):
        return "browse"


    # h·ªèi h√†ng m·ªõi ƒëa ng√¥n ng·ªØ
    if any(re.search(p, raw, flags=re.I) for p in _pat(NEW_ITEMS_PATTERNS, lang)):
        return "new_items"
    # H·ªèi gi√° ‚Üí ∆∞u ti√™n product_info
    if is_price_question(raw, lang):
        return "product_info"

    # s·∫£n ph·∫©m & m√¥ t·∫£
    if any(k in t0 for k in PRODUCT_KEYWORDS): return "product"
    if "c√≥ b√°n" in t0 or "b√°n kh√¥ng" in t0 or "b√°n ko" in t0: return "product"
    if "c√≥ g√¨ ƒë·∫∑c bi·ªát" in t0 or "ƒëi·ªÉm ƒë·∫∑c bi·ªát" in t0 or "c√≥ g√¨ ƒë·∫∑t bi·ªát" in t0: return "product_info"

    return "other"

def build_messages(system, history, context, user_question):
    msgs = [{"role":"system","content":[{"type":"input_text","text":system}]}]
    msgs.extend(FEW_SHOT_EXAMPLES)
    for h in list(history)[-3:]:
        ctype = "output_text" if h["role"] == "assistant" else "input_text"
        msgs.append({"role": h["role"], "content":[{"type": ctype, "text": h["content"]}]})
    user_block = f"(N·∫øu h·ªØu √≠ch th√¨ d√πng CONTEXT)\nCONTEXT:\n{context}\n\nC√ÇU H·ªéI: {user_question}"
    msgs.append({"role":"user","content":[{"type":"input_text","text":user_block}]})
    return msgs


# ---- Hi·ªÉn th·ªã t·ªìn kho/OOS + emoji ----
def _stock_line(d: dict) -> str:
    if d.get("available") and (d.get("inventory_quantity") is None or d.get("inventory_quantity", 0) > 0):
        return "c√≤n h√†ng ‚úÖ"
    if d.get("inventory_quantity") == 0 or (d.get("status") and d.get("status") != "active"):
        return "h·∫øt h√†ng t·∫°m th·ªùi ‚õî"
    return "t√¨nh tr·∫°ng ƒëang c·∫≠p nh·∫≠t ‚è≥"

def _shorten(txt: str, n=280) -> str:
    t = (txt or "").strip()
    return (t[:n].rstrip() + "‚Ä¶") if len(t) > n else t
def _fmt_price(p, currency="‚Ç´"):
    if p is None:
        return None
    try:
        # n·∫øu p l√† string: ch·ªâ gi·ªØ ch·ªØ s·ªë
        s = re.sub(r"\D", "", str(p))
        if not s:
            return None
        val = int(float(s))
        return f"{val:,.0f}".replace(",", ".") + (f" {currency}" if currency else "")
    except Exception:
        return None

def _extract_price_number(txt: str):
    """B·∫Øt 199k / 199.000ƒë / 1,299,000 VND‚Ä¶ ‚Üí s·ªë (float)."""
    if not txt:
        return None
    low = str(txt).lower()
    m = re.search(r"(\d[\d\.\s,]{2,})(?:\s*)(ƒë|‚Ç´|vnd|vnƒë|k)?\b", low)
    try:
        if m:
            digits = re.sub(r"\D", "", m.group(1))  # ch·ªâ ch·ªØ s·ªë
            if not digits:
                return None
            v = float(digits)
            return v * 1000 if (m.group(2) == "k") else v
        m2 = re.search(r"\b(\d{4,})\b", low)  # fallback chu·ªói s·ªë d√†i
        return float(m2.group(1)) if m2 else None
    except Exception:
        return None


def _price_value(d: dict):
    for k in ("price","min_price","max_price"):
        v = d.get(k)
        if v is not None:
            try:
                digits = re.sub(r"\D", "", str(v))
                return float(digits) if digits else None
            except Exception:
                pass
    return _extract_price_number(d.get("text",""))


def _category_key_from_doc(d: dict):
    """X√°c ƒë·ªãnh 'd√≤ng' s·∫£n ph·∫©m ƒë·ªÉ so min‚Äìmax: ∆∞u ti√™n product_type; fallback theo synonyms trong title/tags."""
    pt = (d.get("product_type") or "").strip()
    if pt:
        return _normalize_text(pt)
    raw = " ".join([d.get("title",""), d.get("tags","")])
    n1, n2 = _norm_both(raw)
    for key in VN_SYNONYMS.keys():
        k1, k2 = _norm_both(key)
        if k1 in n1 or k2 in n2:
            return _normalize_text(key)
    return "misc"

def _minmax_in_category(base_doc: dict):
    """T√¨m 1 m·∫´u r·∫ª nh·∫•t v√† 1 m·∫´u ƒë·∫Øt nh·∫•t c√πng d√≤ng (lo·∫°i). N·∫øu kh√¥ng ƒë·ªß, fallback to√†n shop."""
    if not META_PROD:
        return None, None
    cat = _category_key_from_doc(base_doc)
    def same_cat(x):
        return _category_key_from_doc(x) == cat
    cands = [x for x in META_PROD if same_cat(x)]
    if len(cands) < 2:
        cands = [x for x in META_PROD]  # fallback to√†n shop

    items = []
    for x in cands:
        pv = _price_value(x)
        if pv is not None:
            items.append((pv, x))
    if not items:
        return None, None

    # lo·∫°i ch√≠nh ra kh·ªèi candidates n·∫øu tr√πng URL ho·∫∑c title
    def is_same(a, b):
        return (a.get("url") and a.get("url")==b.get("url")) or \
               ((a.get("title") or "").strip().lower()==(b.get("title") or "").strip().lower())

    items = [(p, x) for (p, x) in items if not is_same(x, base_doc)]
    if not items:
        return None, None

    items.sort(key=lambda t: t[0])
    low = items[0][1]
    high = items[-1][1]
    return low, high


def _extract_features_from_text(text_block: str):
    lines = []
    m = re.search(r"Specs:\s*(.+)", text_block, flags=re.I)
    if m:
        lines.extend(re.split(r"\s*\|\s*|\s*;\s*|,\s*", m.group(1)))
    m2 = re.search(r"Body:\s*(.+?)\s*URL:", text_block, flags=re.I|re.S)
    if m2:
        body = re.sub(r"\s+", " ", m2.group(1)).strip()
        chunks = re.split(r"(?<=[.!?])\s+", body)
        lines.extend(chunks)
    lines = [re.sub(r"\s+", " ", l).strip("‚Ä¢- \n\t") for l in lines if l and len(l.strip()) > 0]
    uniq = []
    for l in lines:
        if l not in uniq:
            uniq.append(l)
        if len(uniq) >= 5:
            break
    return ["‚Ä¢ " + _shorten(x, 80) for x in uniq[:5]]

# ====== T·ª™ KH√ìA / ƒê·ªíNG NGHƒ®A (ƒëa ng√¥n ng·ªØ) ======
VN_SYNONYMS = {
    # ===== ƒê·ªìng h·ªì & ph·ª• ki·ªán =====
    "ƒë·ªìng h·ªì": [
        "dong ho","dong-ho","dongho","watch","watchface","watch face","bezel",
        "galaxy watch","apple watch","amazfit","seiko","casio","nh35","nh36",
        "automatic","mechanical","chronograph"
    ],
    "d√¢y ƒë·ªìng h·ªì": [
        "day dong ho","daydongho","watch band","band","strap","nato","loop",
        "bracelet","mesh","leather strap","metal strap","silicone strap"
    ],
    "case ƒë·ªìng h·ªì": [
        "case dong ho","vo dong ho","bao ve dong ho","bezel protector",
        "watch case","watch bumper","watch cover","protective case"
    ],
    "k√≠nh c∆∞·ªùng l·ª±c": [
        "kinh cuong luc","tempered glass","screen protector","glass protector",
        "full glass","full cover","full glue","9h","anti-scratch","privacy glass"
    ],
    "·ªëp l∆∞ng": [
        "op lung","case","cover","bumper","clear case","tpu case",
        "silicone case","shockproof case","phone case","protective case"
    ],
    "v√≤ng tay": [
        "vong tay","bracelet","bangle","chain bracelet","cuff"
    ],
    "√°o thun": [
        "ao thun","ao phong","tshirt","t-shirt","tee","tee shirt","crewneck",
        "basic tee","unisex tee","oversize tee"
    ],
    "√°o ph√¥ng": [
        "ao phong","ao thun","tshirt","t-shirt","tee"
    ],

    # ===== ƒê·ªì ng·ªçt/ƒë·ªì u·ªëng (b·ªï sung cho shop) =====
    "b√°nh": [
        "banh","cake","gateau","pastry","ÁîúÂìÅ","ÈªûÂøÉ","‡πÄ‡∏Ñ‡πâ‡∏Å","‡∏Ç‡∏ô‡∏°",
        "kue","kueh","roti manis"
    ],
    "b√°nh crepe": [
        "banh crepe","crepe","mille crepe","crepe cake",
        "ÂèØ‰∏ΩÈ•º","ÂèØÈ∫óÈ§Ö","ÂçÉÂ±Ç","ÂçÉÂ±§","ÂçÉÂ±ÇËõãÁ≥ï","ÂçÉÂ±§ËõãÁ≥ï",
        "‡πÄ‡∏Ñ‡∏£‡∏õ","‡πÄ‡∏Ñ‡∏£‡∏õ‡πÄ‡∏Ñ‡πâ‡∏Å","kue crepe","mille crepes","kue lapis"
    ],
    "b√°nh s·∫ßu ri√™ng": [
        "banh sau rieng","durian","durian cake","durian crepe",
        "Ê¶¥Ëé≤","Ê¶¥Êß§","Ê¶¥Ëé≤ÂçÉÂ±Ç","Ê¶¥Êß§ÂçÉÂ±§","Ê¶¥Ëé≤ÂçÉÂ±ÇËõãÁ≥ï","Ê¶¥Êß§ÂçÉÂ±§ËõãÁ≥ï","Ê¶¥Ëé≤ÂèØ‰∏ΩÈ•º","Ê¶¥Êß§ÂèØÈ∫óÈ§Ö",
        "‡πÄ‡∏Ñ‡∏£‡∏õ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡πÄ‡∏Ñ‡πâ‡∏Å‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
        "kue durian","crepe durian","kue lapis durian"
    ],
    "tr√† s·ªØa": [
        "tra sua","milk tea","bubble tea","boba","boba tea","pearl milk tea",
        "Â•∂Ëå∂","ÁèçÁè†Â•∂Ëå∂","Ê≥¢Èú∏Â•∂Ëå∂",
        "‡∏ä‡∏≤‡∏ô‡∏°","‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å",
        "teh susu","teh susu boba","minuman boba","bubble tea id"
    ],
    "milktea": [
        "milk tea","bubble tea","boba","pearl milk tea",
        "Â•∂Ëå∂","ÁèçÁè†Â•∂Ëå∂","‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å","teh susu","boba tea"
    ],

    # ===== Chinese (ZH) ‚Äì nh√≥m theo kh√°i ni·ªám ƒë·ªÉ b·∫Øt r·ªông h∆°n =====
    "ÊâãË°®": ["ËÖïË°®","watch","Ë°®Â∏¶","Ë°®Èèà","Ë°®Âúà","Ë°®Â£≥","Ë°®ÊÆº","Èí¢ÂåñËÜú","ÈãºÂåñËÜú","‰øùÊä§Â£≥","‰øùË≠∑ÊÆº"],
    "Ë°®Â∏¶": ["Ë°®Â∏∂","Ë°®Èìæ","Ë°®Èèà","watch band","strap","ÁöÆË°®Â∏¶","ÈáëÂ±ûË°®Â∏¶","Á°ÖËÉ∂Ë°®Â∏¶"],
    "Èí¢ÂåñËÜú": ["ÈãºÂåñËÜú","ÁéªÁíÉËÜú","Ë¥¥ËÜú","Ë≤ºËÜú","‰øùÊä§ËÜú","‰øùË≠∑ËÜú","tempered glass","screen protector","ÂÖ®ËÉ∂","ÂÖ®ËÜ†","9h"],
    "ÊâãÊú∫Â£≥": ["ÊâãÊ©üÊÆº","‰øùÊä§Â£≥","‰øùË≠∑ÊÆº","ÊâãÊú∫Â•ó","phone case","case","bumper","‰øùË≠∑ÊÆº"],
    "TÊÅ§": ["TÊÅ§Ë°´","Áü≠Ë¢ñ","ÂúÜÈ¢Ü","ÂúìÈ†ò","tee","tshirt","t-shirt"], 
    "Â•∂Ëå∂": ["ÁèçÁè†Â•∂Ëå∂","Ê≥¢Èú∏Â•∂Ëå∂","Â•∂ÁõñËå∂","milk tea","bubble tea","boba"],
    "ÂèØ‰∏ΩÈ•º": ["ÂèØÈ∫óÈ§Ö","Ê≥ïÂºèËñÑÈ•º","Ê≥ïÂºèËñÑÈ§Ö","ÂçÉÂ±Ç","ÂçÉÂ±§","ÂçÉÂ±ÇËõãÁ≥ï","ÂçÉÂ±§ËõãÁ≥ï","crepe","mille crepe"],
    "Ê¶¥Ëé≤": ["Ê¶¥Êß§","durian","Ê¶¥Ëé≤ÂçÉÂ±Ç","Ê¶¥Êß§ÂçÉÂ±§","Ê¶¥Ëé≤ÂèØ‰∏ΩÈ•º","Ê¶¥Êß§ÂèØÈ∫óÈ§Ö","Ê¶¥Ëé≤ËõãÁ≥ï","Ê¶¥Êß§ËõãÁ≥ï"],

    # ===== Thai (TH) =====
    "‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤": ["watch","‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å","‡∏Å‡∏£‡∏≠‡∏ö‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡πÄ‡∏Ñ‡∏™‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤"],
    "‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤": ["watch band","strap","‡∏™‡∏≤‡∏¢‡∏´‡∏ô‡∏±‡∏á","‡∏™‡∏≤‡∏¢‡πÇ‡∏•‡∏´‡∏∞","‡∏™‡∏≤‡∏¢‡∏ã‡∏¥‡∏•‡∏¥‡πÇ‡∏Ñ‡∏ô","‡∏ô‡∏≤‡πÇ‡∏ï‡πâ"],
    "‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å": ["tempered glass","‡∏Å‡∏£‡∏∞‡∏à‡∏Å‡∏Å‡∏±‡∏ô‡∏£‡∏≠‡∏¢","full glue","9h","screen protector"],
    "‡πÄ‡∏Ñ‡∏™‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå": ["‡πÄ‡∏Ñ‡∏™","‡∏ã‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠","bumper","phone case","protective case"],
    "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î": ["tshirt","t-shirt","tee","‡∏Ñ‡∏≠‡∏Å‡∏•‡∏°","‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏ã‡πå"],
    "‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å": ["‡∏ä‡∏≤‡∏ô‡∏°","bubble tea","boba","milk tea"],
    "‡πÄ‡∏Ñ‡∏£‡∏õ": ["‡πÄ‡∏Ñ‡∏£‡∏õ‡πÄ‡∏Ñ‡πâ‡∏Å","crepe","mille crepe"],
    "‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô": ["durian","‡πÄ‡∏Ñ‡∏£‡∏õ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡πÄ‡∏Ñ‡πâ‡∏Å‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô"],

    # ===== Indonesian (ID) =====
    "jam tangan": ["watch","tali jam","pelindung layar","casing jam","bezel","case jam"],
    "tali jam": ["watch band","strap","nato","kulit","logam","silikon"],
    "pelindung layar": ["tempered glass","screen protector","kaca tempered","9h","full glue"],
    "casing hp": ["case","casing","sarung hp","bumper","phone case"],
    "kaos": ["tshirt","t-shirt","tee","kaos oversize","kaos unisex"],
    "teh susu": ["bubble tea","boba","milk tea","minuman boba"],
    "crepe": ["kue crepe","mille crepe","kue lapis","crepe cake"],
    "durian": ["kue durian","crepe durian","kue lapis durian"]
}
def add_syn(key, words):
    arr = VN_SYNONYMS.setdefault(key, [])
    # n·ªëi v√† kh·ª≠ tr√πng l·∫∑p, gi·ªØ th·ª© t·ª±
    VN_SYNONYMS[key] = list(dict.fromkeys(arr + words))

add_syn("Ê¶¥Êß§", ["Ê¶¥Ëé≤","durian","Ê¶¥Êß§ÂçÉÂ±§","Ê¶¥Êß§ÂèØÈ∫óÈ§Ö","Ê¶¥Êß§ËõãÁ≥ï","Ê¶¥Êß§ÂçÉÂ±§ËõãÁ≥ï"])
add_syn("ÂèØÈ∫óÈ§Ö", ["ÂèØ‰∏ΩÈ•º","crepe","mille crepe","crepe cake","ÂçÉÂ±§","ÂçÉÂ±§ËõãÁ≥ï","Ê≥ïÂºèÂèØÈ∫óÈ§Ö"])
add_syn("Â•∂Ëå∂", ["ÁèçÁè†Â•∂Ëå∂","Ê≥¢Èú∏Â•∂Ëå∂","Â•∂ËìãËå∂","milk tea","bubble tea","boba"])
add_syn("Â∏ÇÈõÜ", ["market","marketplace","ÈõÜÂ∏Ç","bazaar"])
add_syn("Âè∞ÁÅ£", ["Ëá∫ÁÅ£","Âè∞Êπæ","taiwan","tw"])
add_syn("Ëá∫ÁÅ£", ["Âè∞ÁÅ£","Âè∞Êπæ","taiwan","tw"])
add_syn("Êæ≥Ê¥≤", ["australia","√∫c","au"])
add_syn("Ë∂äÂçó", ["vietnam","vi·ªát nam","vn"])


# --- B·ªï sung ƒë·ªìng nghƒ©a cho shop (TW/ÁπÅÈ´î) ---



def _query_tokens(q: str, lang: str = "vi") -> set:
    """Sinh token t·ª´ c√¢u h·ªèi: c√≥ d·∫•u, kh√¥ng d·∫•u, bigram, c·ª•m phrase v√† synonyms."""
    n1, n2 = _norm_both(q)
    w1 = [w for w in n1.split() if len(w) > 1]
    w2 = [w for w in n2.split() if len(w) > 1]

    tokens = set(w1) | set(w2)

    # bigram cho c·∫£ c√≥ d·∫•u & kh√¥ng d·∫•u (b·∫Øt 's·∫ßu ri√™ng', 'banh sau'...)
    for words in (w1, w2):
        for i in range(len(words) - 1):
            tokens.add((words[i] + " " + words[i+1]).strip())
            tokens.add((words[i] + words[i+1]).strip())  # bi·∫øn th·ªÉ kh√¥ng space

    combo_phrases = {
        "vi": ["ƒë·ªìng h·ªì","d√¢y ƒë·ªìng h·ªì","k√≠nh c∆∞·ªùng l·ª±c","·ªëp l∆∞ng","√°o thun","√°o ph√¥ng","b√°nh crepe","b√°nh s·∫ßu ri√™ng","tr√† s·ªØa"],
        "en": ["watch band","screen protector","phone case","t shirt","t-shirt","mille crepe","durian crepe","milk tea","bubble tea","boba tea"],
        "zh": ["ÊâãË°®","Ë°®Â∏¶","Èí¢ÂåñËÜú","ÊâãÊú∫Â£≥","TÊÅ§","ÂèØ‰∏ΩÈ•º","Ê¶¥Ëé≤ÂçÉÂ±Ç","Â•∂Ëå∂","ÁèçÁè†Â•∂Ëå∂"],
        "th": ["‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å","‡πÄ‡∏Ñ‡∏™‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå","‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î","‡πÄ‡∏Ñ‡∏£‡∏õ","‡πÄ‡∏Ñ‡∏£‡∏õ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å"],
        "id": ["jam tangan","tali jam","pelindung layar","casing hp","kaos","kue crepe","crepe durian","teh susu","bubble tea","boba"]
    }

    joined_n1 = " ".join(w1)
    for phrase in combo_phrases.get(lang, []):
        if phrase in joined_n1:
            p1, p2 = _norm_both(phrase)
            tokens.update({p1, p2, p1.replace(" ", ""), p2.replace(" ", "")})

    # √Ånh x·∫° synonyms (ƒë·∫∑t NGO√ÄI v√≤ng for phrase)
    for key, syns in VN_SYNONYMS.items():
        key_n1, key_n2 = _norm_both(key)

        seen = (key_n1 in n1) or (key_n2 in n2)
        if not seen:
            for s in syns:
                s1, s2 = _norm_both(s)
                if s1 in n1 or s2 in n2:
                    seen = True
                    break

        if seen:
            for s in [key] + list(syns):
                s1, s2 = _norm_both(s)
                tokens.update({s1, s2, s1.replace(" ", ""), s2.replace(" ", "")})

    return {t for t in tokens if len(t) >= 2}



def filter_hits_by_query(hits, q, lang="vi"):
    """Gi·ªØ hit n·∫øu c√≥ token/c·ª•m t·ª´ c√¢u h·ªèi xu·∫•t hi·ªán trong title/tags/type/variant (c√≥ & kh√¥ng d·∫•u)."""
    if not hits:
        return []
    qtoks = _query_tokens(q, lang=lang)

    kept = []
    for d in hits:
        hay_raw = " ".join([
            d.get("title",""), d.get("tags",""), d.get("product_type",""), d.get("variant","")
        ])
        h1, h2 = _norm_both(hay_raw)
        h1_ns, h2_ns = h1.replace(" ", ""), h2.replace(" ", "")

        ok = any(
            (t in h1) or (t in h2) or (t.replace(" ","") in h1_ns) or (t.replace(" ","") in h2_ns)
            for t in qtoks
        )
        if ok:
            kept.append(d)
    return kept


def should_relax_filter(q: str, hits: list) -> bool:
    qn = _normalize_text(q)
    return len(qn.split()) <= 2 and len(hits) > 0

# ====== Compose tr·∫£ l·ªùi ======
def compose_product_reply(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    # ∆Øu ti√™n currency trong meta; n·∫øu kh√¥ng c√≥, m·∫∑c ƒë·ªãnh ‚Ç´ cho VI
    currency = (hits[0].get("currency") or ("‚Ç´" if lang == "vi" else ""))

    items = []
    for d in hits[:2]:
        title     = d.get("title") or "S·∫£n ph·∫©m"
        variant   = d.get("variant")
        stock     = _stock_line(d)

        price_val = _price_value(d)
        price_str = _fmt_price(price_val, currency) if price_val is not None else None

        line = f"‚Ä¢ {title}"
        if variant:
            line += f" ({variant})"
        if price_str:
            line += f" ‚Äî {price_str}"
        line += f" ‚Äî {stock}"
        items.append(line)

    raw = f"{t(lang,'suggest_hdr')}\n" + "\n".join(items) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)

def compose_product_info(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    d = hits[0]
    currency   = d.get("currency") or ("‚Ç´" if lang == "vi" else "")
    title      = d.get("title") or "S·∫£n ph·∫©m"
    stock      = _stock_line(d)

    price_val  = _price_value(d)
    price_line = f"Gi√° tham kh·∫£o: {_fmt_price(price_val, currency)}" if price_val is not None else ""

    bullets = _extract_features_from_text(d.get("text",""))
    body    = "\n".join(bullets) if bullets else "‚Ä¢ Thi·∫øt k·∫ø t·ªëi gi·∫£n, d·ªÖ ph·ªëi ƒë·ªì\n‚Ä¢ Ch·∫•t li·ªáu tho√°ng, d·ªÖ v·ªá sinh"

    parts = [
        f"{t(lang,'highlights', title=title)}",
        body
    ]
    if price_line:
        parts.append(price_line)
    parts.extend([
        f"T√¨nh tr·∫°ng: {stock}",
        t(lang,"product_pts")
    ])

    raw = "\n".join(parts).strip()
    return rephrase_casual(raw, intent="product", lang=lang)


def compose_contextual_answer(context, question, history, lang="vi"):
    ctx = (shop_identity(lang) + "\n" + (context or "")).strip()
    msgs = build_messages(SYSTEM_STYLE, history, ctx, question)
    _, reply = call_openai(msgs, temperature=0.6)
    return reply


def compose_price_with_suggestions(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback"), []

    main = hits[0]
    currency = main.get("currency") or ("‚Ç´" if lang == "vi" else "")
    main_price = _price_value(main)
    main_price_str = _fmt_price(main_price, currency) if main_price is not None else "ƒëang c·∫≠p nh·∫≠t"

    low, high = _minmax_in_category(main)

    lines = []
    title = main.get("title") or "S·∫£n ph·∫©m"
    lines.append(f"V√¢ng ·∫°, **{title}** ƒëang ƒë∆∞·ª£c shop b√°n v·ªõi **gi√° c√¥ng khai: {main_price_str}**.")
    sug = []
    if high:
        hp = _fmt_price(_price_value(high), currency)
        sug.append(f"‚Ä¢ **C√πng d√≤ng ‚Äì gi√° cao nh·∫•t:** {high.get('title','SP')} ‚Äî {hp}")
    if low:
        lp = _fmt_price(_price_value(low), currency)
        sug.append(f"‚Ä¢ **C√πng d√≤ng ‚Äì gi√° th·∫•p nh·∫•t:** {low.get('title','SP')} ‚Äî {lp}")

    if sug:
        lines.append("B·∫°n c≈©ng c√≥ th·ªÉ tham kh·∫£o th√™m:")
        lines += sug
    lines.append(t(lang, "product_pts"))
    raw = "\n".join(lines)

    # Th√™m SP ch√≠nh v√†o button ƒë·∫ßu ti√™n
    btns = [main] + [x for x in (high, low) if x]
    return rephrase_casual(raw, intent="product", lang=lang), btns[:2]
def answer_with_rag(user_id, user_question):
    s = _get_sess(user_id)
    hist = s["hist"]

    intent = detect_intent(user_question)
    lang = detect_lang(user_question)
    print(f"üîé intent={intent} | üó£Ô∏è lang={lang}")

    # ‚Äî‚Äî‚Äî QUICK ROUTES ‚Äî‚Äî‚Äî
    if intent == "greet":
        return greet_text(lang), []
    if intent == "smalltalk":
        return handle_smalltalk(user_question, lang=lang), []
    if intent == "browse":
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return t(lang, "browse", url=url), []
    if intent == "new_items":
        items = get_new_arrivals(days=30, topk=4)
        return compose_new_arrivals(lang=lang, items=items), items[:2]

    # ‚Äî‚Äî‚Äî PRODUCT SEARCH ‚Äî‚Äî‚Äî
    prod_hits, prod_scores = search_products_with_scores(user_question, topk=8)
    # NEW: rerank theo ti√™u ƒë·ªÅ ƒë·ªÉ ƒë·∫©y ƒë√∫ng s·∫£n ph·∫©m l√™n ƒë·∫ßu
    prod_hits = _rerank_by_title(user_question, prod_hits, prod_scores)
    prod_scores = [h.get("score", 0.0) for h in prod_hits]

    best = max(prod_scores or [0.0])

    ok_by_score = _score_gate(user_question, prod_hits, best)


    filtered_hits = filter_hits_by_query(prod_hits, user_question, lang=lang) if STRICT_MATCH else prod_hits
    # n·∫øu STRICT_MATCH l√†m r·ªóng m√† catalog l√† ZH ‚Üí n·ªõi l·ªçc
    if STRICT_MATCH and not filtered_hits and (_any_cjk(user_question) or _cjk_in_hits(prod_hits)):
        filtered_hits = prod_hits

    title_ok = _has_title_overlap(user_question, prod_hits)
    # --- C·ª®U C√ÅNH THEO ƒêI·ªÇM ---
    # n·∫øu filter b·ªã r·ªóng nh∆∞ng ƒëi·ªÉm ƒë√£ ƒë·∫°t ng∆∞·ª°ng ‚Üí gi·ªØ nguy√™n prod_hits
    if not filtered_hits and ok_by_score:
        filtered_hits = prod_hits

    if title_ok and not filtered_hits:
        filtered_hits = prod_hits

    print(f"üìà best_score={best:.3f}, hits={len(prod_hits)}, kept_after_filter={len(filtered_hits)}, title_ok={title_ok}")

    # --- CONTEXT/POLICY ---   # <‚Äî b·ªè th·ª•t v√†o ƒë·∫ßu d√≤ng
    context = retrieve_context(user_question, topk=6)
    if intent == "policy" and context:
        ans = compose_contextual_answer(context, user_question, hist, lang=lang)
        ans = f"{t(lang,'policy_hint')} {ans}"
        return rephrase_casual(ans, intent="policy", temperature=0.5, lang=lang), []


    # --- ∆ØU TI√äN H·ªéI GI√Å ---
    if is_price_question(user_question, lang) and (filtered_hits or title_ok):
        print("‚û°Ô∏è route=price_question‚Üíprice_with_suggestions")
        chosen = filtered_hits if filtered_hits else prod_hits
        reply, sug_hits = compose_price_with_suggestions(chosen, lang=lang)
        return reply, sug_hits

    # --- PRODUCT BRANCHES ---
    # (n·∫øu b·∫°n ƒë√£ th√™m ok_by_score theo patch tr∆∞·ªõc, d√πng n√≥; ch∆∞a c√≥ th√¨ thay ok_by_score b·∫±ng (best >= SCORE_MIN))
    not_enough = (not filtered_hits) or (not ok_by_score and not title_ok)

    if intent in {"product", "product_info"} and not_enough:
        # 1) C√≥ context ‚Üí d√πng LLM + context
        if context:
            print("‚û°Ô∏è route=ctx_fallback_from_product")
            ans = compose_contextual_answer(context, user_question, hist, lang=lang)
            return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []
        # 2) Kh√¥ng c√≥ context ‚Üí LLM tr∆°n (shop_identity v·∫´n ƒë∆∞·ª£c ch√®n trong compose_contextual_answer)
        if ALWAYS_ANSWER:
            print("‚û°Ô∏è route=llm_fallback_from_product")
            ans = compose_contextual_answer("", user_question, hist, lang=lang)
            return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []
        # 3) Cu·ªëi c√πng m·ªõi r∆°i v·ªÅ OOS
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        print("‚û°Ô∏è route=oos_hint")
        return t(lang, "oos", url=url), []

    if intent == "product_info":
        print("‚û°Ô∏è route=product_info")
        return compose_product_info(filtered_hits, lang=lang), filtered_hits[:1]

    if intent in {"product", "other"} and filtered_hits and (ok_by_score or title_ok):
        print("‚û°Ô∏è route=product_reply")
        return compose_product_reply(filtered_hits, lang=lang), filtered_hits[:2]

    # --- CONTEXT FALLBACK CHUNG ---
    if context:
        print("‚û°Ô∏è route=ctx_fallback")
        ans = compose_contextual_answer(context, user_question, hist, lang=lang)
        return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []

    print("‚û°Ô∏è route=fallback")
    if ALWAYS_ANSWER:
        ans = compose_contextual_answer("", user_question, hist, lang=lang)
        return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []
    return t(lang, "fallback"), []


# ========= WEBHOOK =========
@app.route("/webhook", methods=["GET", "POST"])
def webhook():
    # --- Verify (GET)
    if request.method == "GET":
        token = request.args.get("hub.verify_token")
        challenge = request.args.get("hub.challenge")
        print(f"[Webhook][GET] verify_token={token}, challenge={challenge}")
        return (challenge, 200) if token == VERIFY_TOKEN else ("Invalid verification token", 403)

    # --- Events (POST)
    if not _verify_fb_sig(request):
        ua = request.headers.get("User-Agent", "?")
        print(f"[Webhook][POST] ‚ùå Invalid signature (UA={ua})")
        return "Invalid signature", 403

    payload = request.json or {}
    print("[Webhook][POST] üîî incoming:", json.dumps(payload)[:500])

    for entry in payload.get("entry", []):
        owner_id = str(entry.get("id"))
        page_token = TOKEN_MAP.get(owner_id)
        if not page_token:
            print(f"[Webhook] ‚ö†Ô∏è No token mapped for owner_id={owner_id}. TOKEN_MAP size={len(TOKEN_MAP)}")
            continue

        for event in entry.get("messaging", []):
            try:
                # 1) B·ªè qua c√°c event kh√¥ng ph·∫£i user nh·∫Øn tin
                if event.get("message", {}).get("is_echo"):
                    continue
                if "delivery" in event or "read" in event or "reaction" in event:
                    continue
                if not (event.get("message") or event.get("postback")):
                    continue

                psid = event.get("sender", {}).get("id")
                if not psid:
                    continue

                msg = event.get("message", {}) or {}
                pb  = event.get("postback", {}) or {}

                # 2) L·∫•y text h·ª£p l·ªá
                text = None
                if "text" in msg:
                    text = msg["text"]
                elif msg.get("quick_reply", {}).get("payload"):
                    text = msg["quick_reply"]["payload"]
                elif pb.get("payload"):
                    text = pb["payload"]
                elif pb.get("title"):
                    text = pb["title"]

                # 3) N·∫øu ch·ªâ l√† attachments ‚Üí nh·∫Øn 1 c√¢u r·ªìi th√¥i
                if not text:
                    if msg.get("attachments"):
                        fb_send_text(psid, "M√¨nh ƒë√£ nh·∫≠n ·∫£nh/file b·∫°n g·ª≠i. M√¥ t·∫£ th√™m ƒë·ªÉ m√¨nh t∆∞ v·∫•n nh√© üòä", page_token)
                    continue

                # 4) Ch·ªëng tr√πng theo MID (kh√¥ng d√πng timestamp)
                mid = msg.get("mid") or pb.get("mid")
                if mid:
                    sess = _get_sess(psid)
                    if sess.get("last_mid") == mid:
                        continue
                    sess["last_mid"] = mid

                fb_mark_seen(psid, page_token)
                fb_typing_on(psid, page_token)

                _remember(psid, "user", text)
                reply, btn_hits = answer_with_rag(psid, text)
                lang = detect_lang(text)
                _remember(psid, "assistant", reply)

                fb_send_text(psid, reply, page_token)

                if btn_hits:
                    buttons = []
                    for h in btn_hits[:3]:
                        if h.get("url"):
                            buttons.append({
                                "type": "web_url",
                                "url": h["url"],
                                "title": (h.get("title") or t(lang, "btn_view"))[:20]
                            })
                    if buttons:
                        fb_send_buttons(psid, t(lang, "quick_view"), buttons, page_token)

            except Exception as e:
                print("[Webhook][POST] ‚ö†Ô∏è handle event error:", repr(e))
                continue

    # Lu√¥n 200 sau khi x·ª≠ l√Ω xong
    return "ok", 200



# ========= API =========
@app.route("/api/chat", methods=["POST"])
def chat():
    data = request.json or {}
    messages = data.get("messages", [])
    print("üåê /api/chat len =", len(messages))
    openai_raw, _ = call_openai(messages)
    return jsonify(openai_raw)

@app.route("/api/chat_rag", methods=["POST"])
def chat_rag():
    data = request.json or {}
    q = data.get("question", "")
    print("üåê /api/chat_rag question:", q)
    if not q:
        return jsonify({"error": "Missing 'question'"}), 400
    reply, _ = answer_with_rag("anonymous", q)
    return jsonify({"reply": reply})

@app.route("/api/product_search")
def api_product_search():
    try:
        q = (request.args.get("q") or "").strip()
        if not q:
            return jsonify({"ok": False, "msg": "missing q"}), 400

        lang = detect_lang(q)
        if IDX_PROD is None:
            url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
            return jsonify({"ok": True, "reply": t(lang, "oos", url=url), "items": []})

        hits, scores = search_products_with_scores(q, topk=8)
        best = max(scores or [0.0])

        kept = filter_hits_by_query(hits, q, lang=lang) if STRICT_MATCH else hits
        if STRICT_MATCH and not kept and should_relax_filter(q, hits):
            kept = hits
        if STRICT_MATCH and not kept and (_any_cjk(q) or _cjk_in_hits(hits)):
            kept = hits

        ok_by_score = _score_gate(q, hits, best)
        title_ok    = _has_title_overlap(q, hits)
        if not kept and ok_by_score:
            kept = hits

        if not kept or (not ok_by_score and not title_ok):
            url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
            return jsonify({"ok": True, "reply": t(lang, "oos", url=url), "items": []})

        reply = compose_product_reply(kept, lang=lang)
        resp = {"ok": True, "reply": reply, "items": kept[:2]}
        if (request.args.get("debug") or "") == "1":
            resp["debug"] = {
                "best": best, "hits": len(hits),
                "kept_after_filter": len(kept),
                "title_ok": bool(title_ok), "ok_by_score": bool(ok_by_score),
            }
        return jsonify(resp)
    except Exception as e:
        return jsonify({"ok": False, "msg": f"error: {e}"}), 500

# ========= IG OAuth callback & policy pages =========
@app.route("/auth/callback")
def auth_callback():
    code = request.args.get("code")
    print("üîÅ /auth/callback code:", code)
    return f"Auth success! Code: {code}"

@app.route("/privacy")
def privacy():
    return """
    <h1>Privacy Policy - Aloha Bot</h1>
    <p>Ch√∫ng t√¥i ch·ªâ x·ª≠ l√Ω n·ªôi dung tin nh·∫Øn m√† ng∆∞·ªùi d√πng g·ª≠i t·ªõi Fanpage ƒë·ªÉ tr·∫£ l·ªùi.
    Kh√¥ng b√°n/chia s·∫ª d·ªØ li·ªáu c√° nh√¢n. D·ªØ li·ªáu phi√™n tr√≤ chuy·ªán (session) ch·ªâ l∆∞u t·∫°m th·ªùi
    t·ªëi ƒëa 30 ph√∫t ph·ª•c v·ª• tr·∫£ l·ªùi v√† s·∫Ω t·ª± xo√° sau ƒë√≥. Ch·ªâ s·ªë s·∫£n ph·∫©m (vectors) l√† d·ªØ li·ªáu c√¥ng khai t·ª´ c·ª≠a h√†ng.</p>
    <p>Li√™n h·ªá xo√° d·ªØ li·ªáu: g·ª≠i tin nh·∫Øn 'delete my data' t·ªõi Fanpage ho·∫∑c email: <b>hoclac1225@email.com</b>.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

@app.route("/data_deletion")
def data_deletion():
    return """
    <h1>Data Deletion Instructions</h1>
    <p>ƒê·ªÉ y√™u c·∫ßu xo√° d·ªØ li·ªáu: (1) nh·∫Øn 'delete my data' t·ªõi Fanpage, ho·∫∑c (2) g·ª≠i email t·ªõi <b>hoclac1225@email.com</b>
    k√®m ID cu·ªôc tr√≤ chuy·ªán. Ch√∫ng t√¥i s·∫Ω x·ª≠ l√Ω trong th·ªùi gian s·ªõm nh·∫•t.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

# ========= Debug & Health =========
@app.route("/debug/rag_status_simple")
def rag_status_simple():
    return jsonify({
        "vector_dir": os.path.abspath(VECTOR_DIR),
        "products_index": bool(IDX_PROD),
        "products_chunks": len(META_PROD) if META_PROD else 0,
        "policies_index": bool(IDX_POL),
        "policies_chunks": len(META_POL) if META_POL else 0,
        "sessions": len(SESS),
    })

@app.route("/health")
def health():
    return jsonify({
        "ok": True,
        "products": len(META_PROD) if META_PROD else 0,
        "policies": len(META_POL) if META_POL else 0
    })

# ========= Watcher: t·ª± reload khi vector ƒë·ªïi =========


_last_vec_mtime = 0
def _mtime(path):
    try:
        return os.path.getmtime(path)
    except Exception:
        return 0

def _watch_vectors():
    global _last_vec_mtime
    idx_p = os.path.join(VECTOR_DIR, "products.index")
    meta_p = os.path.join(VECTOR_DIR, "products.meta.json")
    idx_k = os.path.join(VECTOR_DIR, "policies.index")
    meta_k = os.path.join(VECTOR_DIR, "policies.meta.json")

    newest = max(_mtime(idx_p), _mtime(meta_p), _mtime(idx_k), _mtime(meta_k))
    if newest and newest != _last_vec_mtime:
        print("üïµÔ∏è Detected vector change ‚Üí reload")
        if _reload_vectors():
            _last_vec_mtime = newest

def _start_vector_watcher():
    try:
        from apscheduler.schedulers.background import BackgroundScheduler
        sch = BackgroundScheduler(timezone="Asia/Ho_Chi_Minh")
        sch.add_job(_watch_vectors, "interval", seconds=30, id="watch_vectors")
        sch.add_job(lambda: (_purge_sessions()), "interval", minutes=10, id="purge_sessions")
        sch.start()
        print("‚è±Ô∏è Vector watcher started (30s)")
    except Exception as e:
        print("‚ö†Ô∏è Scheduler error:", repr(e))



# ======== MAIN ========
if __name__ == "__main__":
    if os.getenv("ENABLE_VECTOR_WATCHER", "true").lower() == "true":
        _start_vector_watcher()
    port = int(os.getenv("PORT", 3000))
    print(f"üöÄ Starting app on 0.0.0.0:{port}")
    # app.run(host="0.0.0.0", port=port, debug=False)
