# -*- coding: utf-8 -*-
import unicodedata
import os, json, time, re, requests, numpy as np, faiss, threading, random
from collections import deque
from flask import Flask, request, jsonify, abort
from flask_cors import CORS
from dotenv import load_dotenv
from openai import OpenAI
import hmac, hashlib, base64

# --- text normalize helpers (c√≥ & kh√¥ng d·∫•u)
def _strip_accents(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if not unicodedata.combining(ch))

def _normalize_text(s: str) -> str:
    # gi·ªØ l·∫°i ch·ªØ, s·ªë, kho·∫£ng tr·∫Øng, c√°c b·∫£ng ch·ªØ c√°i m·ªü r·ªông
    return re.sub(r"[^0-9a-z\u00c0-\u024f\u1e00-\u1eff\u4e00-\u9fff\u0E00-\u0E7F ]", " ", (s or "").lower()).strip()

def _norm_both(s: str):
    """Tr·∫£ v·ªÅ tuple (c√≥_d·∫•u, kh√¥ng_d·∫•u) ƒë√£ normalize & lower."""
    n1 = _normalize_text(s)
    n2 = _normalize_text(_strip_accents(s))
    return n1, n2
# s·ªë t·ª´ t·ªëi thi·ªÉu ph·∫£i tr√πng trong title (c√≥ th·ªÉ cho v√†o ENV n·∫øu mu·ªën)
TITLE_MIN_WORDS = int(os.getenv("TITLE_MIN_WORDS", "2"))

def _has_title_overlap(q, hits, min_words: int = TITLE_MIN_WORDS, min_cover: float = 0.6):
    """
    Tr·∫£ True n·∫øu:
    - C√≥ √≠t nh·∫•t 'min_words' t·ª´ trong c√¢u h·ªèi xu·∫•t hi·ªán trong title (ƒë√£ normalize, c√≥/kh√¥ng d·∫•u), HO·∫∂C
    - T·ª∑ l·ªá ph·ªß t·ª´ (matched/len(tokens)) >= min_cover  (fallback cho c√¢u r·∫•t ng·∫Øn / ng√¥n ng·ªØ kh√¥ng c√≥ kho·∫£ng tr·∫Øng)
    """
    qn1, qn2 = _norm_both(q)
    # tokens theo kho·∫£ng tr·∫Øng, b·ªè t·ª´ 1 k√Ω t·ª±
    qtok = [w for w in qn1.split() if len(w) > 1]
    if not qtok:                    # v√≠ d·ª• ti·∫øng Trung ‚Üí kh√¥ng t√°ch ƒë∆∞·ª£c t·ª´
        qtok = [qn1]                # fallback: d√πng c·∫£ chu·ªói ƒë√£ normalize

    for d in hits[:5]:
        t1, t2 = _norm_both(d.get("title", ""))
        matched = sum(1 for w in qtok if (w in t1) or (w in t2))

        # ƒêi·ªÅu ki·ªán ‚Äú√≠t nh·∫•t N t·ª´ tr√πng‚Äù
        cond_min_words = (len(qtok) >= min_words and matched >= min_words)
        # Fallback coverage (gi·ªØ logic c≈©): h·ªØu √≠ch khi c√¢u h·ªèi qu√° ng·∫Øn
        cond_cover = (matched / max(1, len(qtok))) >= min_cover

        if cond_min_words or cond_cover:
            return True
    return False




# ========= BOOTSTRAP =========
load_dotenv()
app = Flask(__name__)

allowed_origins = os.getenv("ALLOWED_ORIGINS", "")
origins = [o.strip() for o in allowed_origins.split(",")] if allowed_origins else ["*"]
CORS(app, origins=origins)

OPENAI_API_KEY   = os.getenv("OPENAI_API_KEY", "")
VERIFY_TOKEN     = os.getenv("VERIFY_TOKEN", "aloha_verify_123")
# ---- Multi-page map ----
# ---- Multi-id -> token (FB Page & IG) ----
TOKEN_MAP = {}

def _add_map(key, token):
    if key and token:
        TOKEN_MAP[str(key)] = token

for i in range(1, 11):
    pid = os.getenv(f"FB_PAGE_ID_{i}")
    ptk = os.getenv(f"FB_PAGE_TOKEN_{i}")
    _add_map(pid, ptk)  # map Page ID -> Page token

    igid = os.getenv(f"IG_ACCOUNT_ID_{i}")
    _add_map(igid, ptk) # map IG Account ID -> d√πng CHUNG Page token ƒë√£ li√™n k·∫øt IG
print("TOKEN_MAP size:", len(TOKEN_MAP))


VECTOR_DIR       = os.getenv("VECTOR_DIR", "./vectors")
# Shopify
SHOPIFY_SHOP = os.getenv("SHOPIFY_STORE", "")  # domain *.myshopify.com (tham chi·∫øu)
# Link shop m·∫∑c ƒë·ªãnh (fallback)
SHOP_URL         = os.getenv("SHOP_URL", "https://shop.aloha.id.vn/zh")
# ƒêa ng√¥n ng·ªØ
SUPPORTED_LANGS  = [s.strip() for s in os.getenv("SUPPORTED_LANGS", "vi,en,zh,th,id").split(",")]
DEFAULT_LANG     = os.getenv("DEFAULT_LANG", "vi")
SHOP_URL_MAP = {
    "vi": os.getenv("SHOP_URL_VI", SHOP_URL),
    "en": os.getenv("SHOP_URL_EN", SHOP_URL),
    "zh": os.getenv("SHOP_URL_ZH", SHOP_URL),
    "th": os.getenv("SHOP_URL_TH", SHOP_URL),
    "id": os.getenv("SHOP_URL_ID", SHOP_URL),
}

REPHRASE_ENABLED = os.getenv("REPHRASE_ENABLED", "true").lower() == "true"
EMOJI_MODE       = os.getenv("EMOJI_MODE", "cute")  # "cute" | "none"

# L·ªçc & ng∆∞·ª°ng ƒëi·ªÉm
SCORE_MIN = float(os.getenv("PRODUCT_SCORE_MIN", "0.28"))
STRICT_MATCH = os.getenv("STRICT_MATCH", "true").lower() == "true"

print("=== BOOT ===")
print("VECTOR_DIR:", os.path.abspath(VECTOR_DIR))
print("TOKEN_MAP size:", len(TOKEN_MAP))
print("OPENAI key set:", bool(OPENAI_API_KEY))




# OpenAI
OPENAI_URL   = "https://api.openai.com/v1/responses"
OPENAI_MODEL = "gpt-4o-mini"
EMBED_MODEL  = os.getenv("EMBED_MODEL", "text-embedding-3-small")

oai_client   = OpenAI(api_key=OPENAI_API_KEY)

# ========= SMALL IN-MEMORY SESSION =========
SESS = {}
SESS_LOCK = threading.Lock()
SESSION_TTL = 60 * 30  # 30 ph√∫t kh√¥ng t∆∞∆°ng t√°c th√¨ reset

def _get_sess(user_id):
    now = time.time()
    with SESS_LOCK:
        s = SESS.get(user_id)
        if not s or now - s["ts"] > SESSION_TTL:
            s = {"hist": deque(maxlen=8), "last_mid": None, "ts": now}
            SESS[user_id] = s
        s["ts"] = now
        return s

def _remember(user_id, role, text):
    s = _get_sess(user_id)
    s["hist"].append({"role": role, "content": text})

# ========= OPENAI WRAPPER =========
def _to_chat_messages(messages):
    """Chuy·ªÉn format responses -> chat.completions ƒë·ªÉ fallback."""
    chat_msgs = []
    ALLOWED = {"input_text", "output_text", "text"}  # <-- th√™m output_text
    for m in messages:
        role = m.get("role", "user")
        parts = m.get("content", [])
        text = "\n".join([p.get("text","") for p in parts if p.get("type") in ALLOWED]).strip()
        chat_msgs.append({"role": role, "content": text})
    return chat_msgs


def call_openai(messages, temperature=0.7):
    """
    ∆Øu ti√™n /v1/responses; n·∫øu l·ªói -> fallback /v1/chat/completions.
    messages: [{"role":..., "content":[{"type":"input_text","text":"..."}]}]
    """
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": OPENAI_MODEL, "input": messages, "temperature": temperature}
    try:
        t0 = time.time()
        r = requests.post(OPENAI_URL, headers=headers, json=payload, timeout=40)
        dt = (time.time() - t0) * 1000
        print(f"üîÅ OpenAI responses status={r.status_code} in {dt:.0f}ms")
        if r.status_code == 200:
            data = r.json()
            try:
                reply = data["output"][0]["content"][0]["text"]
            except Exception:
                reply = data.get("output_text") or "M√¨nh ƒëang ·ªü ƒë√¢y, s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!"
            return data, reply

        print(f"‚ùå responses body: {r.text[:800]}")
        # Fallback sang chat.completions
        chat_msgs = _to_chat_messages(messages)
        rc = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": chat_msgs, "temperature": temperature},
            timeout=40
        )
        print(f"üîÅ OpenAI chat.status={rc.status_code}")
        if rc.status_code == 200:
            data = rc.json()
            reply = (data.get("choices") or [{}])[0].get("message", {}).get("content") or "..."
            return data, reply

        print(f"‚ùå chat body: {rc.text[:800]}")
        return {}, "Xin l·ªói, hi·ªán m√¨nh g·∫∑p ch√∫t tr·ª•c tr·∫∑c. B·∫°n nh·∫Øn l·∫°i gi√∫p m√¨nh nh√©!"
    except Exception as e:
        print("‚ùå OpenAI error:", repr(e))
        return {}, "Xin l·ªói, hi·ªán m√¨nh g·∫∑p ch√∫t tr·ª•c tr·∫∑c. B·∫°n nh·∫Øn l·∫°i gi√∫p m√¨nh nh√©!"

# === Rephrase m·ªÅm + emoji cute ===
EMOJI_SETS = {
    "generic": ["‚ú®","üôÇ","üòä","üåü","üí´"],
    "greet":   ["üëã","üòä","üôÇ","‚ú®"],
    "browse":  ["üõçÔ∏è","üß≠","üîé","‚ú®"],
    "product": ["üõçÔ∏è","‚ú®","üëç","üíñ"],
    "oos":     ["üôè","‚õî","üòÖ","üõí"],
    "policy":  ["‚ÑπÔ∏è","üì¶","üõ°Ô∏è","‚úÖ"]
}
def em(intent="generic", n=1):
    if EMOJI_MODE == "none": return ""
    arr = EMOJI_SETS.get(intent, EMOJI_SETS["generic"])
    return " " + " ".join(random.choice(arr) for _ in range(max(1, n))).strip()

def rephrase_casual(text: str, intent="generic", temperature=0.7, lang: str = None) -> str:
    """L√†m m·ªÅm c√¢u + th√™m 1‚Äì2 emoji nh·∫π nh√†ng, ƒë√∫ng ng√¥n ng·ªØ lang."""
    if not REPHRASE_ENABLED:
        return text + (em(intent,1) if intent!="generic" else "")
    try:
        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
        msgs = [
            {"role":"system","content":f"B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng, vi·∫øt {lang or 'vi'} t·ª± nhi√™n, th√¢n thi·ªán, ng·∫Øn g·ªçn; th√™m 1‚Äì2 emoji ph√π h·ª£p (kh√¥ng l·∫°m d·ª•ng). Gi·ªØ nguy√™n d·ªØ ki·ªán/gi√°, kh√¥ng b·ªãa."},
            {"role":"user","content": f"Vi·∫øt l·∫°i ƒëo·∫°n sau b·∫±ng {lang or 'vi'} theo gi·ªçng th√¢n thi·ªán, k·∫øt th√∫c b·∫±ng 1 c√¢u ch·ªët h√†nh ƒë·ªông.\n---\n{text}\n---\n{em(intent,2)}"}
        ]
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": msgs, "temperature": temperature, "max_tokens": 220},
            timeout=20
        )
        if r.status_code == 200:
            data = r.json()
            out = (data.get("choices") or [{}])[0].get("message", {}).get("content")
            return out.strip() if out else text
        else:
            print("‚ö†Ô∏è rephrase status=", r.status_code, r.text[:200])
            return text + em(intent,1)
    except Exception as e:
        print("‚ö†Ô∏è rephrase error:", repr(e))
        return text + em(intent,1)
def handle_smalltalk(text: str, lang: str = "vi") -> str:
    # tr·∫£ l·ªùi ng·∫Øn g·ªçn, kh√¥ng g·ªçi rephrase ƒë·ªÉ tr√°nh th√™m CTA b√°n h√†ng
    raw = f"{t(lang, 'smalltalk_hi')} {t(lang, 'smalltalk_askback')}".strip()
    return raw


# ========= FACEBOOK SENDER =========
def fb_call(path, payload=None, method="POST", params=None, page_token=None):
    if not page_token:
        print("‚ùå missing page_token for fb_call")
        return None
    url = f"https://graph.facebook.com/v19.0{path}"
    params = params or {}
    params["access_token"] = page_token
    try:
        r = requests.request(method, url, params=params, json=payload, timeout=15)
        return r
    except Exception as e:
        print("‚ö†Ô∏è FB API error:", repr(e))
        return None

def fb_mark_seen(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"mark_seen"}, page_token=page_token)

def fb_typing_on(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"typing_on"}, page_token=page_token)

def fb_send_text(user_id, text, page_token):
    r = fb_call("/me/messages", {"recipient":{"id":user_id}, "message":{"text":text}}, page_token=page_token)
    print(f"üì© Send text status={getattr(r, 'status_code', None)}")

def fb_send_buttons(user_id, text, buttons, page_token):
    if not buttons: return
    payload = {
        "recipient": {"id": user_id},
        "message": {
            "attachment": {"type": "template","payload": {"template_type": "button","text": text,"buttons": buttons[:2]}}
        }
    }
    r = fb_call("/me/messages", payload, page_token=page_token)
    print(f"üîò ButtonAPI status={getattr(r,'status_code',None)}")

# === Reload vectors (FAISS) ===
CANONICAL_DOMAIN = os.getenv("CANONICAL_DOMAIN", SHOP_URL).rstrip("/")
ALIAS_DOMAINS = [d.strip().rstrip("/") for d in os.getenv("ALIAS_DOMAINS","").split(",") if d.strip()]

def _canon_url(u: str) -> str:
    if not u: return u
    uu = u.strip()
    for dom in ALIAS_DOMAINS:
        if uu.startswith(dom + "/") or uu == dom:
            return CANONICAL_DOMAIN + uu[len(dom):]
    return uu

def _apply_canonical_urls(meta):
    if not meta: return meta
    for d in meta:
        if d.get("url"):
            d["url"] = _canon_url(d["url"])
    return meta

# ========= RAG (FAISS) =========

def _safe_read_index(prefix):
    try:
        idx_path  = os.path.join(VECTOR_DIR, f"{prefix}.index")
        meta_path = os.path.join(VECTOR_DIR, f"{prefix}.meta.json")
        if not (os.path.exists(idx_path) and os.path.exists(meta_path)):
            print(f"‚ö†Ô∏è Missing index/meta for '{prefix}'")
            return None, None
        idx  = faiss.read_index(idx_path)
        meta = json.load(open(meta_path, encoding="utf-8"))

        # --- √Åp canonical domain cho m·ªçi URL trong meta ---
        meta = _apply_canonical_urls(meta)

        print(f"‚úÖ {prefix} loaded: {len(meta)} chunks")
        return idx, meta
    except Exception as e:
        print(f"‚ùå Load index '{prefix}':", repr(e))
        return None, None
    
IDX_PROD, META_PROD = _safe_read_index("products")
IDX_POL,  META_POL  = _safe_read_index("policies")

def _reload_vectors():
    global IDX_PROD, META_PROD, IDX_POL, META_POL
    try:
        IDX_PROD, META_PROD = _safe_read_index("products")
        IDX_POL,  META_POL  = _safe_read_index("policies")
        ok = (IDX_PROD is not None or IDX_POL is not None)
        print("üîÑ Reload vectors:", ok,
              "| prod_chunks=", (len(META_PROD) if META_PROD else 0),
              "| policy_chunks=", (len(META_POL) if META_POL else 0))
        return ok
    except Exception as e:
        print("‚ùå reload vectors:", repr(e))
        return False

@app.post("/admin/reload_vectors")
def admin_reload_vectors():
    ok = _reload_vectors()
    return jsonify({"ok": ok})

def _embed_query(q: str) -> np.ndarray:
    t0 = time.time()
    resp = oai_client.embeddings.create(model=EMBED_MODEL, input=[q])
    v = np.array(resp.data[0].embedding, dtype="float32")[None, :]
    faiss.normalize_L2(v)
    print(f"üß© Embedding in {(time.time()-t0)*1000:.0f}ms")
    return v

def search_products_with_scores(query, topk=8):
    if IDX_PROD is None:
        return [], []
    v = _embed_query(query)
    try:
        D, I = IDX_PROD.search(v, topk)
        hits, scores, seen = [], [], set()
        for idx, score in zip(I[0], D[0]):
            if 0 <= idx < len(META_PROD):
                d = META_PROD[idx]
                key = (d.get("url"), (d.get("title") or "").lower().strip())
                if key in seen:
                    continue
                seen.add(key)
                hits.append(d)
                scores.append(float(score))
        print(f"üìö product hits: {len(hits)}")
        return hits, scores
    except Exception as e:
        print("‚ö†Ô∏è search_products_with_scores:", repr(e))
        return [], []

def retrieve_context(question, topk=6):
    if IDX_PROD is None and IDX_POL is None:
        return ""
    v = _embed_query(question)
    ctx = []
    if IDX_PROD is not None:
        try:
            _, Ip = IDX_PROD.search(v, topk)
            ctx += [META_PROD[i]["text"] for i in Ip[0] if i >= 0]
        except Exception as e:
            print("‚ö†Ô∏è search products:", repr(e))
    if IDX_POL is not None:
        try:
            _, Ik = IDX_POL.search(v, topk)
            ctx += [META_POL[i]["text"] for i in Ik[0] if i >= 0]
        except Exception as e:
            print("‚ö†Ô∏è search policies:", repr(e))
    print("üß† ctx pieces:", len(ctx))
    return "\n\n".join(ctx[:topk]) if ctx else ""
def _parse_ts(s):
    try:
        s = (s or "").replace("Z","").replace("T"," ")
        return time.mktime(time.strptime(s[:19], "%Y-%m-%d %H:%M:%S"))
    except Exception:
        return 0

def get_new_arrivals(days=30, topk=4):
    """T√¨m sp m·ªõi theo timestamp/tags 'new|m·ªõi|v·ª´a v·ªÅ'; fallback FAISS n·∫øu tr·ªëng."""
    if not META_PROD:
        return []
    now = time.time()
    cutoff = now - days*86400
    new_items = []
    for d in META_PROD:
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            if d.get(k):
                ts = max(ts, _parse_ts(d.get(k)))
        tags = (d.get("tags","") or "").lower()
        flag_new = any(x in tags for x in ["new","m·ªõi","v·ª´a v·ªÅ","new arrivals"])
        if flag_new or (ts and ts >= cutoff):
            new_items.append(d)

    if not new_items and IDX_PROD is not None:
        hits, _ = search_products_with_scores("new arrivals h√†ng m·ªõi v·ª´a v·ªÅ", topk=topk*2)
        new_items = hits

    def _key(d):
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            ts = max(ts, _parse_ts(d.get(k)))
        return ts
    new_items.sort(key=_key, reverse=True)
    return new_items[:topk]

def compose_new_arrivals(lang: str = "vi", items=None):
    items = items or []
    if not items:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return rephrase_casual(t(lang,"browse", url=url), intent="browse", lang=lang)
    lines = []
    for d in items[:2]:
        title = d.get("title") or "S·∫£n ph·∫©m"
        price = d.get("price")
        stock = _stock_line(d)
        line = f"‚Ä¢ {title}"
        if price: line += f" ‚Äî {price} ƒë"
        line += f" ‚Äî {stock}"
        lines.append(line)
    raw = f"{t(lang,'new_hdr')}\n" + "\n".join(lines) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)


# ========= INTENT, PERSONA, FEW-SHOT & NATURAL REPLY =========
GREETS = {"hi","hello","hey","helo","heloo","h√≠","h√¨","ch√†o","xin ch√†o","alo","aloha","hello bot","hi bot"}
def is_greeting(text: str) -> bool:
    t = re.sub(r"\s+", " ", (text or "").lower()).strip()
    return any(w in t for w in GREETS) and len(t) <= 40

# ‚Äî‚Äî‚Äî Ng√¥n ng·ªØ: detect & c√¢u ch·ªØ
def detect_lang(text: str) -> str:
    txt = (text or "").strip()
    if not txt: return DEFAULT_LANG
    if re.search(r"[\u4e00-\u9fff]", txt):  # CJK
        return "zh" if "zh" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[\u0E00-\u0E7F]", txt):  # Thai
        return "th" if "th" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[ƒÉ√¢√™√¥∆°∆∞ƒë√°√†·∫£√£·∫°·∫Ø·∫±·∫≥·∫µ·∫∑·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç·ªë·ªì·ªï·ªó·ªô∆°√≥·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ]", txt, flags=re.I):
        return "vi" if "vi" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"\b(yang|dan|tidak|saja|terima|kasih)\b", txt.lower()):
        return "id" if "id" in SUPPORTED_LANGS else DEFAULT_LANG
    return "en" if "en" in SUPPORTED_LANGS else DEFAULT_LANG


# ====== MULTI-LANG PATTERNS (smalltalk & new arrivals) ======
# M·ªói ng√¥n ng·ªØ l√† 1 list regex. C√≥ th·ªÉ b·ªï sung d·∫ßn m√† kh√¥ng ƒë·ª•ng ch·ªó kh√°c.
# ==== Smalltalk & New arrivals (multi-lang) ====

# ========= I18N STRINGS & HELPERS =========
LANG_STRINGS = {
    "vi": {
        "greet": "Xin ch√†o üëã R·∫•t vui ƒë∆∞·ª£c ph·ª•c v·ª• b·∫°n! B·∫°n mu·ªën m√¨nh gi√∫p g√¨ kh√¥ng n√®? üôÇ",
        "browse": "M·ªùi b·∫°n v√†o web tham quan ·∫° üõçÔ∏è üëâ {url}",
        "oos": "Xin l·ªói üôè s·∫£n ph·∫©m ƒë√≥ hi·ªán **ƒëang h·∫øt h√†ng** t·∫°i shop. B·∫°n th·ª≠ xem c√°c m·∫´u t∆∞∆°ng t·ª± tr√™n web nh√© üëâ {url} ‚ú®",
        "fallback": "M√¨nh ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ch·∫Øc ch·∫Øn ü§î. B·∫°n m√¥ t·∫£ th√™m m·∫´u/ki·ªÉu d√°ng/ch·∫•t li·ªáu ƒë·ªÉ m√¨nh t∆∞ v·∫•n chu·∫©n h∆°n nha ‚ú®",
        "suggest_hdr": "M√¨nh ƒë·ªÅ xu·∫•t v√†i l·ª±a ch·ªçn ph√π h·ª£p",
        "product_pts": "B·∫°n th√≠ch ki·ªÉu m·∫£nh hay th·ªÉ thao? M√¨nh l·ªçc th√™m m√†u & size gi√∫p b·∫°n nh√©.",
        "highlights": "{title} c√≥ v√†i ƒëi·ªÉm n·ªïi b·∫≠t n√®",
        "policy_hint": "Theo ch√≠nh s√°ch shop:",
        "smalltalk_hi": "Hi üëã M√¨nh kh·ªèe n√® üòÑ",
        "smalltalk_askback": "H√¥m nay c·ªßa b·∫°n th·∫ø n√†o?",
        "new_hdr": "H√†ng m·ªõi v·ªÅ n√® ‚ú®",
    },
    "en": {
        "greet": "Hello üëã Happy to help! How can I assist you today? üôÇ",
        "browse": "Feel free to explore our store üõçÔ∏è üëâ {url}",
        "oos": "Sorry üôè that item is **out of stock** right now. Check similar picks here üëâ {url} ‚ú®",
        "fallback": "I‚Äôm missing a bit of info ü§î. Share style/material/size and I‚Äôll refine the picks ‚ú®",
        "suggest_hdr": "Here are a few good options",
        "product_pts": "Prefer a slim or sporty style? I can filter color & size for you.",
        "highlights": "{title} highlights",
        "policy_hint": "Store policy:",
         "smalltalk_hi": "Hi üëã I'm good! üòÑ",
        "smalltalk_askback": "How's your day going?",
        "new_hdr": "New arrivals ‚ú®",
    },
    "zh": {
        "greet": "‰Ω†Â•Ω üëã ÂæàÈ´òÂÖ¥‰∏∫‰Ω†ÊúçÂä°ÔºÅÈúÄË¶ÅÊàëÂ∏Æ‰Ω†ÂÅö‰ªÄ‰πàÂë¢ÔºüüôÇ",
        "browse": "Ê¨¢ËøéÈÄõÈÄõÊàë‰ª¨ÁöÑÂïÜÂ∫ó üõçÔ∏è üëâ {url}",
        "oos": "Êä±Ê≠â üôè ËØ•ÂïÜÂìÅÁõÆÂâç**Áº∫Ë¥ß**„ÄÇÂèØ‰ª•ÂÖàÁúãÁúãÁ±ª‰ººÁöÑÊ¨æÂºè üëâ {url} ‚ú®",
        "fallback": "ËøòÈúÄË¶Å‰∏Ä‰∫õ‰ø°ÊÅØÂì¶ ü§î„ÄÇËØ¥‰∏ãÈ£éÊ†º/ÊùêË¥®/Â∞∫ÂØ∏ÔºåÊàëÂÜçÁ≤æÂáÜÊé®Ëçê ‚ú®",
        "suggest_hdr": "Áªô‰Ω†Âá†Ê¨æÂêàÈÄÇÁöÑÈÄâÊã©",
        "product_pts": "ÊÉ≥Ë¶ÅÁ∫§ÁªÜËøòÊòØËøêÂä®È£éÔºüÊàëÂèØ‰ª•ÊåâÈ¢úËâ≤ÂíåÂ∞∫Á†ÅÂÜçÁ≠õ‰∏ÄËΩÆ„ÄÇ",
        "highlights": "{title} ÁöÑ‰∫ÆÁÇπ",
        "policy_hint": "Â∫óÈì∫ÊîøÁ≠ñÔºö",
        "smalltalk_hi": "Âó® üëã ÊàëÂæàÂ•ΩÂñî üòÑ",
        "smalltalk_askback": "‰Ω†‰ªäÂ§©ËøáÂæóÊÄé‰πàÊ†∑Ôºü",
        "new_hdr": "Êñ∞ÂìÅ‰∏äÊû∂ ‚ú®",
    },
    "th": {
        "greet": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ üëã ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á üôÇ",
        "browse": "‡πÄ‡∏ä‡∏¥‡∏ç‡∏ä‡∏°‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ üõçÔ∏è üëâ {url}",
        "oos": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ üôè ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ä‡∏¥‡πâ‡∏ô‡∏ô‡∏±‡πâ‡∏ô **‡∏´‡∏°‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß** ‡∏Ñ‡πà‡∏∞ ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà üëâ {url} ‚ú®",
        "fallback": "‡∏Ç‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î‡∏ô‡∏∞‡∏Ñ‡∏∞/‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô‡∏™‡πÑ‡∏ï‡∏•‡πå/‡∏ß‡∏±‡∏™‡∏î‡∏∏/‡∏Ç‡∏ô‡∏≤‡∏î ‚ú®",
        "suggest_hdr": "‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°",
        "product_pts": "‡∏ä‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏£‡∏µ‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï‡∏î‡∏µ? ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏±‡∏î‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÑ‡∏ã‡∏ã‡πå‡πÉ‡∏´‡πâ‡∏≠‡∏µ‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏∞/‡∏Ñ‡∏£‡∏±‡∏ö",
        "highlights": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á {title}",
        "policy_hint": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡πâ‡∏≤‡∏ô:",
         "smalltalk_hi": "‡πÑ‡∏Æ üëã ‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢‡∏ô‡∏∞ üòÑ",
        "smalltalk_askback": "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á?",
        "new_hdr": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà ‚ú®",
    },
    "id": {
        "greet": "Halo üëã Senang membantu! Ada yang bisa saya bantu? üôÇ",
        "browse": "Silakan jelajahi toko kami üõçÔ∏è üëâ {url}",
        "oos": "Maaf üôè produk itu **sedang kosong**. Coba lihat yang mirip di sini üëâ {url} ‚ú®",
        "fallback": "Butuh info tambahan ü§î. Sebutkan gaya/bahan/ukuran ya, biar saya saringkan ‚ú®",
        "suggest_hdr": "Beberapa pilihan yang cocok",
        "product_pts": "Suka model tipis atau sporty? Saya bisa saring warna & ukuran.",
        "highlights": "Hal menarik dari {title}",
        "policy_hint": "Kebijakan toko:",
       "smalltalk_hi": "Hai üëã Aku baik-baik saja üòÑ",
        "smalltalk_askback": "Harinya kamu gimana?",
        "new_hdr": "Produk baru ‚ú®",
    },
}


def t(lang: str, key: str, **kw) -> str:
    lang2 = lang if lang in LANG_STRINGS else DEFAULT_LANG
    s = (LANG_STRINGS.get(lang2, {}).get(key)
         or LANG_STRINGS.get(DEFAULT_LANG, {}).get(key)
         or "")
    try:
        return s.format(**kw)
    except Exception:
        return s


def greet_text(lang: str) -> str:
    return t(lang, "greet")

# ==== Smalltalk & New arrivals (multi-lang) ====

SMALLTALK_PATTERNS = {
    "vi": [
        # bn/b·∫°n kh·ªèe/kh·ªèe/kho·∫ª kh√¥ng/ko/h√¥ng/hem/hok
        r"\b(bn|b·∫°n)\s*(kh[o√≥√≤·ªè√µ·ªç∆°·ªõ·ªù·ªü·ª°·ª£]e|khoe|kho·∫ª|kh·ªèe)\s*(kh[o√¥]ng|ko|k|h[o∆°√¥√≥√≤√µ·ªè·ªç]ng|hong|h√¥ng|hem|hok)\b",
        r"\b(kh[o√≥√≤·ªè√µ·ªç∆°·ªõ·ªù·ªü·ª°·ª£]e|khoe|kho·∫ª|kh·ªèe)\s*(kh[o√¥]ng|ko|k|h[o∆°√¥√≥√≤√µ·ªè·ªç]ng|hong|h√¥ng|hem|hok)\b",
        # ·ªïn kh√¥ng
        r"\b(·ªïn|on)\s*(kh[o√¥]ng|ko|k|hong|h[o∆°]ng|hem|hok)\b",
        # h√¥m nay th·∫ø n√†o / nay sao
        r"\b(h[o√¥]m?\s*nay|nay)\s*(b·∫°n|bn)?\s*(th[·∫øe]\s*n[a√†]o|sao|ok\s*kh[o√¥]ng)\b",
        # ƒëang l√†m g√¨ / d·∫°o n√†y
        r"\b(ƒëang\s*l√†m\s*g√¨|l√†m\s*g√¨( v·∫≠y| ƒë√≥)?|l√†m\s*chi|lam\s*gi)\b",
        r"\b(d·∫°o\s*n√†y|dao\s*nay)\b",
        # ƒÉn c∆°m ch∆∞a / ng·ªß ch∆∞a
        r"\b(ƒÉn\s*c∆°m\s*ch∆∞a|ƒÉn\s*ch∆∞a|an\s*chua|u·ªëng\s*ch∆∞a|ng[u∆∞]\s*ch[aƒÉ]u?)\b",
        # c·∫£m ∆°n / thanks
        r"\b(c[·∫£a]m\s?∆°n|c[√°a]m\s?∆°n|thanks?|thank you|ty|tks|thx)\b",
        # c∆∞·ªùi/emoji
        r"\b(haha+|hihi+|hehe+|kkk+|=D|:d|:v|:3)\b|[üòÇü§£üòÜ]",
    ],
    "en": [
        r"\b(how('?s)?\s*it\s*going|how\s*are\s*(you|u)|how\s*r\s*u|how\s*u\s*doin?g?)\b",
        r"\b(what('?s)?\s*up|wass?up|sup|wyd)\b",
        r"\b(have\s*you\s*eaten|had\s*(lunch|dinner)|grabbed\s*(lunch|food))\b",
        r"\b(thanks?|thank\s*(you|u)|ty|thx|tysm|tks)\b",
        r"\b(lol|lmao|rofl|haha+|hehe+|:d)\b|[üòÇü§£üòÜ]",
    ],
    "zh": [
        r"(‰Ω†Â•ΩÂêó|Â¶≥Â•ΩÂêó|ÊúÄËøëÊÄé‰πàÊ†∑|ÊúÄËøëÂ¶Ç‰Ωï|ÊúÄËøëËøòÂ•Ω|ËøòÂ•ΩÂêó|ÂøÉÊÉÖÂ¶Ç‰Ωï|ÂºÄÂøÉÂêó|ÈÅéÂæóÊÄéÊ®£|ËøáÂæóÊÄéÊ†∑)",
        r"(ÂêÉÈ•≠‰∫ÜÂêó|ÂêÉËøáÈ•≠Ê≤°|ÂêÉ‰∫ÜÊ≤°|ÂêÉ‰∫ÜÂêó)",
        r"(Ë∞¢Ë∞¢|Â§öË∞¢|Ë¨ùË¨ù|ÊÑüË¨ù|ÊÑüË∞¢|Ë¨ùÂï¶|Ë∞¢Ë∞¢Âï¶|Ë∞¢Âï¶)",
        r"(ÂìàÂìà+|ÂòøÂòø+|ÂëµÂëµ+|Âó®Âó®+)|[üòÇü§£üòÜ]",
    ],
    "th": [
        r"(‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ(‡πÑ‡∏´‡∏°|‡∏°‡∏±‡πâ‡∏¢|‡∏õ‡πà‡∏≤‡∏ß)|‡πÄ‡∏õ‡πá‡∏ô(‡πÑ‡∏á|‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£)‡∏ö‡πâ‡∏≤‡∏á|‡πÇ‡∏≠‡πÄ‡∏Ñ(‡πÑ‡∏´‡∏°|‡∏°‡∏±‡πâ‡∏¢))",
        r"(‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà|‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£|‡∏ó‡∏≥‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà)",
        r"(‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß(‡∏´‡∏£‡∏∑‡∏≠)?‡∏¢‡∏±‡∏á|‡∏ó‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß(‡∏´‡∏£‡∏∑‡∏≠)?‡∏¢‡∏±‡∏á)",
        r"(‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì(‡∏Ñ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏∞)?|‡∏Ç‡∏≠‡∏ö‡πÉ‡∏à|thanks?|thank you|ty)",
        r"(‡∏Æ‡πà‡∏≤+‡πÜ+|555+)|[üòÇü§£üòÜ]",
    ],
    "id": [
        r"(apa\s*kabar|gimana\s*kabarnya|gmn\s*kabar|kabarnya\s*gimana)",
        r"(lagi\s*apa|lg\s*apa|sedang\s*apa|ngapain(\s*nih)?)",
        r"(sudah|udah)\s*makan\s*(belum|blm)",
        r"(terima\s*kasih|terimakasih|trimakasih|makasih|makasi|thanks?|thank you|thx|ty)",
        r"(wkwk+|wk+|haha+|hehe+|:d)|[üòÇü§£üòÜ]",
    ],
}

NEW_ITEMS_PATTERNS = {
    "vi": [
        r"(h√†ng|sp|m·∫´u|s·∫£n\s*ph·∫©m).*(m·ªõi|v·ª´a\s*v·ªÅ|new\s*arrivals)",
        r"(c√≥|ƒë√£).*(m·∫´u|s·∫£n\s*ph·∫©m).*(m·ªõi|v·ª´a\s*v·ªÅ)",
        r"\b(new|m·ªõi|v·ª´a v·ªÅ|new arrivals)\b",
    ],
    "en": [
        r"(new\s*arrivals?|new\s*products?|what's\s*new)",
        r"(any|have).*(new\s*items?)",
    ],
    "zh": [
        r"(Êñ∞ÂìÅ|Êñ∞Âà∞|Êñ∞Ë≤®|Êñ∞Ë¥ß)",
        r"(Êúâ.*Êñ∞(ÂìÅ|Ë¥ß|Ë≤®)|‰æÜ‰∫Ü.*Êñ∞|Êù•‰∫Ü.*Êñ∞)",
    ],
    "th": [
        r"(‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà|‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà|‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà|‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà)",
        r"(‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡∏°‡πà|‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏´‡∏°)",
    ],
    "id": [
        r"(produk baru|barang baru|baru datang)",
        r"(ada yang baru|ada produk baru)",
    ],
}

def _pat(pats: dict, lang: str):
    """L·∫•y list pattern theo ng√¥n ng·ªØ, fallback v·ªÅ DEFAULT_LANG n·∫øu kh√¥ng c√≥."""
    return pats.get(lang) or pats.get(DEFAULT_LANG, [])
# ===== Gi√° / Price questions (multi-lang) =====
PRICE_PATTERNS = {
    "vi": [r"\bgi√°\b", r"bao nhi√™u", r"nhi√™u ti·ªÅn", r"\bgi√° bao nhi√™u\b", r"\bbao nhieu\b"],
    "en": [r"\bprice\b", r"how much", r"\bcost\b"],
    "zh": [r"(‰ª∑Ê†º|ÂπæÈå¢|Â§öÂ∞ëÈí±|Â§öÂ∞ëÈå¢)"],
    "th": [r"(‡∏£‡∏≤‡∏Ñ‡∏≤|‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà|‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£)"],
    "id": [r"(harga|berapa)"],
}
def is_price_question(text: str, lang: str) -> bool:
    raw = (text or "")
    return any(re.search(p, raw, flags=re.I) for p in _pat(PRICE_PATTERNS, lang))


SYSTEM_STYLE = (
    "B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng Aloha t√™n l√† Aloha Bot. T√¥ng gi·ªçng: th√¢n thi·ªán, ch·ªß ƒë·ªông, "
    "tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ ng∆∞·ªùi th·∫≠t; d√πng 1‚Äì3 emoji h·ª£p ng·ªØ c·∫£nh (kh√¥ng l·∫°m d·ª•ng). "
    "Lu√¥n d·ª±a v√†o CONTEXT (n·ªôi dung RAG). Kh√¥ng b·ªãa. N·∫øu thi·∫øu d·ªØ li·ªáu th·ª±c t·∫ø, n√≥i 'm√¨nh ch∆∞a c√≥ d·ªØ li·ªáu' "
    "v√† h·ªèi l·∫°i 1 c√¢u ƒë·ªÉ l√†m r√µ. Tr√¨nh b√†y d·ªÖ ƒë·ªçc: g·∫°ch ƒë·∫ßu d√≤ng khi li·ªát k√™; 1 c√¢u ch·ªët h√†nh ƒë·ªông."
)

# FEW_SHOT_EXAMPLES
FEW_SHOT_EXAMPLES = [
    {"role":"user","content":[{"type":"input_text","text":"helo"}]},
    {"role":"assistant","content":[{"type":"output_text","text":"Xin ch√†o üëã R·∫•t vui ƒë∆∞·ª£c ph·ª•c v·ª• b·∫°n! B·∫°n mu·ªën m√¨nh gi√∫p g√¨ kh√¥ng n√®? üôÇ"}]},
    {"role":"user","content":[{"type":"input_text","text":"shop b·∫°n c√≥ nh·ªØng g√¨"}]},
    {"role":"assistant","content":[{"type":"output_text","text":f"M·ªùi b·∫°n tham quan c·ª≠a h√†ng t·∫°i ƒë√¢y ·∫° üõçÔ∏è üëâ {SHOP_URL_MAP.get('vi', SHOP_URL)}"}]},
]
# ---- Intent routing ----
POLICY_KEYWORDS  = {"ch√≠nh s√°ch","ƒë·ªïi tr·∫£","b·∫£o h√†nh","ship","v·∫≠n chuy·ªÉn","giao h√†ng","tr·∫£ h√†ng","refund"}
PRODUCT_KEYWORDS = {
    "mua","b√°n","gi√°","size","k√≠ch th∆∞·ªõc","ch·∫•t li·ªáu","m√†u","h·ª£p","ph√π h·ª£p",
    "d√¢y","ƒë·ªìng h·ªì","v√≤ng","case","√°o","qu·∫ßn","√°o ph√¥ng","tshirt","t-shirt","√°o thun",
    "s·∫£n ph·∫©m", "b√°nh","crepe","b√°nh crepe","b√°nh s·∫ßu ri√™ng","milktea","tr√† s·ªØa"
}
BROWSE_KEYWORDS  = {"c√≥ nh·ªØng g√¨","b√°n g√¨","c√≥ g√¨","danh m·ª•c","catalog","xem h√†ng","tham quan","xem shop","xem s·∫£n ph·∫©m","shop c√≥ g√¨","nh·ªØng s·∫£n ph·∫©m g√¨"}
_BROWSE_PATTERNS = [
    r"(shop|b√™n b·∫°n|b√™n m√¨nh).*(b√°n|c√≥).*(g√¨|nh·ªØng g√¨|nh·ªØng s·∫£n ph·∫©m g√¨)",
    r"(b√°n|c√≥).*(nh·ªØng\s+)?s·∫£n ph·∫©m g√¨",
]
# ==== Smalltalk & New arrivals ====



def detect_intent(text: str):
    raw = (text or "")
    t0  = re.sub(r"\s+", " ", raw.lower()).strip()
    lang = detect_lang(raw)

    if any(k in t0 for k in POLICY_KEYWORDS):  return "policy"
    if is_greeting(raw):                       return "greet"

    # smalltalk ƒëa ng√¥n ng·ªØ
    if any(re.search(p, raw, flags=re.I) for p in _pat(SMALLTALK_PATTERNS, lang)):
        return "smalltalk"

    # browse: t·ª´ kh√≥a + pattern chung
    if any(k in t0 for k in BROWSE_KEYWORDS):  return "browse"
    if any(re.search(p, t0) for p in _BROWSE_PATTERNS):     return "browse"

    # h·ªèi h√†ng m·ªõi ƒëa ng√¥n ng·ªØ
    if any(re.search(p, raw, flags=re.I) for p in _pat(NEW_ITEMS_PATTERNS, lang)):
        return "new_items"
    # H·ªèi gi√° ‚Üí ∆∞u ti√™n product_info
    if is_price_question(raw, lang):
        return "product_info"

    # s·∫£n ph·∫©m & m√¥ t·∫£
    if any(k in t0 for k in PRODUCT_KEYWORDS): return "product"
    if "c√≥ b√°n" in t0 or "b√°n kh√¥ng" in t0 or "b√°n ko" in t0: return "product"
    if "c√≥ g√¨ ƒë·∫∑c bi·ªát" in t0 or "ƒëi·ªÉm ƒë·∫∑c bi·ªát" in t0 or "c√≥ g√¨ ƒë·∫∑t bi·ªát" in t0: return "product_info"

    return "other"

def build_messages(system, history, context, user_question):
    msgs = [{"role":"system","content":[{"type":"input_text","text":system}]}]
    msgs.extend(FEW_SHOT_EXAMPLES)
    for h in list(history)[-3:]:
        ctype = "output_text" if h["role"] == "assistant" else "input_text"
        msgs.append({"role": h["role"], "content":[{"type": ctype, "text": h["content"]}]})
    user_block = f"(N·∫øu h·ªØu √≠ch th√¨ d√πng CONTEXT)\nCONTEXT:\n{context}\n\nC√ÇU H·ªéI: {user_question}"
    msgs.append({"role":"user","content":[{"type":"input_text","text":user_block}]})
    return msgs


# ---- Hi·ªÉn th·ªã t·ªìn kho/OOS + emoji ----
def _stock_line(d: dict) -> str:
    if d.get("available") and (d.get("inventory_quantity") is None or d.get("inventory_quantity", 0) > 0):
        return "c√≤n h√†ng ‚úÖ"
    if d.get("inventory_quantity") == 0 or (d.get("status") and d.get("status") != "active"):
        return "h·∫øt h√†ng t·∫°m th·ªùi ‚õî"
    return "t√¨nh tr·∫°ng ƒëang c·∫≠p nh·∫≠t ‚è≥"

def _shorten(txt: str, n=280) -> str:
    t = (txt or "").strip()
    return (t[:n].rstrip() + "‚Ä¶") if len(t) > n else t
def _fmt_price(p, currency="‚Ç´"):
    if p is None:
        return None
    try:
        s = re.sub(r"[^\d.]", "", str(p))
        if not s:
            return None
        val = int(float(s))
        return f"{val:,.0f}".replace(",", ".") + (f" {currency}" if currency else "")
    except Exception:
        return str(p)

def _extract_price_number(txt: str):
    """Tr·∫£ v·ªÅ s·ªë (float) n·∫øu b·∫Øt ƒë∆∞·ª£c 199k/199.000ƒë/199000 vnd..., else None"""
    if not txt:
        return None
    low = txt.lower()
    m = re.search(r"(\d[\d\.\s,]{2,})(?:\s?)(ƒë|‚Ç´|vnd|vnƒë|k)\b", low)
    try:
        if m:
            num = re.sub(r"[^\d.]", "", m.group(1))
            v = float(num)
            return v*1000 if m.group(2) == "k" else v
        m2 = re.search(r"\b(\d{5,})\b", low)
        return float(m2.group(1)) if m2 else None
    except Exception:
        return None

def _price_value(d: dict):
    """Tr·∫£ numeric price t·ªët nh·∫•t t·ª´ meta; fallback b·∫Øt trong text."""
    for k in ("price","min_price","max_price"):
        v = d.get(k)
        if v is not None:
            try:
                return float(re.sub(r"[^\d.]", "", str(v)))
            except Exception:
                pass
    return _extract_price_number(d.get("text",""))

def _category_key_from_doc(d: dict):
    """X√°c ƒë·ªãnh 'd√≤ng' s·∫£n ph·∫©m ƒë·ªÉ so min‚Äìmax: ∆∞u ti√™n product_type; fallback theo synonyms trong title/tags."""
    pt = (d.get("product_type") or "").strip()
    if pt:
        return _normalize_text(pt)
    raw = " ".join([d.get("title",""), d.get("tags","")])
    n1, n2 = _norm_both(raw)
    for key in VN_SYNONYMS.keys():
        k1, k2 = _norm_both(key)
        if k1 in n1 or k2 in n2:
            return _normalize_text(key)
    return "misc"

def _minmax_in_category(base_doc: dict):
    """T√¨m 1 m·∫´u r·∫ª nh·∫•t v√† 1 m·∫´u ƒë·∫Øt nh·∫•t c√πng d√≤ng (lo·∫°i). N·∫øu kh√¥ng ƒë·ªß, fallback to√†n shop."""
    if not META_PROD:
        return None, None
    cat = _category_key_from_doc(base_doc)
    def same_cat(x):
        return _category_key_from_doc(x) == cat
    cands = [x for x in META_PROD if same_cat(x)]
    if len(cands) < 2:
        cands = [x for x in META_PROD]  # fallback to√†n shop

    items = []
    for x in cands:
        pv = _price_value(x)
        if pv is not None:
            items.append((pv, x))
    if not items:
        return None, None

    # lo·∫°i ch√≠nh ra kh·ªèi candidates n·∫øu tr√πng URL ho·∫∑c title
    def is_same(a, b):
        return (a.get("url") and a.get("url")==b.get("url")) or \
               ((a.get("title") or "").strip().lower()==(b.get("title") or "").strip().lower())

    items = [(p, x) for (p, x) in items if not is_same(x, base_doc)]
    if not items:
        return None, None

    items.sort(key=lambda t: t[0])
    low = items[0][1]
    high = items[-1][1]
    return low, high


def _extract_features_from_text(text_block: str):
    lines = []
    m = re.search(r"Specs:\s*(.+)", text_block, flags=re.I)
    if m:
        lines.extend(re.split(r"\s*\|\s*|\s*;\s*|,\s*", m.group(1)))
    m2 = re.search(r"Body:\s*(.+?)\s*URL:", text_block, flags=re.I|re.S)
    if m2:
        body = re.sub(r"\s+", " ", m2.group(1)).strip()
        chunks = re.split(r"(?<=[.!?])\s+", body)
        lines.extend(chunks)
    lines = [re.sub(r"\s+", " ", l).strip("‚Ä¢- \n\t") for l in lines if l and len(l.strip()) > 0]
    uniq = []
    for l in lines:
        if l not in uniq:
            uniq.append(l)
        if len(uniq) >= 5:
            break
    return ["‚Ä¢ " + _shorten(x, 80) for x in uniq[:5]]

# ====== T·ª™ KH√ìA / ƒê·ªíNG NGHƒ®A (ƒëa ng√¥n ng·ªØ) ======
VN_SYNONYMS = {
    # ===== ƒê·ªìng h·ªì & ph·ª• ki·ªán =====
    "ƒë·ªìng h·ªì": [
        "dong ho","dong-ho","dongho","watch","watchface","watch face","bezel",
        "galaxy watch","apple watch","amazfit","seiko","casio","nh35","nh36",
        "automatic","mechanical","chronograph"
    ],
    "d√¢y ƒë·ªìng h·ªì": [
        "day dong ho","daydongho","watch band","band","strap","nato","loop",
        "bracelet","mesh","leather strap","metal strap","silicone strap"
    ],
    "case ƒë·ªìng h·ªì": [
        "case dong ho","vo dong ho","bao ve dong ho","bezel protector",
        "watch case","watch bumper","watch cover","protective case"
    ],
    "k√≠nh c∆∞·ªùng l·ª±c": [
        "kinh cuong luc","tempered glass","screen protector","glass protector",
        "full glass","full cover","full glue","9h","anti-scratch","privacy glass"
    ],
    "·ªëp l∆∞ng": [
        "op lung","case","cover","bumper","clear case","tpu case",
        "silicone case","shockproof case","phone case","protective case"
    ],
    "v√≤ng tay": [
        "vong tay","bracelet","bangle","chain bracelet","cuff"
    ],
    "√°o thun": [
        "ao thun","ao phong","tshirt","t-shirt","tee","tee shirt","crewneck",
        "basic tee","unisex tee","oversize tee"
    ],
    "√°o ph√¥ng": [
        "ao phong","ao thun","tshirt","t-shirt","tee"
    ],

    # ===== ƒê·ªì ng·ªçt/ƒë·ªì u·ªëng (b·ªï sung cho shop) =====
    "b√°nh": [
        "banh","cake","gateau","pastry","ÁîúÂìÅ","ÈªûÂøÉ","‡πÄ‡∏Ñ‡πâ‡∏Å","‡∏Ç‡∏ô‡∏°",
        "kue","kueh","roti manis"
    ],
    "b√°nh crepe": [
        "banh crepe","crepe","mille crepe","crepe cake",
        "ÂèØ‰∏ΩÈ•º","ÂèØÈ∫óÈ§Ö","ÂçÉÂ±Ç","ÂçÉÂ±§","ÂçÉÂ±ÇËõãÁ≥ï","ÂçÉÂ±§ËõãÁ≥ï",
        "‡πÄ‡∏Ñ‡∏£‡∏õ","‡πÄ‡∏Ñ‡∏£‡∏õ‡πÄ‡∏Ñ‡πâ‡∏Å","kue crepe","mille crepes","kue lapis"
    ],
    "b√°nh s·∫ßu ri√™ng": [
        "banh sau rieng","durian","durian cake","durian crepe",
        "Ê¶¥Ëé≤","Ê¶¥Êß§","Ê¶¥Ëé≤ÂçÉÂ±Ç","Ê¶¥Êß§ÂçÉÂ±§","Ê¶¥Ëé≤ÂçÉÂ±ÇËõãÁ≥ï","Ê¶¥Êß§ÂçÉÂ±§ËõãÁ≥ï","Ê¶¥Ëé≤ÂèØ‰∏ΩÈ•º","Ê¶¥Êß§ÂèØÈ∫óÈ§Ö",
        "‡πÄ‡∏Ñ‡∏£‡∏õ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡πÄ‡∏Ñ‡πâ‡∏Å‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
        "kue durian","crepe durian","kue lapis durian"
    ],
    "tr√† s·ªØa": [
        "tra sua","milk tea","bubble tea","boba","boba tea","pearl milk tea",
        "Â•∂Ëå∂","ÁèçÁè†Â•∂Ëå∂","Ê≥¢Èú∏Â•∂Ëå∂",
        "‡∏ä‡∏≤‡∏ô‡∏°","‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å",
        "teh susu","teh susu boba","minuman boba","bubble tea id"
    ],
    "milktea": [
        "milk tea","bubble tea","boba","pearl milk tea",
        "Â•∂Ëå∂","ÁèçÁè†Â•∂Ëå∂","‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å","teh susu","boba tea"
    ],

    # ===== Chinese (ZH) ‚Äì nh√≥m theo kh√°i ni·ªám ƒë·ªÉ b·∫Øt r·ªông h∆°n =====
    "ÊâãË°®": ["ËÖïË°®","watch","Ë°®Â∏¶","Ë°®Èèà","Ë°®Âúà","Ë°®Â£≥","Ë°®ÊÆº","Èí¢ÂåñËÜú","ÈãºÂåñËÜú","‰øùÊä§Â£≥","‰øùË≠∑ÊÆº"],
    "Ë°®Â∏¶": ["Ë°®Â∏∂","Ë°®Èìæ","Ë°®Èèà","watch band","strap","ÁöÆË°®Â∏¶","ÈáëÂ±ûË°®Â∏¶","Á°ÖËÉ∂Ë°®Â∏¶"],
    "Èí¢ÂåñËÜú": ["ÈãºÂåñËÜú","ÁéªÁíÉËÜú","Ë¥¥ËÜú","Ë≤ºËÜú","‰øùÊä§ËÜú","‰øùË≠∑ËÜú","tempered glass","screen protector","ÂÖ®ËÉ∂","ÂÖ®ËÜ†","9h"],
    "ÊâãÊú∫Â£≥": ["ÊâãÊ©üÊÆº","‰øùÊä§Â£≥","‰øùË≠∑ÊÆº","ÊâãÊú∫Â•ó","phone case","case","bumper","‰øùË≠∑ÊÆº"],
    "TÊÅ§": ["TÊÅ§Ë°´","Áü≠Ë¢ñ","ÂúÜÈ¢Ü","ÂúìÈ†ò","tee","tshirt","t-shirt"], 
    "Â•∂Ëå∂": ["ÁèçÁè†Â•∂Ëå∂","Ê≥¢Èú∏Â•∂Ëå∂","Â•∂ÁõñËå∂","milk tea","bubble tea","boba"],
    "ÂèØ‰∏ΩÈ•º": ["ÂèØÈ∫óÈ§Ö","Ê≥ïÂºèËñÑÈ•º","Ê≥ïÂºèËñÑÈ§Ö","ÂçÉÂ±Ç","ÂçÉÂ±§","ÂçÉÂ±ÇËõãÁ≥ï","ÂçÉÂ±§ËõãÁ≥ï","crepe","mille crepe"],
    "Ê¶¥Ëé≤": ["Ê¶¥Êß§","durian","Ê¶¥Ëé≤ÂçÉÂ±Ç","Ê¶¥Êß§ÂçÉÂ±§","Ê¶¥Ëé≤ÂèØ‰∏ΩÈ•º","Ê¶¥Êß§ÂèØÈ∫óÈ§Ö","Ê¶¥Ëé≤ËõãÁ≥ï","Ê¶¥Êß§ËõãÁ≥ï"],

    # ===== Thai (TH) =====
    "‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤": ["watch","‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å","‡∏Å‡∏£‡∏≠‡∏ö‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡πÄ‡∏Ñ‡∏™‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤"],
    "‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤": ["watch band","strap","‡∏™‡∏≤‡∏¢‡∏´‡∏ô‡∏±‡∏á","‡∏™‡∏≤‡∏¢‡πÇ‡∏•‡∏´‡∏∞","‡∏™‡∏≤‡∏¢‡∏ã‡∏¥‡∏•‡∏¥‡πÇ‡∏Ñ‡∏ô","‡∏ô‡∏≤‡πÇ‡∏ï‡πâ"],
    "‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å": ["tempered glass","‡∏Å‡∏£‡∏∞‡∏à‡∏Å‡∏Å‡∏±‡∏ô‡∏£‡∏≠‡∏¢","full glue","9h","screen protector"],
    "‡πÄ‡∏Ñ‡∏™‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå": ["‡πÄ‡∏Ñ‡∏™","‡∏ã‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠","bumper","phone case","protective case"],
    "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î": ["tshirt","t-shirt","tee","‡∏Ñ‡∏≠‡∏Å‡∏•‡∏°","‡πÇ‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏ã‡πå"],
    "‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å": ["‡∏ä‡∏≤‡∏ô‡∏°","bubble tea","boba","milk tea"],
    "‡πÄ‡∏Ñ‡∏£‡∏õ": ["‡πÄ‡∏Ñ‡∏£‡∏õ‡πÄ‡∏Ñ‡πâ‡∏Å","crepe","mille crepe"],
    "‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô": ["durian","‡πÄ‡∏Ñ‡∏£‡∏õ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡πÄ‡∏Ñ‡πâ‡∏Å‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô"],

    # ===== Indonesian (ID) =====
    "jam tangan": ["watch","tali jam","pelindung layar","casing jam","bezel","case jam"],
    "tali jam": ["watch band","strap","nato","kulit","logam","silikon"],
    "pelindung layar": ["tempered glass","screen protector","kaca tempered","9h","full glue"],
    "casing hp": ["case","casing","sarung hp","bumper","phone case"],
    "kaos": ["tshirt","t-shirt","tee","kaos oversize","kaos unisex"],
    "teh susu": ["bubble tea","boba","milk tea","minuman boba"],
    "crepe": ["kue crepe","mille crepe","kue lapis","crepe cake"],
    "durian": ["kue durian","crepe durian","kue lapis durian"]
}



def _query_tokens(q: str, lang: str = "vi") -> set:
    """Sinh token t·ª´ c√¢u h·ªèi: c√≥ d·∫•u, kh√¥ng d·∫•u, bigram, v√† c·ª•m ƒë·ªìng nghƒ©a c∆° b·∫£n."""
    n1, n2 = _norm_both(q)
    w1 = [w for w in n1.split() if len(w) > 1]
    w2 = [w for w in n2.split() if len(w) > 1]

    tokens = set(w1) | set(w2)

    # bigram cho c·∫£ c√≥ d·∫•u & kh√¥ng d·∫•u (ƒë·ªÉ b·∫Øt ‚Äús·∫ßu ri√™ng‚Äù, ‚Äúbanh sau‚Äù)
    for words in (w1, w2):
        for i in range(len(words) - 1):
            tokens.add((words[i] + " " + words[i+1]).strip())
            tokens.add((words[i] + words[i+1]).strip())  # bi·∫øn th·ªÉ kh√¥ng space

    combo_phrases = {
        "vi": ["ƒë·ªìng h·ªì","d√¢y ƒë·ªìng h·ªì","k√≠nh c∆∞·ªùng l·ª±c","·ªëp l∆∞ng","√°o thun","√°o ph√¥ng","b√°nh crepe","b√°nh s·∫ßu ri√™ng","tr√† s·ªØa"],
        "en": ["watch band","screen protector","phone case","t shirt","t-shirt","mille crepe","durian crepe","milk tea","bubble tea","boba tea"],
        "zh": ["ÊâãË°®","Ë°®Â∏¶","Èí¢ÂåñËÜú","ÊâãÊú∫Â£≥","TÊÅ§","ÂèØ‰∏ΩÈ•º","Ê¶¥Ëé≤ÂçÉÂ±Ç","Â•∂Ëå∂","ÁèçÁè†Â•∂Ëå∂"],
        "th": ["‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å","‡πÄ‡∏Ñ‡∏™‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå","‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î","‡πÄ‡∏Ñ‡∏£‡∏õ","‡πÄ‡∏Ñ‡∏£‡∏õ‡∏ó‡∏∏‡πÄ‡∏£‡∏µ‡∏¢‡∏ô","‡∏ä‡∏≤‡∏ô‡∏°‡πÑ‡∏Ç‡πà‡∏°‡∏∏‡∏Å"],
        "id": ["jam tangan","tali jam","pelindung layar","casing hp","kaos","kue crepe","crepe durian","teh susu","bubble tea","boba"]
    }

    joined_n1 = " ".join(w1)
    for phrase in combo_phrases.get(lang, []):
        if phrase in joined_n1:
            tokens.add(_normalize_text(phrase))
            tokens.add(_normalize_text(_strip_accents(phrase)))

    # √°nh x·∫° synonyms: n·∫øu text ch·ª©a ‚Äúkey‚Äù th√¨ th√™m t·∫•t c·∫£ synonym v√†o tokens
    for key, syns in VN_SYNONYMS.items():
        key_n1, key_n2 = _norm_both(key)
        if key_n1 in n1 or key_n2 in n2:
            for s in syns:
                s1, s2 = _norm_both(s)
                tokens.add(s1)
                tokens.add(s2)
                tokens.add(s1.replace(" ", ""))
                tokens.add(s2.replace(" ", ""))

    return set(t for t in tokens if len(t) >= 2)


def filter_hits_by_query(hits, q, lang="vi"):
    """Gi·ªØ hit n·∫øu c√≥ token/c·ª•m t·ª´ c√¢u h·ªèi xu·∫•t hi·ªán trong title/tags/type/variant (c√≥ & kh√¥ng d·∫•u)."""
    if not hits:
        return []
    qtoks = _query_tokens(q, lang=lang)

    kept = []
    for d in hits:
        hay_raw = " ".join([
            d.get("title",""), d.get("tags",""), d.get("product_type",""), d.get("variant","")
        ])
        h1, h2 = _norm_both(hay_raw)
        h1_ns, h2_ns = h1.replace(" ", ""), h2.replace(" ", "")

        ok = any(
            (t in h1) or (t in h2) or (t.replace(" ","") in h1_ns) or (t.replace(" ","") in h2_ns)
            for t in qtoks
        )
        if ok:
            kept.append(d)
    return kept


def should_relax_filter(q: str, hits: list) -> bool:
    qn = _normalize_text(q)
    return len(qn.split()) <= 2 and len(hits) > 0

# ====== Compose tr·∫£ l·ªùi ======
def compose_product_reply(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    # ∆Øu ti√™n currency trong meta; n·∫øu kh√¥ng c√≥, m·∫∑c ƒë·ªãnh ‚Ç´ cho VI
    currency = (hits[0].get("currency") or ("‚Ç´" if lang == "vi" else ""))

    items = []
    for d in hits[:2]:
        title     = d.get("title") or "S·∫£n ph·∫©m"
        variant   = d.get("variant")
        stock     = _stock_line(d)

        price_val = _price_value(d)
        price_str = _fmt_price(price_val, currency) if price_val is not None else None

        line = f"‚Ä¢ {title}"
        if variant:
            line += f" ({variant})"
        if price_str:
            line += f" ‚Äî {price_str}"
        line += f" ‚Äî {stock}"
        items.append(line)

    raw = f"{t(lang,'suggest_hdr')}\n" + "\n".join(items) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)

def compose_product_info(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    d = hits[0]
    currency   = d.get("currency") or ("‚Ç´" if lang == "vi" else "")
    title      = d.get("title") or "S·∫£n ph·∫©m"
    stock      = _stock_line(d)

    price_val  = _price_value(d)
    price_line = f"Gi√° tham kh·∫£o: {_fmt_price(price_val, currency)}" if price_val is not None else ""

    bullets = _extract_features_from_text(d.get("text",""))
    body    = "\n".join(bullets) if bullets else "‚Ä¢ Thi·∫øt k·∫ø t·ªëi gi·∫£n, d·ªÖ ph·ªëi ƒë·ªì\n‚Ä¢ Ch·∫•t li·ªáu tho√°ng, d·ªÖ v·ªá sinh"

    parts = [
        f"{t(lang,'highlights', title=title)}",
        body
    ]
    if price_line:
        parts.append(price_line)
    parts.extend([
        f"T√¨nh tr·∫°ng: {stock}",
        t(lang,"product_pts")
    ])

    raw = "\n".join(parts).strip()
    return rephrase_casual(raw, intent="product", lang=lang)


def compose_contextual_answer(context, question, history):
    msgs = build_messages(SYSTEM_STYLE, history, context, question)
    _, reply = call_openai(msgs, temperature=0.6)
    return reply

def compose_price_with_suggestions(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback"), []

    main = hits[0]
    currency = main.get("currency") or ("‚Ç´" if lang == "vi" else "")
    main_price = _price_value(main)
    main_price_str = _fmt_price(main_price, currency) if main_price is not None else "ƒëang c·∫≠p nh·∫≠t"

    low, high = _minmax_in_category(main)

    lines = []
    title = main.get("title") or "S·∫£n ph·∫©m"
    lines.append(f"V√¢ng ·∫°, **{title}** ƒëang ƒë∆∞·ª£c shop b√°n v·ªõi **gi√° c√¥ng khai: {main_price_str}**.")
    sug = []
    if high:
        hp = _fmt_price(_price_value(high), currency)
        sug.append(f"‚Ä¢ **C√πng d√≤ng ‚Äì gi√° cao nh·∫•t:** {high.get('title','SP')} ‚Äî {hp}")
    if low:
        lp = _fmt_price(_price_value(low), currency)  # ƒë√£ s·ª≠a _ue ‚Üí _price_value
        sug.append(f"‚Ä¢ **C√πng d√≤ng ‚Äì gi√° th·∫•p nh·∫•t:** {low.get('title','SP')} ‚Äî {lp}")

    if sug:
        lines.append("B·∫°n c≈©ng c√≥ th·ªÉ tham kh·∫£o th√™m:")
        lines += sug
    lines.append(t(lang, "product_pts"))
    raw = "\n".join(lines)

    btns = [x for x in (high, low) if x]
    return rephrase_casual(raw, intent="product", lang=lang), btns[:2]


def answer_with_rag(user_id, user_question):
    s = _get_sess(user_id)
    hist = s["hist"]

    intent = detect_intent(user_question)
    lang = detect_lang(user_question)
    print(f"üîé intent={intent} | üó£Ô∏è lang={lang}")

    # ‚Äî‚Äî‚Äî QUICK ROUTES ‚Äî‚Äî‚Äî
    if intent == "greet":
        return greet_text(lang), []
    if intent == "smalltalk":
        return handle_smalltalk(user_question, lang=lang), []
    if intent == "browse":
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return t(lang, "browse", url=url), []
    if intent == "new_items":
        items = get_new_arrivals(days=30, topk=4)
        return compose_new_arrivals(lang=lang, items=items), items[:2]

    # ‚Äî‚Äî‚Äî PRODUCT SEARCH ‚Äî‚Äî‚Äî
    prod_hits, prod_scores = search_products_with_scores(user_question, topk=8)
    best = max(prod_scores or [0.0])

    filtered_hits = filter_hits_by_query(prod_hits, user_question, lang=lang) if STRICT_MATCH else prod_hits
    if STRICT_MATCH and not filtered_hits and should_relax_filter(user_question, prod_hits):
        print("üîß relaxed_filter=True (fallback to unfiltered hits)")
        filtered_hits = prod_hits

    # So kh·ªõp ti√™u ƒë·ªÅ t√≠nh tr√™n TO√ÄN B·ªò prod_hits
    title_ok = _has_title_overlap(user_question, prod_hits)

    # N·∫øu g·∫ßn ƒë√∫ng ti√™u ƒë·ªÅ ho·∫∑c c√≥ hit ‚Üí coi nh∆∞ intent=product
    if intent == "other" and (filtered_hits or title_ok):
        intent = "product"

    # N·∫øu tr√πng ti√™u ƒë·ªÅ nh∆∞ng filtered r·ªóng ‚Üí d√πng l·∫°i prod_hits
    if title_ok and not filtered_hits:
        filtered_hits = prod_hits

    print(f"üìà best_score={best:.3f}, hits={len(prod_hits)}, kept_after_filter={len(filtered_hits)}, title_ok={title_ok}")

    # ‚Äî‚Äî‚Äî CONTEXT/POLICY ‚Äî‚Äî‚Äî
    context = retrieve_context(user_question, topk=6)
    if intent == "policy" and context:
        ans = compose_contextual_answer(context, user_question, hist)
        ans = f"{t(lang,'policy_hint')} {ans}"
        return rephrase_casual(ans, intent="policy", temperature=0.5, lang=lang), []

    # ‚Äî‚Äî‚Äî ∆ØU TI√äN H·ªéI GI√Å ‚Äî‚Äî‚Äî
    if is_price_question(user_question, lang) and (filtered_hits or title_ok):
        print("‚û°Ô∏è route=price_question‚Üíprice_with_suggestions")
        chosen = filtered_hits if filtered_hits else prod_hits
        reply, sug_hits = compose_price_with_suggestions(chosen, lang=lang)
        return reply, sug_hits

    # ‚Äî‚Äî‚Äî PRODUCT BRANCHES ‚Äî‚Äî‚Äî
    if intent in {"product", "product_info"}:
        # Kh√¥ng c√≥ hit ho·∫∑c score th·∫•p & kh√¥ng tr√πng ti√™u ƒë·ªÅ ‚Üí OOS/fallback link
        if not filtered_hits or (best < SCORE_MIN and not title_ok):
            url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
            print("‚û°Ô∏è route=oos_hint")
            return t(lang, "oos", url=url), []

    if intent == "product_info":
        print("‚û°Ô∏è route=product_info")
        return compose_product_info(filtered_hits, lang=lang), filtered_hits[:1]

    if intent in {"product", "other"} and filtered_hits and (best >= SCORE_MIN or title_ok):
        print("‚û°Ô∏è route=product_reply")
        return compose_product_reply(filtered_hits, lang=lang), filtered_hits[:2]

    # ‚Äî‚Äî‚Äî CONTEXT FALLBACK ‚Äî‚Äî‚Äî
    if context:
        ans = compose_contextual_answer(context, user_question, hist)
        print("‚û°Ô∏è route=ctx_fallback")
        return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []

    print("‚û°Ô∏è route=fallback")
    return t(lang, "fallback"), []


@app.route("/webhook", methods=["GET", "POST"])
def webhook():
    if request.method == "GET":
        token = request.args.get("hub.verify_token")
        challenge = request.args.get("hub.challenge")
        return (challenge, 200) if token == VERIFY_TOKEN else ("Invalid verification token", 403)

    payload = request.json or {}

    for entry in payload.get("entry", []):
        owner_id = str(entry.get("id"))           # Page ID ho·∫∑c IG Account ID
        access_token = TOKEN_MAP.get(owner_id)
        if not access_token:
            print("‚ö†Ô∏è No token mapped for:", owner_id); 
            continue

        for event in entry.get("messaging", []):
            if event.get("message", {}).get("is_echo"):
                continue
            if event.get("message") and "text" in event["message"]:
                psid = event["sender"]["id"]
                text = event["message"]["text"]
                mid  = event["message"].get("mid")

                sess = _get_sess(psid)
                if mid and sess["last_mid"] == mid: 
                    continue
                sess["last_mid"] = mid

                fb_mark_seen(psid, access_token)
                fb_typing_on(psid, access_token)

                _remember(psid, "user", text)
                reply, btn_hits = answer_with_rag(psid, text)
                _remember(psid, "assistant", reply)

                fb_send_text(psid, reply, access_token)

                if btn_hits:
                    buttons = []
                    for h in btn_hits[:2]:
                        if h.get("url"):
                            buttons.append({"type":"web_url","url":h["url"],"title":(h.get("title") or "Xem s·∫£n ph·∫©m")[:20]})
                    if buttons:
                        fb_send_buttons(psid, "Xem nhanh:", buttons, access_token)

            elif event.get("postback", {}).get("payload"):
                psid = event["sender"]["id"]
                fb_send_text(psid, f"B·∫°n v·ª´a ch·ªçn: {event['postback']['payload']}", access_token)
            # quick reply payload (n·∫øu d√πng quick_replies)
            elif event.get("message", {}).get("quick_reply", {}).get("payload"):
                psid = event["sender"]["id"]
                qr_payload = event["message"]["quick_reply"]["payload"]
                fb_send_text(psid, f"B·∫°n v·ª´a ch·ªçn: {qr_payload}", access_token)


    return "ok", 200


# ========= API =========
@app.route("/api/chat", methods=["POST"])
def chat():
    data = request.json or {}
    messages = data.get("messages", [])
    print("üåê /api/chat len =", len(messages))
    openai_raw, _ = call_openai(messages)
    return jsonify(openai_raw)

@app.route("/api/chat_rag", methods=["POST"])
def chat_rag():
    data = request.json or {}
    q = data.get("question", "")
    print("üåê /api/chat_rag question:", q)
    if not q:
        return jsonify({"error": "Missing 'question'"}), 400
    reply, _ = answer_with_rag("anonymous", q)
    return jsonify({"reply": reply})

@app.route("/api/product_search")
def api_product_search():
    q = (request.args.get("q") or "").strip()
    if not q:
        return jsonify({"ok": False, "msg": "missing q"}), 400
    lang = detect_lang(q)
    hits, scores = search_products_with_scores(q, topk=8)
    best = max(scores or [0.0])
    kept = filter_hits_by_query(hits, q, lang=lang) if STRICT_MATCH else hits
    if STRICT_MATCH and not kept and should_relax_filter(q, hits):
        kept = hits
    if not kept or best < SCORE_MIN:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return jsonify({"ok": True, "reply": t(lang, "oos", url=url), "items": []})
    reply = compose_product_reply(kept, lang=lang)
    return jsonify({"ok": True, "reply": reply, "items": kept[:2]})

# ========= IG OAuth callback & policy pages =========
@app.route("/auth/callback")
def auth_callback():
    code = request.args.get("code")
    print("üîÅ /auth/callback code:", code)
    return f"Auth success! Code: {code}"

@app.route("/privacy")
def privacy():
    return """
    <h1>Privacy Policy - Aloha Bot</h1>
    <p>Ch√∫ng t√¥i ch·ªâ x·ª≠ l√Ω n·ªôi dung tin nh·∫Øn m√† ng∆∞·ªùi d√πng g·ª≠i t·ªõi Fanpage ƒë·ªÉ tr·∫£ l·ªùi.
    Kh√¥ng b√°n/chia s·∫ª d·ªØ li·ªáu c√° nh√¢n. D·ªØ li·ªáu phi√™n tr√≤ chuy·ªán (session) ch·ªâ l∆∞u t·∫°m th·ªùi
    t·ªëi ƒëa 30 ph√∫t ph·ª•c v·ª• tr·∫£ l·ªùi v√† s·∫Ω t·ª± xo√° sau ƒë√≥. Ch·ªâ s·ªë s·∫£n ph·∫©m (vectors) l√† d·ªØ li·ªáu c√¥ng khai t·ª´ c·ª≠a h√†ng.</p>
    <p>Li√™n h·ªá xo√° d·ªØ li·ªáu: g·ª≠i tin nh·∫Øn 'delete my data' t·ªõi Fanpage ho·∫∑c email: <b>hoclac1225@email.com</b>.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

@app.route("/data_deletion")
def data_deletion():
    return """
    <h1>Data Deletion Instructions</h1>
    <p>ƒê·ªÉ y√™u c·∫ßu xo√° d·ªØ li·ªáu: (1) nh·∫Øn 'delete my data' t·ªõi Fanpage, ho·∫∑c (2) g·ª≠i email t·ªõi <b>hoclac1225@email.com</b>
    k√®m ID cu·ªôc tr√≤ chuy·ªán. Ch√∫ng t√¥i s·∫Ω x·ª≠ l√Ω trong th·ªùi gian s·ªõm nh·∫•t.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

# ========= Debug & Health =========
@app.route("/debug/rag_status")
def rag_status():
    return jsonify({
        "vector_dir": os.path.abspath(VECTOR_DIR),
        "products_index": bool(IDX_PROD),
        "products_chunks": len(META_PROD) if META_PROD else 0,
        "policies_index": bool(IDX_POL),
        "policies_chunks": len(META_POL) if META_POL else 0,
        "sessions": len(SESS),
    })

@app.route("/health")
def health():
    return jsonify({
        "ok": True,
        "products": len(META_PROD) if META_PROD else 0,
        "policies": len(META_POL) if META_POL else 0
    })

# ========= Watcher: t·ª± reload khi vector ƒë·ªïi =========
from apscheduler.schedulers.background import BackgroundScheduler

_last_vec_mtime = 0
def _mtime(path):
    try:
        return os.path.getmtime(path)
    except Exception:
        return 0

def _watch_vectors():
    global _last_vec_mtime
    idx_p = os.path.join(VECTOR_DIR, "products.index")
    meta_p = os.path.join(VECTOR_DIR, "products.meta.json")
    idx_k = os.path.join(VECTOR_DIR, "policies.index")
    meta_k = os.path.join(VECTOR_DIR, "policies.meta.json")

    newest = max(_mtime(idx_p), _mtime(meta_p), _mtime(idx_k), _mtime(meta_k))
    if newest and newest != _last_vec_mtime:
        print("üïµÔ∏è Detected vector change ‚Üí reload")
        if _reload_vectors():
            _last_vec_mtime = newest

def _start_vector_watcher():
    try:
        sch = BackgroundScheduler(timezone="Asia/Ho_Chi_Minh")
        sch.add_job(_watch_vectors, "interval", seconds=30, id="watch_vectors")
        sch.start()
        print("‚è±Ô∏è Vector watcher started (30s)")
    except Exception as e:
        print("‚ö†Ô∏è Scheduler error:", repr(e))

# ======== MAIN ========
if __name__ == "__main__":
    _start_vector_watcher()
    port = int(os.getenv("PORT", 3000))
    print(f"üöÄ Starting app on 0.0.0.0:{port}")
    # app.run(host="0.0.0.0", port=port, debug=False)  # khi ch·∫°y local
