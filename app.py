# -*- coding: utf-8 -*-
import unicodedata
import os, json, time, re, requests, numpy as np, faiss, threading, random
from collections import deque
from flask import Flask, request, jsonify, abort
from flask_cors import CORS
from dotenv import load_dotenv
from openai import OpenAI
import hmac, hashlib, base64

# --- text normalize helpers (cÃ³ & khÃ´ng dáº¥u)
def _strip_accents(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if not unicodedata.combining(ch))

def _normalize_text(s: str) -> str:
    # giá»¯ láº¡i chá»¯, sá»‘, khoáº£ng tráº¯ng, cÃ¡c báº£ng chá»¯ cÃ¡i má»Ÿ rá»™ng
    return re.sub(r"[^0-9a-z\u00c0-\u024f\u1e00-\u1eff\u4e00-\u9fff\u0E00-\u0E7F ]", " ", (s or "").lower()).strip()

def _norm_both(s: str):
    """Tráº£ vá» tuple (cÃ³_dáº¥u, khÃ´ng_dáº¥u) Ä‘Ã£ normalize & lower."""
    n1 = _normalize_text(s)
    n2 = _normalize_text(_strip_accents(s))
    return n1, n2
# sá»‘ tá»« tá»‘i thiá»ƒu pháº£i trÃ¹ng trong title (cÃ³ thá»ƒ cho vÃ o ENV náº¿u muá»‘n)
TITLE_MIN_WORDS = int(os.getenv("TITLE_MIN_WORDS", "2"))

def _has_title_overlap(q, hits, min_words: int = TITLE_MIN_WORDS, min_cover: float = 0.6):
    """
    Tráº£ True náº¿u:
    - CÃ³ Ã­t nháº¥t 'min_words' tá»« trong cÃ¢u há»i xuáº¥t hiá»‡n trong title (Ä‘Ã£ normalize, cÃ³/khÃ´ng dáº¥u), HOáº¶C
    - Tá»· lá»‡ phá»§ tá»« (matched/len(tokens)) >= min_cover  (fallback cho cÃ¢u ráº¥t ngáº¯n / ngÃ´n ngá»¯ khÃ´ng cÃ³ khoáº£ng tráº¯ng)
    """
    qn1, qn2 = _norm_both(q)
    # tokens theo khoáº£ng tráº¯ng, bá» tá»« 1 kÃ½ tá»±
    qtok = [w for w in qn1.split() if len(w) > 1]
    if not qtok:                    # vÃ­ dá»¥ tiáº¿ng Trung â†’ khÃ´ng tÃ¡ch Ä‘Æ°á»£c tá»«
        qtok = [qn1]                # fallback: dÃ¹ng cáº£ chuá»—i Ä‘Ã£ normalize

    for d in hits[:5]:
        t1, t2 = _norm_both(d.get("title", ""))
        matched = sum(1 for w in qtok if (w in t1) or (w in t2))

        # Äiá»u kiá»‡n â€œÃ­t nháº¥t N tá»« trÃ¹ngâ€
        cond_min_words = (len(qtok) >= min_words and matched >= min_words)
        # Fallback coverage (giá»¯ logic cÅ©): há»¯u Ã­ch khi cÃ¢u há»i quÃ¡ ngáº¯n
        cond_cover = (matched / max(1, len(qtok))) >= min_cover

        if cond_min_words or cond_cover:
            return True
    return False




# ========= BOOTSTRAP =========
load_dotenv()
app = Flask(__name__)

allowed_origins = os.getenv("ALLOWED_ORIGINS", "")
origins = [o.strip() for o in allowed_origins.split(",")] if allowed_origins else ["*"]
CORS(app, origins=origins)

OPENAI_API_KEY   = os.getenv("OPENAI_API_KEY", "")
VERIFY_TOKEN     = os.getenv("VERIFY_TOKEN", "aloha_verify_123")
# ---- Multi-page map ----
# ---- Multi-id -> token (FB Page & IG) ----
TOKEN_MAP = {}

def _add_map(key, token):
    if key and token:
        TOKEN_MAP[str(key)] = token

for i in range(1, 11):
    pid = os.getenv(f"FB_PAGE_ID_{i}")
    ptk = os.getenv(f"FB_PAGE_TOKEN_{i}")
    _add_map(pid, ptk)  # map Page ID -> Page token

    igid = os.getenv(f"IG_ACCOUNT_ID_{i}")
    _add_map(igid, ptk) # map IG Account ID -> dÃ¹ng CHUNG Page token Ä‘Ã£ liÃªn káº¿t IG
print("TOKEN_MAP size:", len(TOKEN_MAP))


VECTOR_DIR       = os.getenv("VECTOR_DIR", "./vectors")
# Shopify
SHOPIFY_SHOP = os.getenv("SHOPIFY_STORE", "")  # domain *.myshopify.com (tham chiáº¿u)
# Link shop máº·c Ä‘á»‹nh (fallback)
SHOP_URL         = os.getenv("SHOP_URL", "https://shop.aloha.id.vn/zh")
# Äa ngÃ´n ngá»¯
SUPPORTED_LANGS  = [s.strip() for s in os.getenv("SUPPORTED_LANGS", "vi,en,zh,th,id").split(",")]
DEFAULT_LANG     = os.getenv("DEFAULT_LANG", "vi")
SHOP_URL_MAP = {
    "vi": os.getenv("SHOP_URL_VI", SHOP_URL),
    "en": os.getenv("SHOP_URL_EN", SHOP_URL),
    "zh": os.getenv("SHOP_URL_ZH", SHOP_URL),
    "th": os.getenv("SHOP_URL_TH", SHOP_URL),
    "id": os.getenv("SHOP_URL_ID", SHOP_URL),
}

REPHRASE_ENABLED = os.getenv("REPHRASE_ENABLED", "true").lower() == "true"
EMOJI_MODE       = os.getenv("EMOJI_MODE", "cute")  # "cute" | "none"

# Lá»c & ngÆ°á»¡ng Ä‘iá»ƒm
SCORE_MIN = float(os.getenv("PRODUCT_SCORE_MIN", "0.28"))
STRICT_MATCH = os.getenv("STRICT_MATCH", "true").lower() == "true"

print("=== BOOT ===")
print("VECTOR_DIR:", os.path.abspath(VECTOR_DIR))
print("TOKEN_MAP size:", len(TOKEN_MAP))
print("OPENAI key set:", bool(OPENAI_API_KEY))




# OpenAI
OPENAI_URL   = "https://api.openai.com/v1/responses"
OPENAI_MODEL = "gpt-4o-mini"
EMBED_MODEL  = os.getenv("EMBED_MODEL", "text-embedding-3-small")

oai_client   = OpenAI(api_key=OPENAI_API_KEY)

# ========= SMALL IN-MEMORY SESSION =========
SESS = {}
SESS_LOCK = threading.Lock()
SESSION_TTL = 60 * 30  # 30 phÃºt khÃ´ng tÆ°Æ¡ng tÃ¡c thÃ¬ reset

def _get_sess(user_id):
    now = time.time()
    with SESS_LOCK:
        s = SESS.get(user_id)
        if not s or now - s["ts"] > SESSION_TTL:
            s = {"hist": deque(maxlen=8), "last_mid": None, "ts": now}
            SESS[user_id] = s
        s["ts"] = now
        return s

def _remember(user_id, role, text):
    s = _get_sess(user_id)
    s["hist"].append({"role": role, "content": text})

# ========= OPENAI WRAPPER =========
def _to_chat_messages(messages):
    """Chuyá»ƒn format responses -> chat.completions Ä‘á»ƒ fallback."""
    chat_msgs = []
    ALLOWED = {"input_text", "output_text", "text"}  # <-- thÃªm output_text
    for m in messages:
        role = m.get("role", "user")
        parts = m.get("content", [])
        text = "\n".join([p.get("text","") for p in parts if p.get("type") in ALLOWED]).strip()
        chat_msgs.append({"role": role, "content": text})
    return chat_msgs


def call_openai(messages, temperature=0.7):
    """
    Æ¯u tiÃªn /v1/responses; náº¿u lá»—i -> fallback /v1/chat/completions.
    messages: [{"role":..., "content":[{"type":"input_text","text":"..."}]}]
    """
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": OPENAI_MODEL, "input": messages, "temperature": temperature}
    try:
        t0 = time.time()
        r = requests.post(OPENAI_URL, headers=headers, json=payload, timeout=40)
        dt = (time.time() - t0) * 1000
        print(f"ğŸ” OpenAI responses status={r.status_code} in {dt:.0f}ms")
        if r.status_code == 200:
            data = r.json()
            try:
                reply = data["output"][0]["content"][0]["text"]
            except Exception:
                reply = data.get("output_text") or "MÃ¬nh Ä‘ang á»Ÿ Ä‘Ã¢y, sáºµn sÃ ng há»— trá»£ báº¡n!"
            return data, reply

        print(f"âŒ responses body: {r.text[:800]}")
        # Fallback sang chat.completions
        chat_msgs = _to_chat_messages(messages)
        rc = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": chat_msgs, "temperature": temperature},
            timeout=40
        )
        print(f"ğŸ” OpenAI chat.status={rc.status_code}")
        if rc.status_code == 200:
            data = rc.json()
            reply = (data.get("choices") or [{}])[0].get("message", {}).get("content") or "..."
            return data, reply

        print(f"âŒ chat body: {rc.text[:800]}")
        return {}, "Xin lá»—i, hiá»‡n mÃ¬nh gáº·p chÃºt trá»¥c tráº·c. Báº¡n nháº¯n láº¡i giÃºp mÃ¬nh nhÃ©!"
    except Exception as e:
        print("âŒ OpenAI error:", repr(e))
        return {}, "Xin lá»—i, hiá»‡n mÃ¬nh gáº·p chÃºt trá»¥c tráº·c. Báº¡n nháº¯n láº¡i giÃºp mÃ¬nh nhÃ©!"

# === Rephrase má»m + emoji cute ===
EMOJI_SETS = {
    "generic": ["âœ¨","ğŸ™‚","ğŸ˜Š","ğŸŒŸ","ğŸ’«"],
    "greet":   ["ğŸ‘‹","ğŸ˜Š","ğŸ™‚","âœ¨"],
    "browse":  ["ğŸ›ï¸","ğŸ§­","ğŸ”","âœ¨"],
    "product": ["ğŸ›ï¸","âœ¨","ğŸ‘","ğŸ’–"],
    "oos":     ["ğŸ™","â›”","ğŸ˜…","ğŸ›’"],
    "policy":  ["â„¹ï¸","ğŸ“¦","ğŸ›¡ï¸","âœ…"]
}
def em(intent="generic", n=1):
    if EMOJI_MODE == "none": return ""
    arr = EMOJI_SETS.get(intent, EMOJI_SETS["generic"])
    return " " + " ".join(random.choice(arr) for _ in range(max(1, n))).strip()

def rephrase_casual(text: str, intent="generic", temperature=0.7, lang: str = None) -> str:
    """LÃ m má»m cÃ¢u + thÃªm 1â€“2 emoji nháº¹ nhÃ ng, Ä‘Ãºng ngÃ´n ngá»¯ lang."""
    if not REPHRASE_ENABLED:
        return text + (em(intent,1) if intent!="generic" else "")
    try:
        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
        msgs = [
            {"role":"system","content":f"Báº¡n lÃ  trá»£ lÃ½ bÃ¡n hÃ ng, viáº¿t {lang or 'vi'} tá»± nhiÃªn, thÃ¢n thiá»‡n, ngáº¯n gá»n; thÃªm 1â€“2 emoji phÃ¹ há»£p (khÃ´ng láº¡m dá»¥ng). Giá»¯ nguyÃªn dá»¯ kiá»‡n/giÃ¡, khÃ´ng bá»‹a."},
            {"role":"user","content": f"Viáº¿t láº¡i Ä‘oáº¡n sau báº±ng {lang or 'vi'} theo giá»ng thÃ¢n thiá»‡n, káº¿t thÃºc báº±ng 1 cÃ¢u chá»‘t hÃ nh Ä‘á»™ng.\n---\n{text}\n---\n{em(intent,2)}"}
        ]
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": msgs, "temperature": temperature, "max_tokens": 220},
            timeout=20
        )
        if r.status_code == 200:
            data = r.json()
            out = (data.get("choices") or [{}])[0].get("message", {}).get("content")
            return out.strip() if out else text
        else:
            print("âš ï¸ rephrase status=", r.status_code, r.text[:200])
            return text + em(intent,1)
    except Exception as e:
        print("âš ï¸ rephrase error:", repr(e))
        return text + em(intent,1)
def handle_smalltalk(text: str, lang: str = "vi") -> str:
    # tráº£ lá»i ngáº¯n gá»n, khÃ´ng gá»i rephrase Ä‘á»ƒ trÃ¡nh thÃªm CTA bÃ¡n hÃ ng
    raw = f"{t(lang, 'smalltalk_hi')} {t(lang, 'smalltalk_askback')}".strip()
    return raw


# ========= FACEBOOK SENDER =========
def fb_call(path, payload=None, method="POST", params=None, page_token=None):
    if not page_token:
        print("âŒ missing page_token for fb_call")
        return None
    url = f"https://graph.facebook.com/v19.0{path}"
    params = params or {}
    params["access_token"] = page_token
    try:
        r = requests.request(method, url, params=params, json=payload, timeout=15)
        return r
    except Exception as e:
        print("âš ï¸ FB API error:", repr(e))
        return None

def fb_mark_seen(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"mark_seen"}, page_token=page_token)

def fb_typing_on(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"typing_on"}, page_token=page_token)

def fb_send_text(user_id, text, page_token):
    r = fb_call("/me/messages", {"recipient":{"id":user_id}, "message":{"text":text}}, page_token=page_token)
    print(f"ğŸ“© Send text status={getattr(r, 'status_code', None)}")

def fb_send_buttons(user_id, text, buttons, page_token):
    if not buttons: return
    payload = {
        "recipient": {"id": user_id},
        "message": {
            "attachment": {"type": "template","payload": {"template_type": "button","text": text,"buttons": buttons[:2]}}
        }
    }
    r = fb_call("/me/messages", payload, page_token=page_token)
    print(f"ğŸ”˜ ButtonAPI status={getattr(r,'status_code',None)}")

# === Reload vectors (FAISS) ===
CANONICAL_DOMAIN = os.getenv("CANONICAL_DOMAIN", SHOP_URL).rstrip("/")
ALIAS_DOMAINS = [d.strip().rstrip("/") for d in os.getenv("ALIAS_DOMAINS","").split(",") if d.strip()]

def _canon_url(u: str) -> str:
    if not u: return u
    uu = u.strip()
    for dom in ALIAS_DOMAINS:
        if uu.startswith(dom + "/") or uu == dom:
            return CANONICAL_DOMAIN + uu[len(dom):]
    return uu

def _apply_canonical_urls(meta):
    if not meta: return meta
    for d in meta:
        if d.get("url"):
            d["url"] = _canon_url(d["url"])
    return meta

# ========= RAG (FAISS) =========

def _safe_read_index(prefix):
    try:
        idx_path  = os.path.join(VECTOR_DIR, f"{prefix}.index")
        meta_path = os.path.join(VECTOR_DIR, f"{prefix}.meta.json")
        if not (os.path.exists(idx_path) and os.path.exists(meta_path)):
            print(f"âš ï¸ Missing index/meta for '{prefix}'")
            return None, None
        idx  = faiss.read_index(idx_path)
        meta = json.load(open(meta_path, encoding="utf-8"))

        # --- Ãp canonical domain cho má»i URL trong meta ---
        meta = _apply_canonical_urls(meta)

        print(f"âœ… {prefix} loaded: {len(meta)} chunks")
        return idx, meta
    except Exception as e:
        print(f"âŒ Load index '{prefix}':", repr(e))
        return None, None
    
IDX_PROD, META_PROD = _safe_read_index("products")
IDX_POL,  META_POL  = _safe_read_index("policies")

def _reload_vectors():
    global IDX_PROD, META_PROD, IDX_POL, META_POL
    try:
        IDX_PROD, META_PROD = _safe_read_index("products")
        IDX_POL,  META_POL  = _safe_read_index("policies")
        ok = (IDX_PROD is not None or IDX_POL is not None)
        print("ğŸ”„ Reload vectors:", ok,
              "| prod_chunks=", (len(META_PROD) if META_PROD else 0),
              "| policy_chunks=", (len(META_POL) if META_POL else 0))
        return ok
    except Exception as e:
        print("âŒ reload vectors:", repr(e))
        return False

@app.post("/admin/reload_vectors")
def admin_reload_vectors():
    ok = _reload_vectors()
    return jsonify({"ok": ok})

def _embed_query(q: str) -> np.ndarray:
    t0 = time.time()
    resp = oai_client.embeddings.create(model=EMBED_MODEL, input=[q])
    v = np.array(resp.data[0].embedding, dtype="float32")[None, :]
    faiss.normalize_L2(v)
    print(f"ğŸ§© Embedding in {(time.time()-t0)*1000:.0f}ms")
    return v

def search_products_with_scores(query, topk=8):
    if IDX_PROD is None:
        return [], []
    v = _embed_query(query)
    try:
        D, I = IDX_PROD.search(v, topk)
        hits, scores, seen = [], [], set()
        for idx, score in zip(I[0], D[0]):
            if 0 <= idx < len(META_PROD):
                d = META_PROD[idx]
                key = (d.get("url"), (d.get("title") or "").lower().strip())
                if key in seen:
                    continue
                seen.add(key)
                hits.append(d)
                scores.append(float(score))
        print(f"ğŸ“š product hits: {len(hits)}")
        return hits, scores
    except Exception as e:
        print("âš ï¸ search_products_with_scores:", repr(e))
        return [], []

def retrieve_context(question, topk=6):
    if IDX_PROD is None and IDX_POL is None:
        return ""
    v = _embed_query(question)
    ctx = []
    if IDX_PROD is not None:
        try:
            _, Ip = IDX_PROD.search(v, topk)
            ctx += [META_PROD[i]["text"] for i in Ip[0] if i >= 0]
        except Exception as e:
            print("âš ï¸ search products:", repr(e))
    if IDX_POL is not None:
        try:
            _, Ik = IDX_POL.search(v, topk)
            ctx += [META_POL[i]["text"] for i in Ik[0] if i >= 0]
        except Exception as e:
            print("âš ï¸ search policies:", repr(e))
    print("ğŸ§  ctx pieces:", len(ctx))
    return "\n\n".join(ctx[:topk]) if ctx else ""
def _parse_ts(s):
    try:
        s = (s or "").replace("Z","").replace("T"," ")
        return time.mktime(time.strptime(s[:19], "%Y-%m-%d %H:%M:%S"))
    except Exception:
        return 0

def get_new_arrivals(days=30, topk=4):
    """TÃ¬m sp má»›i theo timestamp/tags 'new|má»›i|vá»«a vá»'; fallback FAISS náº¿u trá»‘ng."""
    if not META_PROD:
        return []
    now = time.time()
    cutoff = now - days*86400
    new_items = []
    for d in META_PROD:
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            if d.get(k):
                ts = max(ts, _parse_ts(d.get(k)))
        tags = (d.get("tags","") or "").lower()
        flag_new = any(x in tags for x in ["new","má»›i","vá»«a vá»","new arrivals"])
        if flag_new or (ts and ts >= cutoff):
            new_items.append(d)

    if not new_items and IDX_PROD is not None:
        hits, _ = search_products_with_scores("new arrivals hÃ ng má»›i vá»«a vá»", topk=topk*2)
        new_items = hits

    def _key(d):
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            ts = max(ts, _parse_ts(d.get(k)))
        return ts
    new_items.sort(key=_key, reverse=True)
    return new_items[:topk]

def compose_new_arrivals(lang: str = "vi", items=None):
    items = items or []
    if not items:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return rephrase_casual(t(lang,"browse", url=url), intent="browse", lang=lang)
    lines = []
    for d in items[:2]:
        title = d.get("title") or "Sáº£n pháº©m"
        price = d.get("price")
        stock = _stock_line(d)
        line = f"â€¢ {title}"
        if price: line += f" â€” {price} Ä‘"
        line += f" â€” {stock}"
        lines.append(line)
    raw = f"{t(lang,'new_hdr')}\n" + "\n".join(lines) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)


# ========= INTENT, PERSONA, FEW-SHOT & NATURAL REPLY =========
GREETS = {"hi","hello","hey","helo","heloo","hÃ­","hÃ¬","chÃ o","xin chÃ o","alo","aloha","hello bot","hi bot"}
def is_greeting(text: str) -> bool:
    t = re.sub(r"\s+", " ", (text or "").lower()).strip()
    return any(w in t for w in GREETS) and len(t) <= 40

# â€”â€”â€” NgÃ´n ngá»¯: detect & cÃ¢u chá»¯
def detect_lang(text: str) -> str:
    txt = (text or "").strip()
    if not txt: return DEFAULT_LANG
    if re.search(r"[\u4e00-\u9fff]", txt):  # CJK
        return "zh" if "zh" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[\u0E00-\u0E7F]", txt):  # Thai
        return "th" if "th" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[ÄƒÃ¢ÃªÃ´Æ¡Æ°Ä‘Ã¡Ã áº£Ã£áº¡áº¯áº±áº³áºµáº·áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹áº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»á»‘á»“á»•á»—á»™Æ¡Ã³á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ]", txt, flags=re.I):
        return "vi" if "vi" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"\b(yang|dan|tidak|saja|terima|kasih)\b", txt.lower()):
        return "id" if "id" in SUPPORTED_LANGS else DEFAULT_LANG
    return "en" if "en" in SUPPORTED_LANGS else DEFAULT_LANG


# ====== MULTI-LANG PATTERNS (smalltalk & new arrivals) ======
# Má»—i ngÃ´n ngá»¯ lÃ  1 list regex. CÃ³ thá»ƒ bá»• sung dáº§n mÃ  khÃ´ng Ä‘á»¥ng chá»— khÃ¡c.
# ==== Smalltalk & New arrivals (multi-lang) ====

# ========= I18N STRINGS & HELPERS =========
LANG_STRINGS = {
    "vi": {
        "greet": "Xin chÃ o ğŸ‘‹ Ráº¥t vui Ä‘Æ°á»£c phá»¥c vá»¥ báº¡n! Báº¡n muá»‘n mÃ¬nh giÃºp gÃ¬ khÃ´ng nÃ¨? ğŸ™‚",
        "browse": "Má»i báº¡n vÃ o web tham quan áº¡ ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "Xin lá»—i ğŸ™ sáº£n pháº©m Ä‘Ã³ hiá»‡n **Ä‘ang háº¿t hÃ ng** táº¡i shop. Báº¡n thá»­ xem cÃ¡c máº«u tÆ°Æ¡ng tá»± trÃªn web nhÃ© ğŸ‘‰ {url} âœ¨",
        "fallback": "MÃ¬nh chÆ°a Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ cháº¯c cháº¯n ğŸ¤”. Báº¡n mÃ´ táº£ thÃªm máº«u/kiá»ƒu dÃ¡ng/cháº¥t liá»‡u Ä‘á»ƒ mÃ¬nh tÆ° váº¥n chuáº©n hÆ¡n nha âœ¨",
        "suggest_hdr": "MÃ¬nh Ä‘á» xuáº¥t vÃ i lá»±a chá»n phÃ¹ há»£p",
        "product_pts": "Báº¡n thÃ­ch kiá»ƒu máº£nh hay thá»ƒ thao? MÃ¬nh lá»c thÃªm mÃ u & size giÃºp báº¡n nhÃ©.",
        "highlights": "{title} cÃ³ vÃ i Ä‘iá»ƒm ná»•i báº­t nÃ¨",
        "policy_hint": "Theo chÃ­nh sÃ¡ch shop:",
        "smalltalk_hi": "Hi ğŸ‘‹ MÃ¬nh khá»e nÃ¨ ğŸ˜„",
        "smalltalk_askback": "HÃ´m nay cá»§a báº¡n tháº¿ nÃ o?",
        "new_hdr": "HÃ ng má»›i vá» nÃ¨ âœ¨",
    },
    "en": {
        "greet": "Hello ğŸ‘‹ Happy to help! How can I assist you today? ğŸ™‚",
        "browse": "Feel free to explore our store ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "Sorry ğŸ™ that item is **out of stock** right now. Check similar picks here ğŸ‘‰ {url} âœ¨",
        "fallback": "Iâ€™m missing a bit of info ğŸ¤”. Share style/material/size and Iâ€™ll refine the picks âœ¨",
        "suggest_hdr": "Here are a few good options",
        "product_pts": "Prefer a slim or sporty style? I can filter color & size for you.",
        "highlights": "{title} highlights",
        "policy_hint": "Store policy:",
         "smalltalk_hi": "Hi ğŸ‘‹ I'm good! ğŸ˜„",
        "smalltalk_askback": "How's your day going?",
        "new_hdr": "New arrivals âœ¨",
    },
    "zh": {
        "greet": "ä½ å¥½ ğŸ‘‹ å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼éœ€è¦æˆ‘å¸®ä½ åšä»€ä¹ˆå‘¢ï¼ŸğŸ™‚",
        "browse": "æ¬¢è¿é€›é€›æˆ‘ä»¬çš„å•†åº— ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "æŠ±æ­‰ ğŸ™ è¯¥å•†å“ç›®å‰**ç¼ºè´§**ã€‚å¯ä»¥å…ˆçœ‹çœ‹ç±»ä¼¼çš„æ¬¾å¼ ğŸ‘‰ {url} âœ¨",
        "fallback": "è¿˜éœ€è¦ä¸€äº›ä¿¡æ¯å“¦ ğŸ¤”ã€‚è¯´ä¸‹é£æ ¼/æè´¨/å°ºå¯¸ï¼Œæˆ‘å†ç²¾å‡†æ¨è âœ¨",
        "suggest_hdr": "ç»™ä½ å‡ æ¬¾åˆé€‚çš„é€‰æ‹©",
        "product_pts": "æƒ³è¦çº¤ç»†è¿˜æ˜¯è¿åŠ¨é£ï¼Ÿæˆ‘å¯ä»¥æŒ‰é¢œè‰²å’Œå°ºç å†ç­›ä¸€è½®ã€‚",
        "highlights": "{title} çš„äº®ç‚¹",
        "policy_hint": "åº—é“ºæ”¿ç­–ï¼š",
        "smalltalk_hi": "å—¨ ğŸ‘‹ æˆ‘å¾ˆå¥½å–” ğŸ˜„",
        "smalltalk_askback": "ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ",
        "new_hdr": "æ–°å“ä¸Šæ¶ âœ¨",
    },
    "th": {
        "greet": "à¸ªà¸§à¸±à¸ªà¸”à¸µ ğŸ‘‹ à¸¢à¸´à¸™à¸”à¸µà¹ƒà¸«à¹‰à¸šà¸£à¸´à¸à¸²à¸£à¸™à¸°à¸„à¸£à¸±à¸š/à¸„à¹ˆà¸° à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹ƒà¸«à¹‰à¸Šà¹ˆà¸§à¸¢à¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡ ğŸ™‚",
        "browse": "à¹€à¸Šà¸´à¸à¸Šà¸¡à¸ªà¸´à¸™à¸„à¹‰à¸²à¹ƒà¸™à¹€à¸§à¹‡à¸šà¹„à¸”à¹‰à¹€à¸¥à¸¢ ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "à¸‚à¸­à¸­à¸ à¸±à¸¢ ğŸ™ à¸ªà¸´à¸™à¸„à¹‰à¸²à¸Šà¸´à¹‰à¸™à¸™à¸±à¹‰à¸™ **à¸«à¸¡à¸”à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§** à¸„à¹ˆà¸° à¸¥à¸­à¸‡à¸”à¸¹à¸£à¸¸à¹ˆà¸™à¹ƒà¸à¸¥à¹‰à¹€à¸„à¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸™à¸µà¹ˆ ğŸ‘‰ {url} âœ¨",
        "fallback": "à¸‚à¸­à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹€à¸à¸´à¹ˆà¸¡à¸­à¸µà¸à¸™à¸´à¸”à¸™à¸°à¸„à¸°/à¸„à¸£à¸±à¸š à¹€à¸Šà¹ˆà¸™à¸ªà¹„à¸•à¸¥à¹Œ/à¸§à¸±à¸ªà¸”à¸¸/à¸‚à¸™à¸²à¸” âœ¨",
        "suggest_hdr": "à¸‚à¸­à¹à¸™à¸°à¸™à¸³à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡",
        "product_pts": "à¸Šà¸­à¸šà¹à¸šà¸šà¹€à¸à¸£à¸µà¸¢à¸§à¸«à¸£à¸·à¸­à¸ªà¸›à¸­à¸£à¹Œà¸•à¸”à¸µ? à¹€à¸”à¸µà¹‹à¸¢à¸§à¸Šà¹ˆà¸§à¸¢à¸„à¸±à¸”à¸ªà¸µà¹à¸¥à¸°à¹„à¸‹à¸‹à¹Œà¹ƒà¸«à¹‰à¸­à¸µà¸à¹„à¸”à¹‰à¸„à¹ˆà¸°/à¸„à¸£à¸±à¸š",
        "highlights": "à¸ˆà¸¸à¸”à¹€à¸”à¹ˆà¸™à¸‚à¸­à¸‡ {title}",
        "policy_hint": "à¸™à¹‚à¸¢à¸šà¸²à¸¢à¸£à¹‰à¸²à¸™:",
         "smalltalk_hi": "à¹„à¸® ğŸ‘‹ à¸ªà¸šà¸²à¸¢à¸”à¸µà¸¡à¸²à¸à¹€à¸¥à¸¢à¸™à¸° ğŸ˜„",
        "smalltalk_askback": "à¸§à¸±à¸™à¸™à¸µà¹‰à¸‚à¸­à¸‡à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™à¸¢à¸±à¸‡à¹„à¸‡à¸šà¹‰à¸²à¸‡?",
        "new_hdr": "à¸ªà¸´à¸™à¸„à¹‰à¸²à¹€à¸‚à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆ âœ¨",
    },
    "id": {
        "greet": "Halo ğŸ‘‹ Senang membantu! Ada yang bisa saya bantu? ğŸ™‚",
        "browse": "Silakan jelajahi toko kami ğŸ›ï¸ ğŸ‘‰ {url}",
        "oos": "Maaf ğŸ™ produk itu **sedang kosong**. Coba lihat yang mirip di sini ğŸ‘‰ {url} âœ¨",
        "fallback": "Butuh info tambahan ğŸ¤”. Sebutkan gaya/bahan/ukuran ya, biar saya saringkan âœ¨",
        "suggest_hdr": "Beberapa pilihan yang cocok",
        "product_pts": "Suka model tipis atau sporty? Saya bisa saring warna & ukuran.",
        "highlights": "Hal menarik dari {title}",
        "policy_hint": "Kebijakan toko:",
       "smalltalk_hi": "Hai ğŸ‘‹ Aku baik-baik saja ğŸ˜„",
        "smalltalk_askback": "Harinya kamu gimana?",
        "new_hdr": "Produk baru âœ¨",
    },
}


def t(lang: str, key: str, **kw) -> str:
    lang2 = lang if lang in LANG_STRINGS else DEFAULT_LANG
    s = (LANG_STRINGS.get(lang2, {}).get(key)
         or LANG_STRINGS.get(DEFAULT_LANG, {}).get(key)
         or "")
    try:
        return s.format(**kw)
    except Exception:
        return s


def greet_text(lang: str) -> str:
    return t(lang, "greet")

# ==== Smalltalk & New arrivals (multi-lang) ====

SMALLTALK_PATTERNS = {
    "vi": [
        # bn/báº¡n khá»e/khá»e/khoáº» khÃ´ng/ko/hÃ´ng/hem/hok
        r"\b(bn|báº¡n)\s*(kh[oÃ³Ã²á»Ãµá»Æ¡á»›á»á»Ÿá»¡á»£]e|khoe|khoáº»|khá»e)\s*(kh[oÃ´]ng|ko|k|h[oÆ¡Ã´Ã³Ã²Ãµá»á»]ng|hong|hÃ´ng|hem|hok)\b",
        r"\b(kh[oÃ³Ã²á»Ãµá»Æ¡á»›á»á»Ÿá»¡á»£]e|khoe|khoáº»|khá»e)\s*(kh[oÃ´]ng|ko|k|h[oÆ¡Ã´Ã³Ã²Ãµá»á»]ng|hong|hÃ´ng|hem|hok)\b",
        # á»•n khÃ´ng
        r"\b(á»•n|on)\s*(kh[oÃ´]ng|ko|k|hong|h[oÆ¡]ng|hem|hok)\b",
        # hÃ´m nay tháº¿ nÃ o / nay sao
        r"\b(h[oÃ´]m?\s*nay|nay)\s*(báº¡n|bn)?\s*(th[áº¿e]\s*n[aÃ ]o|sao|ok\s*kh[oÃ´]ng)\b",
        # Ä‘ang lÃ m gÃ¬ / dáº¡o nÃ y
        r"\b(Ä‘ang\s*lÃ m\s*gÃ¬|lÃ m\s*gÃ¬( váº­y| Ä‘Ã³)?|lÃ m\s*chi|lam\s*gi)\b",
        r"\b(dáº¡o\s*nÃ y|dao\s*nay)\b",
        # Äƒn cÆ¡m chÆ°a / ngá»§ chÆ°a
        r"\b(Äƒn\s*cÆ¡m\s*chÆ°a|Äƒn\s*chÆ°a|an\s*chua|uá»‘ng\s*chÆ°a|ng[uÆ°]\s*ch[aÄƒ]u?)\b",
        # cáº£m Æ¡n / thanks
        r"\b(c[áº£a]m\s?Æ¡n|c[Ã¡a]m\s?Æ¡n|thanks?|thank you|ty|tks|thx)\b",
        # cÆ°á»i/emoji
        r"\b(haha+|hihi+|hehe+|kkk+|=D|:d|:v|:3)\b|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "en": [
        r"\b(how('?s)?\s*it\s*going|how\s*are\s*(you|u)|how\s*r\s*u|how\s*u\s*doin?g?)\b",
        r"\b(what('?s)?\s*up|wass?up|sup|wyd)\b",
        r"\b(have\s*you\s*eaten|had\s*(lunch|dinner)|grabbed\s*(lunch|food))\b",
        r"\b(thanks?|thank\s*(you|u)|ty|thx|tysm|tks)\b",
        r"\b(lol|lmao|rofl|haha+|hehe+|:d)\b|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "zh": [
        r"(ä½ å¥½å—|å¦³å¥½å—|æœ€è¿‘æ€ä¹ˆæ ·|æœ€è¿‘å¦‚ä½•|æœ€è¿‘è¿˜å¥½|è¿˜å¥½å—|å¿ƒæƒ…å¦‚ä½•|å¼€å¿ƒå—|éå¾—æ€æ¨£|è¿‡å¾—æ€æ ·)",
        r"(åƒé¥­äº†å—|åƒè¿‡é¥­æ²¡|åƒäº†æ²¡|åƒäº†å—)",
        r"(è°¢è°¢|å¤šè°¢|è¬è¬|æ„Ÿè¬|æ„Ÿè°¢|è¬å•¦|è°¢è°¢å•¦|è°¢å•¦)",
        r"(å“ˆå“ˆ+|å˜¿å˜¿+|å‘µå‘µ+|å—¨å—¨+)|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "th": [
        r"(à¸ªà¸šà¸²à¸¢à¸”à¸µ(à¹„à¸«à¸¡|à¸¡à¸±à¹‰à¸¢|à¸›à¹ˆà¸²à¸§)|à¹€à¸›à¹‡à¸™(à¹„à¸‡|à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£)à¸šà¹‰à¸²à¸‡|à¹‚à¸­à¹€à¸„(à¹„à¸«à¸¡|à¸¡à¸±à¹‰à¸¢))",
        r"(à¸—à¸³à¸­à¸°à¹„à¸£à¸­à¸¢à¸¹à¹ˆ|à¸à¸³à¸¥à¸±à¸‡à¸—à¸³à¸­à¸°à¹„à¸£|à¸—à¸³à¹„à¸£à¸­à¸¢à¸¹à¹ˆ)",
        r"(à¸à¸´à¸™à¸‚à¹‰à¸²à¸§(à¸«à¸£à¸·à¸­)?à¸¢à¸±à¸‡|à¸—à¸²à¸™à¸‚à¹‰à¸²à¸§(à¸«à¸£à¸·à¸­)?à¸¢à¸±à¸‡)",
        r"(à¸‚à¸­à¸šà¸„à¸¸à¸“(à¸„à¸£à¸±à¸š|à¸„à¹ˆà¸°)?|à¸‚à¸­à¸šà¹ƒà¸ˆ|thanks?|thank you|ty)",
        r"(à¸®à¹ˆà¸²+à¹†+|555+)|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
    "id": [
        r"(apa\s*kabar|gimana\s*kabarnya|gmn\s*kabar|kabarnya\s*gimana)",
        r"(lagi\s*apa|lg\s*apa|sedang\s*apa|ngapain(\s*nih)?)",
        r"(sudah|udah)\s*makan\s*(belum|blm)",
        r"(terima\s*kasih|terimakasih|trimakasih|makasih|makasi|thanks?|thank you|thx|ty)",
        r"(wkwk+|wk+|haha+|hehe+|:d)|[ğŸ˜‚ğŸ¤£ğŸ˜†]",
    ],
}

NEW_ITEMS_PATTERNS = {
    "vi": [
        r"(hÃ ng|sp|máº«u|sáº£n\s*pháº©m).*(má»›i|vá»«a\s*vá»|new\s*arrivals)",
        r"(cÃ³|Ä‘Ã£).*(máº«u|sáº£n\s*pháº©m).*(má»›i|vá»«a\s*vá»)",
        r"\b(new|má»›i|vá»«a vá»|new arrivals)\b",
    ],
    "en": [
        r"(new\s*arrivals?|new\s*products?|what's\s*new)",
        r"(any|have).*(new\s*items?)",
    ],
    "zh": [
        r"(æ–°å“|æ–°åˆ°|æ–°è²¨|æ–°è´§)",
        r"(æœ‰.*æ–°(å“|è´§|è²¨)|ä¾†äº†.*æ–°|æ¥äº†.*æ–°)",
    ],
    "th": [
        r"(à¸ªà¸´à¸™à¸„à¹‰à¸²à¹€à¸‚à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆ|à¸‚à¸­à¸‡à¹€à¸‚à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆ|à¸‚à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆ|à¸¡à¸²à¹ƒà¸«à¸¡à¹ˆ)",
        r"(à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¸¡à¹ˆ|à¸¡à¸µà¸ªà¸´à¸™à¸„à¹‰à¸²à¹ƒà¸«à¸¡à¹ˆà¹„à¸«à¸¡)",
    ],
    "id": [
        r"(produk baru|barang baru|baru datang)",
        r"(ada yang baru|ada produk baru)",
    ],
}

def _pat(pats: dict, lang: str):
    """Láº¥y list pattern theo ngÃ´n ngá»¯, fallback vá» DEFAULT_LANG náº¿u khÃ´ng cÃ³."""
    return pats.get(lang) or pats.get(DEFAULT_LANG, [])
# ===== GiÃ¡ / Price questions (multi-lang) =====
PRICE_PATTERNS = {
    "vi": [r"\bgiÃ¡\b", r"bao nhiÃªu", r"nhiÃªu tiá»n", r"\bgiÃ¡ bao nhiÃªu\b", r"\bbao nhieu\b"],
    "en": [r"\bprice\b", r"how much", r"\bcost\b"],
    "zh": [r"(ä»·æ ¼|å¹¾éŒ¢|å¤šå°‘é’±|å¤šå°‘éŒ¢)"],
    "th": [r"(à¸£à¸²à¸„à¸²|à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆ|à¹€à¸—à¹ˆà¸²à¹„à¸£)"],
    "id": [r"(harga|berapa)"],
}
def is_price_question(text: str, lang: str) -> bool:
    raw = (text or "")
    return any(re.search(p, raw, flags=re.I) for p in _pat(PRICE_PATTERNS, lang))


SYSTEM_STYLE = (
    "Báº¡n lÃ  trá»£ lÃ½ bÃ¡n hÃ ng Aloha tÃªn lÃ  Aloha Bot. TÃ´ng giá»ng: thÃ¢n thiá»‡n, chá»§ Ä‘á»™ng, "
    "tráº£ lá»i ngáº¯n gá»n nhÆ° ngÆ°á»i tháº­t; dÃ¹ng 1â€“3 emoji há»£p ngá»¯ cáº£nh (khÃ´ng láº¡m dá»¥ng). "
    "LuÃ´n dá»±a vÃ o CONTEXT (ná»™i dung RAG). KhÃ´ng bá»‹a. Náº¿u thiáº¿u dá»¯ liá»‡u thá»±c táº¿, nÃ³i 'mÃ¬nh chÆ°a cÃ³ dá»¯ liá»‡u' "
    "vÃ  há»i láº¡i 1 cÃ¢u Ä‘á»ƒ lÃ m rÃµ. TrÃ¬nh bÃ y dá»… Ä‘á»c: gáº¡ch Ä‘áº§u dÃ²ng khi liá»‡t kÃª; 1 cÃ¢u chá»‘t hÃ nh Ä‘á»™ng."
)

# FEW_SHOT_EXAMPLES
FEW_SHOT_EXAMPLES = [
    {"role":"user","content":[{"type":"input_text","text":"helo"}]},
    {"role":"assistant","content":[{"type":"output_text","text":"Xin chÃ o ğŸ‘‹ Ráº¥t vui Ä‘Æ°á»£c phá»¥c vá»¥ báº¡n! Báº¡n muá»‘n mÃ¬nh giÃºp gÃ¬ khÃ´ng nÃ¨? ğŸ™‚"}]},
    {"role":"user","content":[{"type":"input_text","text":"shop báº¡n cÃ³ nhá»¯ng gÃ¬"}]},
    {"role":"assistant","content":[{"type":"output_text","text":f"Má»i báº¡n tham quan cá»­a hÃ ng táº¡i Ä‘Ã¢y áº¡ ğŸ›ï¸ ğŸ‘‰ {SHOP_URL_MAP.get('vi', SHOP_URL)}"}]},
]
# ---- Intent routing ----
POLICY_KEYWORDS  = {"chÃ­nh sÃ¡ch","Ä‘á»•i tráº£","báº£o hÃ nh","ship","váº­n chuyá»ƒn","giao hÃ ng","tráº£ hÃ ng","refund"}
PRODUCT_KEYWORDS = {
    "mua","bÃ¡n","giÃ¡","size","kÃ­ch thÆ°á»›c","cháº¥t liá»‡u","mÃ u","há»£p","phÃ¹ há»£p",
    "dÃ¢y","Ä‘á»“ng há»“","vÃ²ng","case","Ã¡o","quáº§n","Ã¡o phÃ´ng","tshirt","t-shirt","Ã¡o thun",
    "sáº£n pháº©m", "bÃ¡nh","crepe","bÃ¡nh crepe","bÃ¡nh sáº§u riÃªng","milktea","trÃ  sá»¯a"
}
BROWSE_KEYWORDS  = {"cÃ³ nhá»¯ng gÃ¬","bÃ¡n gÃ¬","cÃ³ gÃ¬","danh má»¥c","catalog","xem hÃ ng","tham quan","xem shop","xem sáº£n pháº©m","shop cÃ³ gÃ¬","nhá»¯ng sáº£n pháº©m gÃ¬"}
_BROWSE_PATTERNS = [
    r"(shop|bÃªn báº¡n|bÃªn mÃ¬nh).*(bÃ¡n|cÃ³).*(gÃ¬|nhá»¯ng gÃ¬|nhá»¯ng sáº£n pháº©m gÃ¬)",
    r"(bÃ¡n|cÃ³).*(nhá»¯ng\s+)?sáº£n pháº©m gÃ¬",
]
# ==== Smalltalk & New arrivals ====



def detect_intent(text: str):
    raw = (text or "")
    t0  = re.sub(r"\s+", " ", raw.lower()).strip()
    lang = detect_lang(raw)

    if any(k in t0 for k in POLICY_KEYWORDS):  return "policy"
    if is_greeting(raw):                       return "greet"

    # smalltalk Ä‘a ngÃ´n ngá»¯
    if any(re.search(p, raw, flags=re.I) for p in _pat(SMALLTALK_PATTERNS, lang)):
        return "smalltalk"

    # browse: tá»« khÃ³a + pattern chung
    if any(k in t0 for k in BROWSE_KEYWORDS):  return "browse"
    if any(re.search(p, t0) for p in _BROWSE_PATTERNS):     return "browse"

    # há»i hÃ ng má»›i Ä‘a ngÃ´n ngá»¯
    if any(re.search(p, raw, flags=re.I) for p in _pat(NEW_ITEMS_PATTERNS, lang)):
        return "new_items"
    # Há»i giÃ¡ â†’ Æ°u tiÃªn product_info
    if is_price_question(raw, lang):
        return "product_info"

    # sáº£n pháº©m & mÃ´ táº£
    if any(k in t0 for k in PRODUCT_KEYWORDS): return "product"
    if "cÃ³ bÃ¡n" in t0 or "bÃ¡n khÃ´ng" in t0 or "bÃ¡n ko" in t0: return "product"
    if "cÃ³ gÃ¬ Ä‘áº·c biá»‡t" in t0 or "Ä‘iá»ƒm Ä‘áº·c biá»‡t" in t0 or "cÃ³ gÃ¬ Ä‘áº·t biá»‡t" in t0: return "product_info"

    return "other"

def build_messages(system, history, context, user_question):
    msgs = [{"role":"system","content":[{"type":"input_text","text":system}]}]
    msgs.extend(FEW_SHOT_EXAMPLES)
    for h in list(history)[-3:]:
        ctype = "output_text" if h["role"] == "assistant" else "input_text"
        msgs.append({"role": h["role"], "content":[{"type": ctype, "text": h["content"]}]})
    user_block = f"(Náº¿u há»¯u Ã­ch thÃ¬ dÃ¹ng CONTEXT)\nCONTEXT:\n{context}\n\nCÃ‚U Há»I: {user_question}"
    msgs.append({"role":"user","content":[{"type":"input_text","text":user_block}]})
    return msgs


# ---- Hiá»ƒn thá»‹ tá»“n kho/OOS + emoji ----
def _stock_line(d: dict) -> str:
    if d.get("available") and (d.get("inventory_quantity") is None or d.get("inventory_quantity", 0) > 0):
        return "cÃ²n hÃ ng âœ…"
    if d.get("inventory_quantity") == 0 or (d.get("status") and d.get("status") != "active"):
        return "háº¿t hÃ ng táº¡m thá»i â›”"
    return "tÃ¬nh tráº¡ng Ä‘ang cáº­p nháº­t â³"

def _shorten(txt: str, n=280) -> str:
    t = (txt or "").strip()
    return (t[:n].rstrip() + "â€¦") if len(t) > n else t
def _fmt_price(p, currency="â‚«"):
    if p is None:
        return None
    try:
        s = re.sub(r"[^\d.]", "", str(p))
        if not s:
            return None
        val = int(float(s))
        return f"{val:,.0f}".replace(",", ".") + (f" {currency}" if currency else "")
    except Exception:
        return str(p)

def _extract_price_number(txt: str):
    """Tráº£ vá» sá»‘ (float) náº¿u báº¯t Ä‘Æ°á»£c 199k/199.000Ä‘/199000 vnd..., else None"""
    if not txt:
        return None
    low = txt.lower()
    m = re.search(r"(\d[\d\.\s,]{2,})(?:\s?)(Ä‘|â‚«|vnd|vnÄ‘|k)\b", low)
    try:
        if m:
            num = re.sub(r"[^\d.]", "", m.group(1))
            v = float(num)
            return v*1000 if m.group(2) == "k" else v
        m2 = re.search(r"\b(\d{5,})\b", low)
        return float(m2.group(1)) if m2 else None
    except Exception:
        return None

def _price_value(d: dict):
    """Tráº£ numeric price tá»‘t nháº¥t tá»« meta; fallback báº¯t trong text."""
    for k in ("price","min_price","max_price"):
        v = d.get(k)
        if v is not None:
            try:
                return float(re.sub(r"[^\d.]", "", str(v)))
            except Exception:
                pass
    return _extract_price_number(d.get("text",""))

def _category_key_from_doc(d: dict):
    """XÃ¡c Ä‘á»‹nh 'dÃ²ng' sáº£n pháº©m Ä‘á»ƒ so minâ€“max: Æ°u tiÃªn product_type; fallback theo synonyms trong title/tags."""
    pt = (d.get("product_type") or "").strip()
    if pt:
        return _normalize_text(pt)
    raw = " ".join([d.get("title",""), d.get("tags","")])
    n1, n2 = _norm_both(raw)
    for key in VN_SYNONYMS.keys():
        k1, k2 = _norm_both(key)
        if k1 in n1 or k2 in n2:
            return _normalize_text(key)
    return "misc"

def _minmax_in_category(base_doc: dict):
    """TÃ¬m 1 máº«u ráº» nháº¥t vÃ  1 máº«u Ä‘áº¯t nháº¥t cÃ¹ng dÃ²ng (loáº¡i). Náº¿u khÃ´ng Ä‘á»§, fallback toÃ n shop."""
    if not META_PROD:
        return None, None
    cat = _category_key_from_doc(base_doc)
    def same_cat(x):
        return _category_key_from_doc(x) == cat
    cands = [x for x in META_PROD if same_cat(x)]
    if len(cands) < 2:
        cands = [x for x in META_PROD]  # fallback toÃ n shop

    items = []
    for x in cands:
        pv = _price_value(x)
        if pv is not None:
            items.append((pv, x))
    if not items:
        return None, None

    # loáº¡i chÃ­nh ra khá»i candidates náº¿u trÃ¹ng URL hoáº·c title
    def is_same(a, b):
        return (a.get("url") and a.get("url")==b.get("url")) or \
               ((a.get("title") or "").strip().lower()==(b.get("title") or "").strip().lower())

    items = [(p, x) for (p, x) in items if not is_same(x, base_doc)]
    if not items:
        return None, None

    items.sort(key=lambda t: t[0])
    low = items[0][1]
    high = items[-1][1]
    return low, high


def _extract_features_from_text(text_block: str):
    lines = []
    m = re.search(r"Specs:\s*(.+)", text_block, flags=re.I)
    if m:
        lines.extend(re.split(r"\s*\|\s*|\s*;\s*|,\s*", m.group(1)))
    m2 = re.search(r"Body:\s*(.+?)\s*URL:", text_block, flags=re.I|re.S)
    if m2:
        body = re.sub(r"\s+", " ", m2.group(1)).strip()
        chunks = re.split(r"(?<=[.!?])\s+", body)
        lines.extend(chunks)
    lines = [re.sub(r"\s+", " ", l).strip("â€¢- \n\t") for l in lines if l and len(l.strip()) > 0]
    uniq = []
    for l in lines:
        if l not in uniq:
            uniq.append(l)
        if len(uniq) >= 5:
            break
    return ["â€¢ " + _shorten(x, 80) for x in uniq[:5]]

# ====== Tá»ª KHÃ“A / Äá»’NG NGHÄ¨A (Ä‘a ngÃ´n ngá»¯) ======
VN_SYNONYMS = {
    # ===== Äá»“ng há»“ & phá»¥ kiá»‡n =====
    "Ä‘á»“ng há»“": [
        "dong ho","dong-ho","dongho","watch","watchface","watch face","bezel",
        "galaxy watch","apple watch","amazfit","seiko","casio","nh35","nh36",
        "automatic","mechanical","chronograph"
    ],
    "dÃ¢y Ä‘á»“ng há»“": [
        "day dong ho","daydongho","watch band","band","strap","nato","loop",
        "bracelet","mesh","leather strap","metal strap","silicone strap"
    ],
    "case Ä‘á»“ng há»“": [
        "case dong ho","vo dong ho","bao ve dong ho","bezel protector",
        "watch case","watch bumper","watch cover","protective case"
    ],
    "kÃ­nh cÆ°á»ng lá»±c": [
        "kinh cuong luc","tempered glass","screen protector","glass protector",
        "full glass","full cover","full glue","9h","anti-scratch","privacy glass"
    ],
    "á»‘p lÆ°ng": [
        "op lung","case","cover","bumper","clear case","tpu case",
        "silicone case","shockproof case","phone case","protective case"
    ],
    "vÃ²ng tay": [
        "vong tay","bracelet","bangle","chain bracelet","cuff"
    ],
    "Ã¡o thun": [
        "ao thun","ao phong","tshirt","t-shirt","tee","tee shirt","crewneck",
        "basic tee","unisex tee","oversize tee"
    ],
    "Ã¡o phÃ´ng": [
        "ao phong","ao thun","tshirt","t-shirt","tee"
    ],

    # ===== Äá»“ ngá»t/Ä‘á»“ uá»‘ng (bá»• sung cho shop) =====
    "bÃ¡nh": [
        "banh","cake","gateau","pastry","ç”œå“","é»å¿ƒ","à¹€à¸„à¹‰à¸","à¸‚à¸™à¸¡",
        "kue","kueh","roti manis"
    ],
    "bÃ¡nh crepe": [
        "banh crepe","crepe","mille crepe","crepe cake",
        "å¯ä¸½é¥¼","å¯éº—é¤…","åƒå±‚","åƒå±¤","åƒå±‚è›‹ç³•","åƒå±¤è›‹ç³•",
        "à¹€à¸„à¸£à¸›","à¹€à¸„à¸£à¸›à¹€à¸„à¹‰à¸","kue crepe","mille crepes","kue lapis"
    ],
    "bÃ¡nh sáº§u riÃªng": [
        "banh sau rieng","durian","durian cake","durian crepe",
        "æ¦´è²","æ¦´æ§¤","æ¦´è²åƒå±‚","æ¦´æ§¤åƒå±¤","æ¦´è²åƒå±‚è›‹ç³•","æ¦´æ§¤åƒå±¤è›‹ç³•","æ¦´è²å¯ä¸½é¥¼","æ¦´æ§¤å¯éº—é¤…",
        "à¹€à¸„à¸£à¸›à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™","à¹€à¸„à¹‰à¸à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™",
        "kue durian","crepe durian","kue lapis durian"
    ],
    "trÃ  sá»¯a": [
        "tra sua","milk tea","bubble tea","boba","boba tea","pearl milk tea",
        "å¥¶èŒ¶","çç å¥¶èŒ¶","æ³¢éœ¸å¥¶èŒ¶",
        "à¸Šà¸²à¸™à¸¡","à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸",
        "teh susu","teh susu boba","minuman boba","bubble tea id"
    ],
    "milktea": [
        "milk tea","bubble tea","boba","pearl milk tea",
        "å¥¶èŒ¶","çç å¥¶èŒ¶","à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸","teh susu","boba tea"
    ],

    # ===== Chinese (ZH) â€“ nhÃ³m theo khÃ¡i niá»‡m Ä‘á»ƒ báº¯t rá»™ng hÆ¡n =====
    "æ‰‹è¡¨": ["è…•è¡¨","watch","è¡¨å¸¦","è¡¨éˆ","è¡¨åœˆ","è¡¨å£³","è¡¨æ®¼","é’¢åŒ–è†œ","é‹¼åŒ–è†œ","ä¿æŠ¤å£³","ä¿è­·æ®¼"],
    "è¡¨å¸¦": ["è¡¨å¸¶","è¡¨é“¾","è¡¨éˆ","watch band","strap","çš®è¡¨å¸¦","é‡‘å±è¡¨å¸¦","ç¡…èƒ¶è¡¨å¸¦"],
    "é’¢åŒ–è†œ": ["é‹¼åŒ–è†œ","ç»ç’ƒè†œ","è´´è†œ","è²¼è†œ","ä¿æŠ¤è†œ","ä¿è­·è†œ","tempered glass","screen protector","å…¨èƒ¶","å…¨è† ","9h"],
    "æ‰‹æœºå£³": ["æ‰‹æ©Ÿæ®¼","ä¿æŠ¤å£³","ä¿è­·æ®¼","æ‰‹æœºå¥—","phone case","case","bumper","ä¿è­·æ®¼"],
    "Tæ¤": ["Tæ¤è¡«","çŸ­è¢–","åœ†é¢†","åœ“é ˜","tee","tshirt","t-shirt"], 
    "å¥¶èŒ¶": ["çç å¥¶èŒ¶","æ³¢éœ¸å¥¶èŒ¶","å¥¶ç›–èŒ¶","milk tea","bubble tea","boba"],
    "å¯ä¸½é¥¼": ["å¯éº—é¤…","æ³•å¼è–„é¥¼","æ³•å¼è–„é¤…","åƒå±‚","åƒå±¤","åƒå±‚è›‹ç³•","åƒå±¤è›‹ç³•","crepe","mille crepe"],
    "æ¦´è²": ["æ¦´æ§¤","durian","æ¦´è²åƒå±‚","æ¦´æ§¤åƒå±¤","æ¦´è²å¯ä¸½é¥¼","æ¦´æ§¤å¯éº—é¤…","æ¦´è²è›‹ç³•","æ¦´æ§¤è›‹ç³•"],

    # ===== Thai (TH) =====
    "à¸™à¸²à¸¬à¸´à¸à¸²": ["watch","à¸ªà¸²à¸¢à¸™à¸²à¸¬à¸´à¸à¸²","à¸Ÿà¸´à¸¥à¹Œà¸¡à¸à¸£à¸°à¸ˆà¸","à¸à¸£à¸­à¸šà¸™à¸²à¸¬à¸´à¸à¸²","à¹€à¸„à¸ªà¸™à¸²à¸¬à¸´à¸à¸²"],
    "à¸ªà¸²à¸¢à¸™à¸²à¸¬à¸´à¸à¸²": ["watch band","strap","à¸ªà¸²à¸¢à¸«à¸™à¸±à¸‡","à¸ªà¸²à¸¢à¹‚à¸¥à¸«à¸°","à¸ªà¸²à¸¢à¸‹à¸´à¸¥à¸´à¹‚à¸„à¸™","à¸™à¸²à¹‚à¸•à¹‰"],
    "à¸Ÿà¸´à¸¥à¹Œà¸¡à¸à¸£à¸°à¸ˆà¸": ["tempered glass","à¸à¸£à¸°à¸ˆà¸à¸à¸±à¸™à¸£à¸­à¸¢","full glue","9h","screen protector"],
    "à¹€à¸„à¸ªà¹‚à¸—à¸£à¸¨à¸±à¸à¸—à¹Œ": ["à¹€à¸„à¸ª","à¸‹à¸­à¸‡à¸¡à¸·à¸­à¸–à¸·à¸­","bumper","phone case","protective case"],
    "à¹€à¸ªà¸·à¹‰à¸­à¸¢à¸·à¸”": ["tshirt","t-shirt","tee","à¸„à¸­à¸à¸¥à¸¡","à¹‚à¸­à¹€à¸§à¸­à¸£à¹Œà¹„à¸‹à¸‹à¹Œ"],
    "à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸": ["à¸Šà¸²à¸™à¸¡","bubble tea","boba","milk tea"],
    "à¹€à¸„à¸£à¸›": ["à¹€à¸„à¸£à¸›à¹€à¸„à¹‰à¸","crepe","mille crepe"],
    "à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™": ["durian","à¹€à¸„à¸£à¸›à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™","à¹€à¸„à¹‰à¸à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™"],

    # ===== Indonesian (ID) =====
    "jam tangan": ["watch","tali jam","pelindung layar","casing jam","bezel","case jam"],
    "tali jam": ["watch band","strap","nato","kulit","logam","silikon"],
    "pelindung layar": ["tempered glass","screen protector","kaca tempered","9h","full glue"],
    "casing hp": ["case","casing","sarung hp","bumper","phone case"],
    "kaos": ["tshirt","t-shirt","tee","kaos oversize","kaos unisex"],
    "teh susu": ["bubble tea","boba","milk tea","minuman boba"],
    "crepe": ["kue crepe","mille crepe","kue lapis","crepe cake"],
    "durian": ["kue durian","crepe durian","kue lapis durian"]
}



def _query_tokens(q: str, lang: str = "vi") -> set:
    """Sinh token tá»« cÃ¢u há»i: cÃ³ dáº¥u, khÃ´ng dáº¥u, bigram, vÃ  cá»¥m Ä‘á»“ng nghÄ©a cÆ¡ báº£n."""
    n1, n2 = _norm_both(q)
    w1 = [w for w in n1.split() if len(w) > 1]
    w2 = [w for w in n2.split() if len(w) > 1]

    tokens = set(w1) | set(w2)

    # bigram cho cáº£ cÃ³ dáº¥u & khÃ´ng dáº¥u (Ä‘á»ƒ báº¯t â€œsáº§u riÃªngâ€, â€œbanh sauâ€)
    for words in (w1, w2):
        for i in range(len(words) - 1):
            tokens.add((words[i] + " " + words[i+1]).strip())
            tokens.add((words[i] + words[i+1]).strip())  # biáº¿n thá»ƒ khÃ´ng space

    combo_phrases = {
        "vi": ["Ä‘á»“ng há»“","dÃ¢y Ä‘á»“ng há»“","kÃ­nh cÆ°á»ng lá»±c","á»‘p lÆ°ng","Ã¡o thun","Ã¡o phÃ´ng","bÃ¡nh crepe","bÃ¡nh sáº§u riÃªng","trÃ  sá»¯a"],
        "en": ["watch band","screen protector","phone case","t shirt","t-shirt","mille crepe","durian crepe","milk tea","bubble tea","boba tea"],
        "zh": ["æ‰‹è¡¨","è¡¨å¸¦","é’¢åŒ–è†œ","æ‰‹æœºå£³","Tæ¤","å¯ä¸½é¥¼","æ¦´è²åƒå±‚","å¥¶èŒ¶","çç å¥¶èŒ¶"],
        "th": ["à¸™à¸²à¸¬à¸´à¸à¸²","à¸ªà¸²à¸¢à¸™à¸²à¸¬à¸´à¸à¸²","à¸Ÿà¸´à¸¥à¹Œà¸¡à¸à¸£à¸°à¸ˆà¸","à¹€à¸„à¸ªà¹‚à¸—à¸£à¸¨à¸±à¸à¸—à¹Œ","à¹€à¸ªà¸·à¹‰à¸­à¸¢à¸·à¸”","à¹€à¸„à¸£à¸›","à¹€à¸„à¸£à¸›à¸—à¸¸à¹€à¸£à¸µà¸¢à¸™","à¸Šà¸²à¸™à¸¡à¹„à¸‚à¹ˆà¸¡à¸¸à¸"],
        "id": ["jam tangan","tali jam","pelindung layar","casing hp","kaos","kue crepe","crepe durian","teh susu","bubble tea","boba"]
    }

    joined_n1 = " ".join(w1)
    for phrase in combo_phrases.get(lang, []):
        if phrase in joined_n1:
            tokens.add(_normalize_text(phrase))
            tokens.add(_normalize_text(_strip_accents(phrase)))

    # Ã¡nh xáº¡ synonyms: náº¿u text chá»©a â€œkeyâ€ thÃ¬ thÃªm táº¥t cáº£ synonym vÃ o tokens
    for key, syns in VN_SYNONYMS.items():
        key_n1, key_n2 = _norm_both(key)
        if key_n1 in n1 or key_n2 in n2:
            for s in syns:
                s1, s2 = _norm_both(s)
                tokens.add(s1)
                tokens.add(s2)
                tokens.add(s1.replace(" ", ""))
                tokens.add(s2.replace(" ", ""))

    return set(t for t in tokens if len(t) >= 2)


def filter_hits_by_query(hits, q, lang="vi"):
    """Giá»¯ hit náº¿u cÃ³ token/cá»¥m tá»« cÃ¢u há»i xuáº¥t hiá»‡n trong title/tags/type/variant (cÃ³ & khÃ´ng dáº¥u)."""
    if not hits:
        return []
    qtoks = _query_tokens(q, lang=lang)

    kept = []
    for d in hits:
        hay_raw = " ".join([
            d.get("title",""), d.get("tags",""), d.get("product_type",""), d.get("variant","")
        ])
        h1, h2 = _norm_both(hay_raw)
        h1_ns, h2_ns = h1.replace(" ", ""), h2.replace(" ", "")

        ok = any(
            (t in h1) or (t in h2) or (t.replace(" ","") in h1_ns) or (t.replace(" ","") in h2_ns)
            for t in qtoks
        )
        if ok:
            kept.append(d)
    return kept


def should_relax_filter(q: str, hits: list) -> bool:
    qn = _normalize_text(q)
    return len(qn.split()) <= 2 and len(hits) > 0

# ====== Compose tráº£ lá»i ======
def compose_product_reply(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    # Æ¯u tiÃªn currency trong meta; náº¿u khÃ´ng cÃ³, máº·c Ä‘á»‹nh â‚« cho VI
    currency = (hits[0].get("currency") or ("â‚«" if lang == "vi" else ""))

    items = []
    for d in hits[:2]:
        title     = d.get("title") or "Sáº£n pháº©m"
        variant   = d.get("variant")
        stock     = _stock_line(d)

        price_val = _price_value(d)
        price_str = _fmt_price(price_val, currency) if price_val is not None else None

        line = f"â€¢ {title}"
        if variant:
            line += f" ({variant})"
        if price_str:
            line += f" â€” {price_str}"
        line += f" â€” {stock}"
        items.append(line)

    raw = f"{t(lang,'suggest_hdr')}\n" + "\n".join(items) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)

def compose_product_info(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")

    d = hits[0]
    currency   = d.get("currency") or ("â‚«" if lang == "vi" else "")
    title      = d.get("title") or "Sáº£n pháº©m"
    stock      = _stock_line(d)

    price_val  = _price_value(d)
    price_line = f"GiÃ¡ tham kháº£o: {_fmt_price(price_val, currency)}" if price_val is not None else ""

    bullets = _extract_features_from_text(d.get("text",""))
    body    = "\n".join(bullets) if bullets else "â€¢ Thiáº¿t káº¿ tá»‘i giáº£n, dá»… phá»‘i Ä‘á»“\nâ€¢ Cháº¥t liá»‡u thoÃ¡ng, dá»… vá»‡ sinh"

    parts = [
        f"{t(lang,'highlights', title=title)}",
        body
    ]
    if price_line:
        parts.append(price_line)
    parts.extend([
        f"TÃ¬nh tráº¡ng: {stock}",
        t(lang,"product_pts")
    ])

    raw = "\n".join(parts).strip()
    return rephrase_casual(raw, intent="product", lang=lang)


def compose_contextual_answer(context, question, history):
    msgs = build_messages(SYSTEM_STYLE, history, context, question)
    _, reply = call_openai(msgs, temperature=0.6)
    return reply

def compose_price_with_suggestions(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback"), []

    main = hits[0]
    currency = main.get("currency") or ("â‚«" if lang == "vi" else "")
    main_price = _price_value(main)
    main_price_str = _fmt_price(main_price, currency) if main_price is not None else "Ä‘ang cáº­p nháº­t"

    low, high = _minmax_in_category(main)

    lines = []
    title = main.get("title") or "Sáº£n pháº©m"
    lines.append(f"VÃ¢ng áº¡, **{title}** Ä‘ang Ä‘Æ°á»£c shop bÃ¡n vá»›i **giÃ¡ cÃ´ng khai: {main_price_str}**.")
    sug = []
    if high:
        hp = _fmt_price(_price_value(high), currency)
        sug.append(f"â€¢ **CÃ¹ng dÃ²ng â€“ giÃ¡ cao nháº¥t:** {high.get('title','SP')} â€” {hp}")
    if low:
        lp = _fmt_price(_price_value(low), currency)  # Ä‘Ã£ sá»­a _ue â†’ _price_value
        sug.append(f"â€¢ **CÃ¹ng dÃ²ng â€“ giÃ¡ tháº¥p nháº¥t:** {low.get('title','SP')} â€” {lp}")

    if sug:
        lines.append("Báº¡n cÅ©ng cÃ³ thá»ƒ tham kháº£o thÃªm:")
        lines += sug
    lines.append(t(lang, "product_pts"))
    raw = "\n".join(lines)

    btns = [x for x in (high, low) if x]
    return rephrase_casual(raw, intent="product", lang=lang), btns[:2]


def answer_with_rag(user_id, user_question):
    s = _get_sess(user_id)
    hist = s["hist"]

    intent = detect_intent(user_question)
    lang = detect_lang(user_question)
    print(f"ğŸ” intent={intent} | ğŸ—£ï¸ lang={lang}")

    # â€”â€”â€” QUICK ROUTES â€”â€”â€”
    if intent == "greet":
        return greet_text(lang), []
    if intent == "smalltalk":
        return handle_smalltalk(user_question, lang=lang), []
    if intent == "browse":
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return t(lang, "browse", url=url), []
    if intent == "new_items":
        items = get_new_arrivals(days=30, topk=4)
        return compose_new_arrivals(lang=lang, items=items), items[:2]

    # â€”â€”â€” PRODUCT SEARCH â€”â€”â€”
    prod_hits, prod_scores = search_products_with_scores(user_question, topk=8)
    best = max(prod_scores or [0.0])

    filtered_hits = filter_hits_by_query(prod_hits, user_question, lang=lang) if STRICT_MATCH else prod_hits
    if STRICT_MATCH and not filtered_hits and should_relax_filter(user_question, prod_hits):
        print("ğŸ”§ relaxed_filter=True (fallback to unfiltered hits)")
        filtered_hits = prod_hits

    # So khá»›p tiÃªu Ä‘á» tÃ­nh trÃªn TOÃ€N Bá»˜ prod_hits
    title_ok = _has_title_overlap(user_question, prod_hits)

    # Náº¿u gáº§n Ä‘Ãºng tiÃªu Ä‘á» hoáº·c cÃ³ hit â†’ coi nhÆ° intent=product
    if intent == "other" and (filtered_hits or title_ok):
        intent = "product"

    # Náº¿u trÃ¹ng tiÃªu Ä‘á» nhÆ°ng filtered rá»—ng â†’ dÃ¹ng láº¡i prod_hits
    if title_ok and not filtered_hits:
        filtered_hits = prod_hits

    print(f"ğŸ“ˆ best_score={best:.3f}, hits={len(prod_hits)}, kept_after_filter={len(filtered_hits)}, title_ok={title_ok}")

    # â€”â€”â€” CONTEXT/POLICY â€”â€”â€”
    context = retrieve_context(user_question, topk=6)
    if intent == "policy" and context:
        ans = compose_contextual_answer(context, user_question, hist)
        ans = f"{t(lang,'policy_hint')} {ans}"
        return rephrase_casual(ans, intent="policy", temperature=0.5, lang=lang), []

    # â€”â€”â€” Æ¯U TIÃŠN Há»I GIÃ â€”â€”â€”
    if is_price_question(user_question, lang) and (filtered_hits or title_ok):
        print("â¡ï¸ route=price_questionâ†’price_with_suggestions")
        chosen = filtered_hits if filtered_hits else prod_hits
        reply, sug_hits = compose_price_with_suggestions(chosen, lang=lang)
        return reply, sug_hits

    # â€”â€”â€” PRODUCT BRANCHES â€”â€”â€”
    if intent in {"product", "product_info"}:
        # KhÃ´ng cÃ³ hit hoáº·c score tháº¥p & khÃ´ng trÃ¹ng tiÃªu Ä‘á» â†’ OOS/fallback link
        if not filtered_hits or (best < SCORE_MIN and not title_ok):
            url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
            print("â¡ï¸ route=oos_hint")
            return t(lang, "oos", url=url), []

    if intent == "product_info":
        print("â¡ï¸ route=product_info")
        return compose_product_info(filtered_hits, lang=lang), filtered_hits[:1]

    if intent in {"product", "other"} and filtered_hits and (best >= SCORE_MIN or title_ok):
        print("â¡ï¸ route=product_reply")
        return compose_product_reply(filtered_hits, lang=lang), filtered_hits[:2]

    # â€”â€”â€” CONTEXT FALLBACK â€”â€”â€”
    if context:
        ans = compose_contextual_answer(context, user_question, hist)
        print("â¡ï¸ route=ctx_fallback")
        return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []

    print("â¡ï¸ route=fallback")
    return t(lang, "fallback"), []


@app.route("/webhook", methods=["GET", "POST"])
def webhook():
    if request.method == "GET":
        token = request.args.get("hub.verify_token")
        challenge = request.args.get("hub.challenge")
        return (challenge, 200) if token == VERIFY_TOKEN else ("Invalid verification token", 403)

    payload = request.json or {}

    for entry in payload.get("entry", []):
        owner_id = str(entry.get("id"))           # Page ID hoáº·c IG Account ID
        access_token = TOKEN_MAP.get(owner_id)
        if not access_token:
            print("âš ï¸ No token mapped for:", owner_id); 
            continue

        for event in entry.get("messaging", []):
            if event.get("message", {}).get("is_echo"):
                continue
            if event.get("message") and "text" in event["message"]:
                psid = event["sender"]["id"]
                text = event["message"]["text"]
                mid  = event["message"].get("mid")

                sess = _get_sess(psid)
                if mid and sess["last_mid"] == mid: 
                    continue
                sess["last_mid"] = mid

                fb_mark_seen(psid, access_token)
                fb_typing_on(psid, access_token)

                _remember(psid, "user", text)
                reply, btn_hits = answer_with_rag(psid, text)
                _remember(psid, "assistant", reply)

                fb_send_text(psid, reply, access_token)

                if btn_hits:
                    buttons = []
                    for h in btn_hits[:2]:
                        if h.get("url"):
                            buttons.append({"type":"web_url","url":h["url"],"title":(h.get("title") or "Xem sáº£n pháº©m")[:20]})
                    if buttons:
                        fb_send_buttons(psid, "Xem nhanh:", buttons, access_token)

            elif event.get("postback", {}).get("payload"):
                psid = event["sender"]["id"]
                fb_send_text(psid, f"Báº¡n vá»«a chá»n: {event['postback']['payload']}", access_token)
            # quick reply payload (náº¿u dÃ¹ng quick_replies)
            elif event.get("message", {}).get("quick_reply", {}).get("payload"):
                psid = event["sender"]["id"]
                qr_payload = event["message"]["quick_reply"]["payload"]
                fb_send_text(psid, f"Báº¡n vá»«a chá»n: {qr_payload}", access_token)


    return "ok", 200


# ========= API =========
@app.route("/api/chat", methods=["POST"])
def chat():
    data = request.json or {}
    messages = data.get("messages", [])
    print("ğŸŒ /api/chat len =", len(messages))
    openai_raw, _ = call_openai(messages)
    return jsonify(openai_raw)

@app.route("/api/chat_rag", methods=["POST"])
def chat_rag():
    data = request.json or {}
    q = data.get("question", "")
    print("ğŸŒ /api/chat_rag question:", q)
    if not q:
        return jsonify({"error": "Missing 'question'"}), 400
    reply, _ = answer_with_rag("anonymous", q)
    return jsonify({"reply": reply})

@app.route("/api/product_search")
def api_product_search():
    q = (request.args.get("q") or "").strip()
    if not q:
        return jsonify({"ok": False, "msg": "missing q"}), 400
    lang = detect_lang(q)
    hits, scores = search_products_with_scores(q, topk=8)
    best = max(scores or [0.0])
    kept = filter_hits_by_query(hits, q, lang=lang) if STRICT_MATCH else hits
    if STRICT_MATCH and not kept and should_relax_filter(q, hits):
        kept = hits
    if not kept or best < SCORE_MIN:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return jsonify({"ok": True, "reply": t(lang, "oos", url=url), "items": []})
    reply = compose_product_reply(kept, lang=lang)
    return jsonify({"ok": True, "reply": reply, "items": kept[:2]})

# ========= IG OAuth callback & policy pages =========
@app.route("/auth/callback")
def auth_callback():
    code = request.args.get("code")
    print("ğŸ” /auth/callback code:", code)
    return f"Auth success! Code: {code}"

@app.route("/privacy")
def privacy():
    return """
    <h1>Privacy Policy - Aloha Bot</h1>
    <p>ChÃºng tÃ´i chá»‰ xá»­ lÃ½ ná»™i dung tin nháº¯n mÃ  ngÆ°á»i dÃ¹ng gá»­i tá»›i Fanpage Ä‘á»ƒ tráº£ lá»i.
    KhÃ´ng bÃ¡n/chia sáº» dá»¯ liá»‡u cÃ¡ nhÃ¢n. Dá»¯ liá»‡u phiÃªn trÃ² chuyá»‡n (session) chá»‰ lÆ°u táº¡m thá»i
    tá»‘i Ä‘a 30 phÃºt phá»¥c vá»¥ tráº£ lá»i vÃ  sáº½ tá»± xoÃ¡ sau Ä‘Ã³. Chá»‰ sá»‘ sáº£n pháº©m (vectors) lÃ  dá»¯ liá»‡u cÃ´ng khai tá»« cá»­a hÃ ng.</p>
    <p>LiÃªn há»‡ xoÃ¡ dá»¯ liá»‡u: gá»­i tin nháº¯n 'delete my data' tá»›i Fanpage hoáº·c email: <b>hoclac1225@email.com</b>.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

@app.route("/data_deletion")
def data_deletion():
    return """
    <h1>Data Deletion Instructions</h1>
    <p>Äá»ƒ yÃªu cáº§u xoÃ¡ dá»¯ liá»‡u: (1) nháº¯n 'delete my data' tá»›i Fanpage, hoáº·c (2) gá»­i email tá»›i <b>hoclac1225@email.com</b>
    kÃ¨m ID cuá»™c trÃ² chuyá»‡n. ChÃºng tÃ´i sáº½ xá»­ lÃ½ trong thá»i gian sá»›m nháº¥t.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

# ========= Debug & Health =========
@app.route("/debug/rag_status")
def rag_status():
    return jsonify({
        "vector_dir": os.path.abspath(VECTOR_DIR),
        "products_index": bool(IDX_PROD),
        "products_chunks": len(META_PROD) if META_PROD else 0,
        "policies_index": bool(IDX_POL),
        "policies_chunks": len(META_POL) if META_POL else 0,
        "sessions": len(SESS),
    })

@app.route("/health")
def health():
    return jsonify({
        "ok": True,
        "products": len(META_PROD) if META_PROD else 0,
        "policies": len(META_POL) if META_POL else 0
    })

# ========= Watcher: tá»± reload khi vector Ä‘á»•i =========
from apscheduler.schedulers.background import BackgroundScheduler

_last_vec_mtime = 0
def _mtime(path):
    try:
        return os.path.getmtime(path)
    except Exception:
        return 0

def _watch_vectors():
    global _last_vec_mtime
    idx_p = os.path.join(VECTOR_DIR, "products.index")
    meta_p = os.path.join(VECTOR_DIR, "products.meta.json")
    idx_k = os.path.join(VECTOR_DIR, "policies.index")
    meta_k = os.path.join(VECTOR_DIR, "policies.meta.json")

    newest = max(_mtime(idx_p), _mtime(meta_p), _mtime(idx_k), _mtime(meta_k))
    if newest and newest != _last_vec_mtime:
        print("ğŸ•µï¸ Detected vector change â†’ reload")
        if _reload_vectors():
            _last_vec_mtime = newest

def _start_vector_watcher():
    try:
        sch = BackgroundScheduler(timezone="Asia/Ho_Chi_Minh")
        sch.add_job(_watch_vectors, "interval", seconds=30, id="watch_vectors")
        sch.start()
        print("â±ï¸ Vector watcher started (30s)")
    except Exception as e:
        print("âš ï¸ Scheduler error:", repr(e))

# ======== MAIN ========
if __name__ == "__main__":
    _start_vector_watcher()
    port = int(os.getenv("PORT", 3000))
    print(f"ğŸš€ Starting app on 0.0.0.0:{port}")
    # app.run(host="0.0.0.0", port=port, debug=False)  # khi cháº¡y local
