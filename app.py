# -*- coding: utf-8 -*-
import unicodedata
import os, json, time, re, requests, numpy as np, faiss, threading, random
from collections import deque
from flask import Flask, request, jsonify, abort
from flask_cors import CORS
from dotenv import load_dotenv
from openai import OpenAI
import hmac, hashlib, base64


# ========= BOOTSTRAP =========
load_dotenv()
app = Flask(__name__)

allowed_origins = os.getenv("ALLOWED_ORIGINS", "")
origins = [o.strip() for o in allowed_origins.split(",")] if allowed_origins else ["*"]
CORS(app, origins=origins)

OPENAI_API_KEY   = os.getenv("OPENAI_API_KEY", "")
VERIFY_TOKEN     = os.getenv("VERIFY_TOKEN", "aloha_verify_123")
# ---- Multi-page map ----
# ---- Multi-id -> token (FB Page & IG) ----
TOKEN_MAP = {}

def _add_map(key, token):
    if key and token:
        TOKEN_MAP[str(key)] = token

for i in range(1, 11):
    pid = os.getenv(f"FB_PAGE_ID_{i}")
    ptk = os.getenv(f"FB_PAGE_TOKEN_{i}")
    _add_map(pid, ptk)  # map Page ID -> Page token

    igid = os.getenv(f"IG_ACCOUNT_ID_{i}")
    _add_map(igid, ptk) # map IG Account ID -> d√πng CHUNG Page token ƒë√£ li√™n k·∫øt IG
print("TOKEN_MAP size:", len(TOKEN_MAP))


VECTOR_DIR       = os.getenv("VECTOR_DIR", "./vectors")
# Shopify
SHOPIFY_SHOP = os.getenv("SHOPIFY_STORE", "")  # domain *.myshopify.com (tham chi·∫øu)
# Link shop m·∫∑c ƒë·ªãnh (fallback)
SHOP_URL         = os.getenv("SHOP_URL", "https://shop.aloha.id.vn/zh")
# ƒêa ng√¥n ng·ªØ
SUPPORTED_LANGS  = [s.strip() for s in os.getenv("SUPPORTED_LANGS", "vi,en,zh,th,id").split(",")]
DEFAULT_LANG     = os.getenv("DEFAULT_LANG", "vi")
SHOP_URL_MAP = {
    "vi": os.getenv("SHOP_URL_VI", SHOP_URL),
    "en": os.getenv("SHOP_URL_EN", SHOP_URL),
    "zh": os.getenv("SHOP_URL_ZH", SHOP_URL),
    "th": os.getenv("SHOP_URL_TH", SHOP_URL),
    "id": os.getenv("SHOP_URL_ID", SHOP_URL),
}

REPHRASE_ENABLED = os.getenv("REPHRASE_ENABLED", "true").lower() == "true"
EMOJI_MODE       = os.getenv("EMOJI_MODE", "cute")  # "cute" | "none"

# L·ªçc & ng∆∞·ª°ng ƒëi·ªÉm
SCORE_MIN    = float(os.getenv("PRODUCT_SCORE_MIN", "0.30"))
STRICT_MATCH = os.getenv("STRICT_MATCH", "true").lower() == "true"

print("=== BOOT ===")
print("VECTOR_DIR:", os.path.abspath(VECTOR_DIR))
print("TOKEN_MAP size:", len(TOKEN_MAP))
print("OPENAI key set:", bool(OPENAI_API_KEY))


# OpenAI
OPENAI_URL   = "https://api.openai.com/v1/responses"
OPENAI_MODEL = "gpt-4o-mini"
EMBED_MODEL  = os.getenv("EMBED_MODEL", "text-embedding-3-small")

oai_client   = OpenAI(api_key=OPENAI_API_KEY)

# ========= SMALL IN-MEMORY SESSION =========
SESS = {}
SESS_LOCK = threading.Lock()
SESSION_TTL = 60 * 30  # 30 ph√∫t kh√¥ng t∆∞∆°ng t√°c th√¨ reset

def _get_sess(user_id):
    now = time.time()
    with SESS_LOCK:
        s = SESS.get(user_id)
        if not s or now - s["ts"] > SESSION_TTL:
            s = {"hist": deque(maxlen=8), "last_mid": None, "ts": now}
            SESS[user_id] = s
        s["ts"] = now
        return s

def _remember(user_id, role, text):
    s = _get_sess(user_id)
    s["hist"].append({"role": role, "content": text})

# ========= OPENAI WRAPPER =========
def _to_chat_messages(messages):
    """Chuy·ªÉn format responses -> chat.completions ƒë·ªÉ fallback."""
    chat_msgs = []
    for m in messages:
        role = m.get("role", "user")
        parts = m.get("content", [])
        text = "\n".join([p.get("text","") for p in parts if p.get("type") in ("input_text","text")]).strip()
        chat_msgs.append({"role": role, "content": text})
    return chat_msgs

def call_openai(messages, temperature=0.7):
    """
    ∆Øu ti√™n /v1/responses; n·∫øu l·ªói -> fallback /v1/chat/completions.
    messages: [{"role":..., "content":[{"type":"input_text","text":"..."}]}]
    """
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": OPENAI_MODEL, "input": messages, "temperature": temperature}
    try:
        t0 = time.time()
        r = requests.post(OPENAI_URL, headers=headers, json=payload, timeout=40)
        dt = (time.time() - t0) * 1000
        print(f"üîÅ OpenAI responses status={r.status_code} in {dt:.0f}ms")
        if r.status_code == 200:
            data = r.json()
            try:
                reply = data["output"][0]["content"][0]["text"]
            except Exception:
                reply = data.get("output_text") or "M√¨nh ƒëang ·ªü ƒë√¢y, s·∫µn s√†ng h·ªó tr·ª£ b·∫°n!"
            return data, reply

        print(f"‚ùå responses body: {r.text[:800]}")
        # Fallback sang chat.completions
        chat_msgs = _to_chat_messages(messages)
        rc = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": chat_msgs, "temperature": temperature},
            timeout=40
        )
        print(f"üîÅ OpenAI chat.status={rc.status_code}")
        if rc.status_code == 200:
            data = rc.json()
            reply = (data.get("choices") or [{}])[0].get("message", {}).get("content") or "..."
            return data, reply

        print(f"‚ùå chat body: {rc.text[:800]}")
        return {}, "Xin l·ªói, hi·ªán m√¨nh g·∫∑p ch√∫t tr·ª•c tr·∫∑c. B·∫°n nh·∫Øn l·∫°i gi√∫p m√¨nh nh√©!"
    except Exception as e:
        print("‚ùå OpenAI error:", repr(e))
        return {}, "Xin l·ªói, hi·ªán m√¨nh g·∫∑p ch√∫t tr·ª•c tr·∫∑c. B·∫°n nh·∫Øn l·∫°i gi√∫p m√¨nh nh√©!"

# === Rephrase m·ªÅm + emoji cute ===
EMOJI_SETS = {
    "generic": ["‚ú®","üôÇ","üòä","üåü","üí´"],
    "greet":   ["üëã","üòä","üôÇ","‚ú®"],
    "browse":  ["üõçÔ∏è","üß≠","üîé","‚ú®"],
    "product": ["üõçÔ∏è","‚ú®","üëç","üíñ"],
    "oos":     ["üôè","‚õî","üòÖ","üõí"],
    "policy":  ["‚ÑπÔ∏è","üì¶","üõ°Ô∏è","‚úÖ"]
}
def em(intent="generic", n=1):
    if EMOJI_MODE == "none": return ""
    arr = EMOJI_SETS.get(intent, EMOJI_SETS["generic"])
    return " " + " ".join(random.choice(arr) for _ in range(max(1, n))).strip()

def rephrase_casual(text: str, intent="generic", temperature=0.7, lang: str = None) -> str:
    """L√†m m·ªÅm c√¢u + th√™m 1‚Äì2 emoji nh·∫π nh√†ng, ƒë√∫ng ng√¥n ng·ªØ lang."""
    if not REPHRASE_ENABLED:
        return text + (em(intent,1) if intent!="generic" else "")
    try:
        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
        msgs = [
            {"role":"system","content":f"B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng, vi·∫øt {lang or 'vi'} t·ª± nhi√™n, th√¢n thi·ªán, ng·∫Øn g·ªçn; th√™m 1‚Äì2 emoji ph√π h·ª£p (kh√¥ng l·∫°m d·ª•ng). Gi·ªØ nguy√™n d·ªØ ki·ªán/gi√°, kh√¥ng b·ªãa."},
            {"role":"user","content": f"Vi·∫øt l·∫°i ƒëo·∫°n sau b·∫±ng {lang or 'vi'} theo gi·ªçng th√¢n thi·ªán, k·∫øt th√∫c b·∫±ng 1 c√¢u ch·ªët h√†nh ƒë·ªông.\n---\n{text}\n---\n{em(intent,2)}"}
        ]
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json={"model": OPENAI_MODEL, "messages": msgs, "temperature": temperature, "max_tokens": 220},
            timeout=20
        )
        if r.status_code == 200:
            data = r.json()
            out = (data.get("choices") or [{}])[0].get("message", {}).get("content")
            return out.strip() if out else text
        else:
            print("‚ö†Ô∏è rephrase status=", r.status_code, r.text[:200])
            return text + em(intent,1)
    except Exception as e:
        print("‚ö†Ô∏è rephrase error:", repr(e))
        return text + em(intent,1)
def handle_smalltalk(text: str, lang: str = "vi") -> str:
    alt = {
        "vi": ["H√¥m nay shop nhi·ªÅu nƒÉng l∆∞·ª£ng l·∫Øm n√® ‚ö°", "Vui gh√™, ƒëang ship ƒë∆°n ƒë·ªÅu tay ü§ù"],
        "en": ["We‚Äôre full of energy today ‚ö°", "Orders are shipping steadily ü§ù"],
        "zh": ["‰ªäÂ§©Á≤æÁ•ûÊª°Êª° ‚ö°", "ËÆ¢ÂçïÊ≠£Âú®Á®≥ÂÆöÂèëË¥ß‰∏≠ ü§ù"],
        "th": ["‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡πâ‡∏≤‡∏ô‡πÑ‡∏ü‡πÅ‡∏£‡∏á‡∏°‡∏≤‡∏Å ‚ö°", "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏û‡πá‡∏Å‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏ö‡πÄ‡∏•‡∏¢ ü§ù"],
        "id": ["Toko lagi semangat banget hari ini ‚ö°", "Pesanan lagi ramai dikirim ü§ù"],
    }
    base = [t(lang, "smalltalk_hi")] + alt.get(lang, alt["vi"])
    follow = t(lang, "smalltalk_askback")
    raw = f"{random.choice(base)} {follow}"
    return rephrase_casual(raw, intent="generic", lang=lang, temperature=0.5)


# ========= FACEBOOK SENDER =========
def fb_call(path, payload=None, method="POST", params=None, page_token=None):
    if not page_token:
        print("‚ùå missing page_token for fb_call")
        return None
    url = f"https://graph.facebook.com/v19.0{path}"
    params = params or {}
    params["access_token"] = page_token
    try:
        r = requests.request(method, url, params=params, json=payload, timeout=15)
        return r
    except Exception as e:
        print("‚ö†Ô∏è FB API error:", repr(e))
        return None

def fb_mark_seen(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"mark_seen"}, page_token=page_token)

def fb_typing_on(user_id, page_token):
    fb_call("/me/messages", {"recipient":{"id":user_id}, "sender_action":"typing_on"}, page_token=page_token)

def fb_send_text(user_id, text, page_token):
    r = fb_call("/me/messages", {"recipient":{"id":user_id}, "message":{"text":text}}, page_token=page_token)
    print(f"üì© Send text status={getattr(r, 'status_code', None)}")

def fb_send_buttons(user_id, text, buttons, page_token):
    if not buttons: return
    payload = {
        "recipient": {"id": user_id},
        "message": {
            "attachment": {"type": "template","payload": {"template_type": "button","text": text,"buttons": buttons[:2]}}
        }
    }
    r = fb_call("/me/messages", payload, page_token=page_token)
    print(f"üîò ButtonAPI status={getattr(r,'status_code',None)}")


# ========= RAG (FAISS) =========

def _safe_read_index(prefix):
        
    try:
        idx_path  = os.path.join(VECTOR_DIR, f"{prefix}.index")
        meta_path = os.path.join(VECTOR_DIR, f"{prefix}.meta.json")
        if not (os.path.exists(idx_path) and os.path.exists(meta_path)):
            print(f"‚ö†Ô∏è Missing index/meta for '{prefix}'")
            return None, None
        idx  = faiss.read_index(idx_path)
        meta = json.load(open(meta_path, encoding="utf-8"))
        print(f"‚úÖ {prefix} loaded: {len(meta)} chunks")
        return idx, meta
    except Exception as e:
        print(f"‚ùå Load index '{prefix}':", repr(e))
        return None, None


IDX_PROD, META_PROD = _safe_read_index("products")
IDX_POL,  META_POL  = _safe_read_index("policies")

# === Reload vectors (FAISS) ===
def _reload_vectors():
    global IDX_PROD, META_PROD, IDX_POL, META_POL
    try:
        IDX_PROD, META_PROD = _safe_read_index("products")
        IDX_POL,  META_POL  = _safe_read_index("policies")
        ok = (IDX_PROD is not None or IDX_POL is not None)
        print("üîÑ Reload vectors:", ok,
              "| prod_chunks=", (len(META_PROD) if META_PROD else 0),
              "| policy_chunks=", (len(META_POL) if META_POL else 0))
        return ok
    except Exception as e:
        print("‚ùå reload vectors:", repr(e))
        return False

@app.post("/admin/reload_vectors")
def admin_reload_vectors():
    ok = _reload_vectors()
    return jsonify({"ok": ok})

def _embed_query(q: str) -> np.ndarray:
    t0 = time.time()
    resp = oai_client.embeddings.create(model=EMBED_MODEL, input=[q])
    v = np.array(resp.data[0].embedding, dtype="float32")[None, :]
    faiss.normalize_L2(v)
    print(f"üß© Embedding in {(time.time()-t0)*1000:.0f}ms")
    return v

def search_products_with_scores(query, topk=8):
    if IDX_PROD is None:
        return [], []
    v = _embed_query(query)
    try:
        D, I = IDX_PROD.search(v, topk)
        hits, scores, seen = [], [], set()
        for idx, score in zip(I[0], D[0]):
            if 0 <= idx < len(META_PROD):
                d = META_PROD[idx]
                key = (d.get("url"), (d.get("title") or "").lower().strip())
                if key in seen:
                    continue
                seen.add(key)
                hits.append(d)
                scores.append(float(score))
        print(f"üìö product hits: {len(hits)}")
        return hits, scores
    except Exception as e:
        print("‚ö†Ô∏è search_products_with_scores:", repr(e))
        return [], []

def retrieve_context(question, topk=6):
    if IDX_PROD is None and IDX_POL is None:
        return ""
    v = _embed_query(question)
    ctx = []
    if IDX_PROD is not None:
        try:
            _, Ip = IDX_PROD.search(v, topk)
            ctx += [META_PROD[i]["text"] for i in Ip[0] if i >= 0]
        except Exception as e:
            print("‚ö†Ô∏è search products:", repr(e))
    if IDX_POL is not None:
        try:
            _, Ik = IDX_POL.search(v, topk)
            ctx += [META_POL[i]["text"] for i in Ik[0] if i >= 0]
        except Exception as e:
            print("‚ö†Ô∏è search policies:", repr(e))
    print("üß† ctx pieces:", len(ctx))
    return "\n\n".join(ctx[:topk]) if ctx else ""
def _parse_ts(s):
    try:
        s = (s or "").replace("Z","").replace("T"," ")
        return time.mktime(time.strptime(s[:19], "%Y-%m-%d %H:%M:%S"))
    except Exception:
        return 0

def get_new_arrivals(days=30, topk=4):
    """T√¨m sp m·ªõi theo timestamp/tags 'new|m·ªõi|v·ª´a v·ªÅ'; fallback FAISS n·∫øu tr·ªëng."""
    if not META_PROD:
        return []
    now = time.time()
    cutoff = now - days*86400
    new_items = []
    for d in META_PROD:
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            if d.get(k):
                ts = max(ts, _parse_ts(d.get(k)))
        tags = (d.get("tags","") or "").lower()
        flag_new = any(x in tags for x in ["new","m·ªõi","v·ª´a v·ªÅ","new arrivals"])
        if flag_new or (ts and ts >= cutoff):
            new_items.append(d)

    if not new_items and IDX_PROD is not None:
        hits, _ = search_products_with_scores("new arrivals h√†ng m·ªõi v·ª´a v·ªÅ", topk=topk*2)
        new_items = hits

    def _key(d):
        ts = 0
        for k in ("created_at","published_at","updated_at"):
            ts = max(ts, _parse_ts(d.get(k)))
        return ts
    new_items.sort(key=_key, reverse=True)
    return new_items[:topk]

def compose_new_arrivals(lang: str = "vi", items=None):
    items = items or []
    if not items:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return rephrase_casual(t(lang,"browse", url=url), intent="browse", lang=lang)
    lines = []
    for d in items[:2]:
        title = d.get("title") or "S·∫£n ph·∫©m"
        price = d.get("price")
        stock = _stock_line(d)
        line = f"‚Ä¢ {title}"
        if price: line += f" ‚Äî {price} ƒë"
        line += f" ‚Äî {stock}"
        lines.append(line)
    raw = f"{t(lang,'new_hdr')}\n" + "\n".join(lines) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)


# ========= INTENT, PERSONA, FEW-SHOT & NATURAL REPLY =========
GREETS = {"hi","hello","hey","helo","heloo","h√≠","h√¨","ch√†o","xin ch√†o","alo","aloha","hello bot","hi bot"}
def is_greeting(text: str) -> bool:
    t = re.sub(r"\s+", " ", (text or "").lower()).strip()
    return any(w in t for w in GREETS) and len(t) <= 40

# ‚Äî‚Äî‚Äî Ng√¥n ng·ªØ: detect & c√¢u ch·ªØ
def detect_lang(text: str) -> str:
    t = (text or "").strip()
    if not t:
        return DEFAULT_LANG
    if re.search(r"[\u4e00-\u9fff]", t):  # CJK
        return "zh" if "zh" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"[\u0E00-\u0E7F]", t):  # Thai
        return "th" if "th" in SUPPORTED_LANGS else DEFAULT_LANG
    # Vietnamese diacritics
    if re.search(r"[ƒÉ√¢√™√¥∆°∆∞ƒë√°√†·∫£√£·∫°·∫Ø·∫±·∫≥·∫µ·∫∑·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç·ªë·ªì·ªï·ªó·ªô∆°√≥·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ]", t, flags=re.I):
        return "vi" if "vi" in SUPPORTED_LANGS else DEFAULT_LANG
    if re.search(r"\b(yang|dan|tidak|saja|terima|kasih)\b", t.lower()):
        return "id" if "id" in SUPPORTED_LANGS else DEFAULT_LANG
    return "en" if "en" in SUPPORTED_LANGS else DEFAULT_LANG

# ====== MULTI-LANG PATTERNS (smalltalk & new arrivals) ======
# M·ªói ng√¥n ng·ªØ l√† 1 list regex. C√≥ th·ªÉ b·ªï sung d·∫ßn m√† kh√¥ng ƒë·ª•ng ch·ªó kh√°c.
# ==== Smalltalk & New arrivals (multi-lang) ====

# ========= I18N STRINGS & HELPERS =========
LANG_STRINGS = {
    "vi": {
        "greet": "Xin ch√†o üëã R·∫•t vui ƒë∆∞·ª£c ph·ª•c v·ª• b·∫°n! B·∫°n mu·ªën m√¨nh gi√∫p g√¨ kh√¥ng n√®? üôÇ",
        "browse": "M·ªùi b·∫°n v√†o web tham quan ·∫° üõçÔ∏è üëâ {url}",
        "oos": "Xin l·ªói üôè s·∫£n ph·∫©m ƒë√≥ hi·ªán **ƒëang h·∫øt h√†ng** t·∫°i shop. B·∫°n th·ª≠ xem c√°c m·∫´u t∆∞∆°ng t·ª± tr√™n web nh√© üëâ {url} ‚ú®",
        "fallback": "M√¨nh ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ch·∫Øc ch·∫Øn ü§î. B·∫°n m√¥ t·∫£ th√™m m·∫´u/ki·ªÉu d√°ng/ch·∫•t li·ªáu ƒë·ªÉ m√¨nh t∆∞ v·∫•n chu·∫©n h∆°n nha ‚ú®",
        "suggest_hdr": "M√¨nh ƒë·ªÅ xu·∫•t v√†i l·ª±a ch·ªçn ph√π h·ª£p",
        "product_pts": "B·∫°n th√≠ch ki·ªÉu m·∫£nh hay th·ªÉ thao? M√¨nh l·ªçc th√™m m√†u & size gi√∫p b·∫°n nh√©.",
        "highlights": "{title} c√≥ v√†i ƒëi·ªÉm n·ªïi b·∫≠t n√®",
        "policy_hint": "Theo ch√≠nh s√°ch shop:",
        "smalltalk_hi": "M√¨nh v·∫´n ·ªïn n√®, ƒëang tr·ª±c chat h·ªó tr·ª£ b·∫°n ƒë√¢y üòä",
        "smalltalk_askback": "B·∫°n c·∫ßn t√¨m m√≥n n√†o h√¥m nay ƒë·ªÉ m√¨nh g·ª£i √Ω nhanh nha?",
        "new_hdr": "H√†ng m·ªõi v·ªÅ n√® ‚ú®",
    },
    "en": {
        "greet": "Hello üëã Happy to help! How can I assist you today? üôÇ",
        "browse": "Feel free to explore our store üõçÔ∏è üëâ {url}",
        "oos": "Sorry üôè that item is **out of stock** right now. Check similar picks here üëâ {url} ‚ú®",
        "fallback": "I‚Äôm missing a bit of info ü§î. Share style/material/size and I‚Äôll refine the picks ‚ú®",
        "suggest_hdr": "Here are a few good options",
        "product_pts": "Prefer a slim or sporty style? I can filter color & size for you.",
        "highlights": "{title} highlights",
        "policy_hint": "Store policy:",
        "smalltalk_hi": "I‚Äôm doing great and ready to help üòä",
        "smalltalk_askback": "What are you looking for today so I can suggest fast?",
        "new_hdr": "New arrivals ‚ú®",
    },
    "zh": {
        "greet": "‰Ω†Â•Ω üëã ÂæàÈ´òÂÖ¥‰∏∫‰Ω†ÊúçÂä°ÔºÅÈúÄË¶ÅÊàëÂ∏Æ‰Ω†ÂÅö‰ªÄ‰πàÂë¢ÔºüüôÇ",
        "browse": "Ê¨¢ËøéÈÄõÈÄõÊàë‰ª¨ÁöÑÂïÜÂ∫ó üõçÔ∏è üëâ {url}",
        "oos": "Êä±Ê≠â üôè ËØ•ÂïÜÂìÅÁõÆÂâç**Áº∫Ë¥ß**„ÄÇÂèØ‰ª•ÂÖàÁúãÁúãÁ±ª‰ººÁöÑÊ¨æÂºè üëâ {url} ‚ú®",
        "fallback": "ËøòÈúÄË¶Å‰∏Ä‰∫õ‰ø°ÊÅØÂì¶ ü§î„ÄÇËØ¥‰∏ãÈ£éÊ†º/ÊùêË¥®/Â∞∫ÂØ∏ÔºåÊàëÂÜçÁ≤æÂáÜÊé®Ëçê ‚ú®",
        "suggest_hdr": "Áªô‰Ω†Âá†Ê¨æÂêàÈÄÇÁöÑÈÄâÊã©",
        "product_pts": "ÊÉ≥Ë¶ÅÁ∫§ÁªÜËøòÊòØËøêÂä®È£éÔºüÊàëÂèØ‰ª•ÊåâÈ¢úËâ≤ÂíåÂ∞∫Á†ÅÂÜçÁ≠õ‰∏ÄËΩÆ„ÄÇ",
        "highlights": "{title} ÁöÑ‰∫ÆÁÇπ",
        "policy_hint": "Â∫óÈì∫ÊîøÁ≠ñÔºö",
        "smalltalk_hi": "ÊàëÂæàÂ•ΩÔºåÈöèÊó∂‰∏∫‰Ω†ÊúçÂä°Âì¶ üòä",
        "smalltalk_askback": "‰ªäÂ§©ÊÉ≥Êâæ‰ªÄ‰πàÔºüÊàëÂ∏Æ‰Ω†Âø´ÈÄüÊé®ËçêÔΩû",
        "new_hdr": "Êñ∞ÂìÅ‰∏äÊû∂ ‚ú®",
    },
    "th": {
        "greet": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ üëã ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á üôÇ",
        "browse": "‡πÄ‡∏ä‡∏¥‡∏ç‡∏ä‡∏°‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ üõçÔ∏è üëâ {url}",
        "oos": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ üôè ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ä‡∏¥‡πâ‡∏ô‡∏ô‡∏±‡πâ‡∏ô **‡∏´‡∏°‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß** ‡∏Ñ‡πà‡∏∞ ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà üëâ {url} ‚ú®",
        "fallback": "‡∏Ç‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î‡∏ô‡∏∞‡∏Ñ‡∏∞/‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô‡∏™‡πÑ‡∏ï‡∏•‡πå/‡∏ß‡∏±‡∏™‡∏î‡∏∏/‡∏Ç‡∏ô‡∏≤‡∏î ‚ú®",
        "suggest_hdr": "‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°",
        "product_pts": "‡∏ä‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏£‡∏µ‡∏¢‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏õ‡∏≠‡∏£‡πå‡∏ï‡∏î‡∏µ? ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏±‡∏î‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÑ‡∏ã‡∏ã‡πå‡πÉ‡∏´‡πâ‡∏≠‡∏µ‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏∞/‡∏Ñ‡∏£‡∏±‡∏ö",
        "highlights": "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á {title}",
        "policy_hint": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡πâ‡∏≤‡∏ô:",
        "smalltalk_hi": "‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö/‡∏Ñ‡πà‡∏∞ üòä",
        "smalltalk_askback": "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏≤‡∏Å‡∏´‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß ‡πÜ ‡∏ô‡∏∞",
        "new_hdr": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà ‚ú®",
    },
    "id": {
        "greet": "Halo üëã Senang membantu! Ada yang bisa saya bantu? üôÇ",
        "browse": "Silakan jelajahi toko kami üõçÔ∏è üëâ {url}",
        "oos": "Maaf üôè produk itu **sedang kosong**. Coba lihat yang mirip di sini üëâ {url} ‚ú®",
        "fallback": "Butuh info tambahan ü§î. Sebutkan gaya/bahan/ukuran ya, biar saya saringkan ‚ú®",
        "suggest_hdr": "Beberapa pilihan yang cocok",
        "product_pts": "Suka model tipis atau sporty? Saya bisa saring warna & ukuran.",
        "highlights": "Hal menarik dari {title}",
        "policy_hint": "Kebijakan toko:",
        "smalltalk_hi": "Baik banget dan siap bantu üòä",
        "smalltalk_askback": "Hari ini cari apa? Biar saya rekomendasikan cepat ya.",
        "new_hdr": "Produk baru ‚ú®",
    },
}


def t(lang: str, key: str, **kw) -> str:
    lang2 = lang if lang in LANG_STRINGS else DEFAULT_LANG
    s = (LANG_STRINGS.get(lang2, {}).get(key)
         or LANG_STRINGS.get(DEFAULT_LANG, {}).get(key)
         or "")
    try:
        return s.format(**kw)
    except Exception:
        return s


def greet_text(lang: str) -> str:
    return t(lang, "greet")

# ==== Smalltalk & New arrivals (multi-lang) ====

SMALLTALK_PATTERNS = {
    "vi": [
        # bn/b·∫°n kh·ªèe/kh·ªèe/kho·∫ª kh√¥ng/ko/h√¥ng/hem/hok
        r"\b(bn|b·∫°n)\s*(kh[o√≥√≤·ªè√µ·ªç∆°·ªõ·ªù·ªü·ª°·ª£]e|khoe|kho·∫ª|kh·ªèe)\s*(kh[o√¥]ng|ko|k|h[o∆°√¥√≥√≤√µ·ªè·ªç]ng|hong|h√¥ng|hem|hok)\b",
        r"\b(kh[o√≥√≤·ªè√µ·ªç∆°·ªõ·ªù·ªü·ª°·ª£]e|khoe|kho·∫ª|kh·ªèe)\s*(kh[o√¥]ng|ko|k|h[o∆°√¥√≥√≤√µ·ªè·ªç]ng|hong|h√¥ng|hem|hok)\b",
        # ·ªïn kh√¥ng
        r"\b(·ªïn|on)\s*(kh[o√¥]ng|ko|k|hong|h[o∆°]ng|hem|hok)\b",
        # h√¥m nay th·∫ø n√†o / nay sao
        r"\b(h[o√¥]m?\s*nay|nay)\s*(b·∫°n|bn)?\s*(th[·∫øe]\s*n[a√†]o|sao|ok\s*kh[o√¥]ng)\b",
        # ƒëang l√†m g√¨ / d·∫°o n√†y
        r"\b(ƒëang\s*l√†m\s*g√¨|l√†m\s*g√¨( v·∫≠y| ƒë√≥)?|l√†m\s*chi|lam\s*gi)\b",
        r"\b(d·∫°o\s*n√†y|dao\s*nay)\b",
        # ƒÉn c∆°m ch∆∞a / ng·ªß ch∆∞a
        r"\b(ƒÉn\s*c∆°m\s*ch∆∞a|ƒÉn\s*ch∆∞a|an\s*chua|u·ªëng\s*ch∆∞a|ng[u∆∞]\s*ch[aƒÉ]u?)\b",
        # c·∫£m ∆°n / thanks
        r"\b(c[·∫£a]m\s?∆°n|c[√°a]m\s?∆°n|thanks?|thank you|ty|tks|thx)\b",
        # c∆∞·ªùi/emoji
        r"\b(haha+|hihi+|hehe+|kkk+|=D|:d|:v|:3)\b|[üòÇü§£üòÜ]",
    ],
    "en": [
        r"\b(how('?s)?\s*it\s*going|how\s*are\s*(you|u)|how\s*r\s*u|how\s*u\s*doin?g?)\b",
        r"\b(what('?s)?\s*up|wass?up|sup|wyd)\b",
        r"\b(have\s*you\s*eaten|had\s*(lunch|dinner)|grabbed\s*(lunch|food))\b",
        r"\b(thanks?|thank\s*(you|u)|ty|thx|tysm|tks)\b",
        r"\b(lol|lmao|rofl|haha+|hehe+|:d)\b|[üòÇü§£üòÜ]",
    ],
    "zh": [
        r"(‰Ω†Â•ΩÂêó|Â¶≥Â•ΩÂêó|ÊúÄËøëÊÄé‰πàÊ†∑|ÊúÄËøëÂ¶Ç‰Ωï|ÊúÄËøëËøòÂ•Ω|ËøòÂ•ΩÂêó|ÂøÉÊÉÖÂ¶Ç‰Ωï|ÂºÄÂøÉÂêó|ÈÅéÂæóÊÄéÊ®£|ËøáÂæóÊÄéÊ†∑)",
        r"(ÂêÉÈ•≠‰∫ÜÂêó|ÂêÉËøáÈ•≠Ê≤°|ÂêÉ‰∫ÜÊ≤°|ÂêÉ‰∫ÜÂêó)",
        r"(Ë∞¢Ë∞¢|Â§öË∞¢|Ë¨ùË¨ù|ÊÑüË¨ù|ÊÑüË∞¢|Ë¨ùÂï¶|Ë∞¢Ë∞¢Âï¶|Ë∞¢Âï¶)",
        r"(ÂìàÂìà+|ÂòøÂòø+|ÂëµÂëµ+|Âó®Âó®+)|[üòÇü§£üòÜ]",
    ],
    "th": [
        r"(‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ(‡πÑ‡∏´‡∏°|‡∏°‡∏±‡πâ‡∏¢|‡∏õ‡πà‡∏≤‡∏ß)|‡πÄ‡∏õ‡πá‡∏ô(‡πÑ‡∏á|‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£)‡∏ö‡πâ‡∏≤‡∏á|‡πÇ‡∏≠‡πÄ‡∏Ñ(‡πÑ‡∏´‡∏°|‡∏°‡∏±‡πâ‡∏¢))",
        r"(‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà|‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£|‡∏ó‡∏≥‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà)",
        r"(‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß(‡∏´‡∏£‡∏∑‡∏≠)?‡∏¢‡∏±‡∏á|‡∏ó‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß(‡∏´‡∏£‡∏∑‡∏≠)?‡∏¢‡∏±‡∏á)",
        r"(‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì(‡∏Ñ‡∏£‡∏±‡∏ö|‡∏Ñ‡πà‡∏∞)?|‡∏Ç‡∏≠‡∏ö‡πÉ‡∏à|thanks?|thank you|ty)",
        r"(‡∏Æ‡πà‡∏≤+‡πÜ+|555+)|[üòÇü§£üòÜ]",
    ],
    "id": [
        r"(apa\s*kabar|gimana\s*kabarnya|gmn\s*kabar|kabarnya\s*gimana)",
        r"(lagi\s*apa|lg\s*apa|sedang\s*apa|ngapain(\s*nih)?)",
        r"(sudah|udah)\s*makan\s*(belum|blm)",
        r"(terima\s*kasih|terimakasih|trimakasih|makasih|makasi|thanks?|thank you|thx|ty)",
        r"(wkwk+|wk+|haha+|hehe+|:d)|[üòÇü§£üòÜ]",
    ],
}

NEW_ITEMS_PATTERNS = {
    "vi": [
        r"(h√†ng|sp|m·∫´u|s·∫£n\s*ph·∫©m).*(m·ªõi|v·ª´a\s*v·ªÅ|new\s*arrivals)",
        r"(c√≥|ƒë√£).*(m·∫´u|s·∫£n\s*ph·∫©m).*(m·ªõi|v·ª´a\s*v·ªÅ)",
        r"\b(new|m·ªõi|v·ª´a v·ªÅ|new arrivals)\b",
    ],
    "en": [
        r"(new\s*arrivals?|new\s*products?|what's\s*new)",
        r"(any|have).*(new\s*items?)",
    ],
    "zh": [
        r"(Êñ∞ÂìÅ|Êñ∞Âà∞|Êñ∞Ë≤®|Êñ∞Ë¥ß)",
        r"(Êúâ.*Êñ∞(ÂìÅ|Ë¥ß|Ë≤®)|‰æÜ‰∫Ü.*Êñ∞|Êù•‰∫Ü.*Êñ∞)",
    ],
    "th": [
        r"(‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà|‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà|‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà|‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà)",
        r"(‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡∏°‡πà|‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏´‡∏°)",
    ],
    "id": [
        r"(produk baru|barang baru|baru datang)",
        r"(ada yang baru|ada produk baru)",
    ],
}

def _pat(pats: dict, lang: str):
    """L·∫•y list pattern theo ng√¥n ng·ªØ, fallback v·ªÅ DEFAULT_LANG n·∫øu kh√¥ng c√≥."""
    return pats.get(lang) or pats.get(DEFAULT_LANG, [])


SYSTEM_STYLE = (
    "B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng Aloha t√™n l√† Aloha Bot. T√¥ng gi·ªçng: th√¢n thi·ªán, ch·ªß ƒë·ªông, "
    "tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ ng∆∞·ªùi th·∫≠t; d√πng 1‚Äì3 emoji h·ª£p ng·ªØ c·∫£nh (kh√¥ng l·∫°m d·ª•ng). "
    "Lu√¥n d·ª±a v√†o CONTEXT (n·ªôi dung RAG). Kh√¥ng b·ªãa. N·∫øu thi·∫øu d·ªØ li·ªáu th·ª±c t·∫ø, n√≥i 'm√¨nh ch∆∞a c√≥ d·ªØ li·ªáu' "
    "v√† h·ªèi l·∫°i 1 c√¢u ƒë·ªÉ l√†m r√µ. Tr√¨nh b√†y d·ªÖ ƒë·ªçc: g·∫°ch ƒë·∫ßu d√≤ng khi li·ªát k√™; 1 c√¢u ch·ªët h√†nh ƒë·ªông."
)

FEW_SHOT_EXAMPLES = [
    {"role":"user","content":[{"type":"input_text","text":"helo"}]},
    {"role":"assistant","content":[{"type":"input_text","text":"Xin ch√†o üëã R·∫•t vui ƒë∆∞·ª£c ph·ª•c v·ª• b·∫°n! B·∫°n mu·ªën m√¨nh gi√∫p g√¨ kh√¥ng n√®? üôÇ"}]},
    {"role":"user","content":[{"type":"input_text","text":"shop b·∫°n c√≥ nh·ªØng g√¨"}]},
    {"role":"assistant","content":[{"type":"input_text","text":"M·ªùi b·∫°n tham quan c·ª≠a h√†ng t·∫°i ƒë√¢y ·∫° üõçÔ∏è üëâ https://shop.aloha.id.vn/zh"}]},
]

# ---- Intent routing ----
POLICY_KEYWORDS  = {"ch√≠nh s√°ch","ƒë·ªïi tr·∫£","b·∫£o h√†nh","ship","v·∫≠n chuy·ªÉn","giao h√†ng","tr·∫£ h√†ng","refund"}
PRODUCT_KEYWORDS = {
    "mua","b√°n","gi√°","size","k√≠ch th∆∞·ªõc","ch·∫•t li·ªáu","m√†u","h·ª£p","ph√π h·ª£p",
    "d√¢y","ƒë·ªìng h·ªì","v√≤ng","case","√°o","qu·∫ßn","√°o ph√¥ng","tshirt","t-shirt","√°o thun",
    "s·∫£n ph·∫©m", "b√°nh","crepe","b√°nh crepe","b√°nh s·∫ßu ri√™ng","milktea","tr√† s·ªØa"
}
BROWSE_KEYWORDS  = {"c√≥ nh·ªØng g√¨","b√°n g√¨","c√≥ g√¨","danh m·ª•c","catalog","xem h√†ng","tham quan","xem shop","xem s·∫£n ph·∫©m","shop c√≥ g√¨","nh·ªØng s·∫£n ph·∫©m g√¨"}
_BROWSE_PATTERNS = [
    r"(shop|b√™n b·∫°n|b√™n m√¨nh).*(b√°n|c√≥).*(g√¨|nh·ªØng g√¨|nh·ªØng s·∫£n ph·∫©m g√¨)",
    r"(b√°n|c√≥).*(nh·ªØng\s+)?s·∫£n ph·∫©m g√¨",
]
# ==== Smalltalk & New arrivals ====



def detect_intent(text: str):
    raw = (text or "")
    t0  = re.sub(r"\s+", " ", raw.lower()).strip()
    lang = detect_lang(raw)

    if any(k in t0 for k in POLICY_KEYWORDS):  return "policy"
    if is_greeting(raw):                       return "greet"

    # smalltalk ƒëa ng√¥n ng·ªØ
    if any(re.search(p, raw, flags=re.I) for p in _pat(SMALLTALK_PATTERNS, lang)):
        return "smalltalk"

    # browse: t·ª´ kh√≥a + pattern chung
    if any(k in t0 for k in BROWSE_KEYWORDS):  return "browse"
    if any(re.search(p, t0) for p in _BROWSE_PATTERNS):     return "browse"

    # h·ªèi h√†ng m·ªõi ƒëa ng√¥n ng·ªØ
    if any(re.search(p, raw, flags=re.I) for p in _pat(NEW_ITEMS_PATTERNS, lang)):
        return "new_items"

    # s·∫£n ph·∫©m & m√¥ t·∫£
    if any(k in t0 for k in PRODUCT_KEYWORDS): return "product"
    if "c√≥ b√°n" in t0 or "b√°n kh√¥ng" in t0 or "b√°n ko" in t0: return "product"
    if "c√≥ g√¨ ƒë·∫∑c bi·ªát" in t0 or "ƒëi·ªÉm ƒë·∫∑c bi·ªát" in t0 or "c√≥ g√¨ ƒë·∫∑t bi·ªát" in t0: return "product_info"

    return "other"

def build_messages(system, history, context, user_question):
    msgs = [{"role":"system","content":[{"type":"input_text","text":system}]}]
    msgs.extend(FEW_SHOT_EXAMPLES)
    for h in list(history)[-3:]:
        msgs.append({"role":h["role"], "content":[{"type":"input_text","text":h["content"]}]})
    user_block = f"(N·∫øu h·ªØu √≠ch th√¨ d√πng CONTEXT)\nCONTEXT:\n{context}\n\nC√ÇU H·ªéI: {user_question}"
    msgs.append({"role":"user","content":[{"type":"input_text","text":user_block}]})
    return msgs

# ---- Hi·ªÉn th·ªã t·ªìn kho/OOS + emoji ----
def _stock_line(d: dict) -> str:
    if d.get("available") and (d.get("inventory_quantity") is None or d.get("inventory_quantity", 0) > 0):
        return "c√≤n h√†ng ‚úÖ"
    if d.get("inventory_quantity") == 0 or (d.get("status") and d.get("status") != "active"):
        return "h·∫øt h√†ng t·∫°m th·ªùi ‚õî"
    return "t√¨nh tr·∫°ng ƒëang c·∫≠p nh·∫≠t ‚è≥"

def _shorten(txt: str, n=280) -> str:
    t = (txt or "").strip()
    return (t[:n].rstrip() + "‚Ä¶") if len(t) > n else t

def _extract_features_from_text(text_block: str):
    lines = []
    m = re.search(r"Specs:\s*(.+)", text_block, flags=re.I)
    if m:
        lines.extend(re.split(r"\s*\|\s*|\s*;\s*|,\s*", m.group(1)))
    m2 = re.search(r"Body:\s*(.+?)\s*URL:", text_block, flags=re.I|re.S)
    if m2:
        body = re.sub(r"\s+", " ", m2.group(1)).strip()
        chunks = re.split(r"(?<=[.!?])\s+", body)
        lines.extend(chunks)
    lines = [re.sub(r"\s+", " ", l).strip("‚Ä¢- \n\t") for l in lines if l and len(l.strip()) > 0]
    uniq = []
    for l in lines:
        if l not in uniq:
            uniq.append(l)
        if len(uniq) >= 5:
            break
    return ["‚Ä¢ " + _shorten(x, 80) for x in uniq[:5]]

# ====== T·ª™ KH√ìA / ƒê·ªíNG NGHƒ®A (ƒëa ng√¥n ng·ªØ) ======
VN_SYNONYMS = {
    "ƒë·ªìng h·ªì": ["dong ho","dong-ho","dongho","watch","watchface","galaxy watch","apple watch","amazfit","seiko","casio","nh35","nh36"],
    "d√¢y ƒë·ªìng h·ªì": ["day dong ho","daydongho","watch band","band","strap","loop","bracelet"],
    "k√≠nh c∆∞·ªùng l·ª±c": ["kinh cuong luc","tempered glass","screen protector","full glass"],
    "·ªëp l∆∞ng": ["op lung","case","cover","bumper"],
    "√°o thun": ["ao thun","ao phong","tshirt","t-shirt","tee"],
    "√°o ph√¥ng": ["ao phong","ao thun","tshirt","t-shirt","tee"],
    # Chinese / Thai / Indonesian
    "ÊâãË°®": ["ËÖïË°®","watch","Ë°®Â∏¶","Èí¢ÂåñËÜú","‰øùÊä§Â£≥"],
    "‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤": ["watch","‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å","‡πÄ‡∏Ñ‡∏™"],
    "jam tangan": ["watch","tali jam","pelindung layar","case"],
}

def _normalize_text(s: str) -> str:
    return re.sub(r"[^0-9a-z\u00c0-\u024f\u1e00-\u1eff\u4e00-\u9fff\u0E00-\u0E7F ]", " ", (s or "").lower())

def _query_tokens(q: str, lang: str = "vi") -> set:
    qn = _normalize_text(q)
    words = [w for w in qn.split() if len(w) > 1]
    tokens = set(words)
    for i in range(len(words) - 1):
        tokens.add((words[i] + words[i+1]).strip())  # bigram

    joined = " ".join(words)
    combo_phrases = {
        "vi": ["ƒë·ªìng h·ªì","√°o thun","√°o ph√¥ng","d√¢y ƒë·ªìng h·ªì","k√≠nh c∆∞·ªùng l·ª±c","·ªëp l∆∞ng"],
        "en": ["watch band","screen protector","phone case","t shirt","t-shirt","watch"],
        "zh": ["ÊâãË°®","ËÖïË°®","Ë°®Â∏¶","Èí¢ÂåñËÜú","ÊâãÊú∫Â£≥"],
        "th": ["‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏™‡∏≤‡∏¢‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤","‡∏ü‡∏¥‡∏•‡πå‡∏°‡∏Å‡∏£‡∏∞‡∏à‡∏Å","‡πÄ‡∏Ñ‡∏™"],
        "id": ["jam tangan","tali jam","pelindung layar","case"],
    }
    for phrase in combo_phrases.get(lang, []):
        if phrase in joined:
            tokens.add(_normalize_text(phrase).replace(" ", ""))
            for k, syns in VN_SYNONYMS.items():
                if _normalize_text(k) in _normalize_text(phrase):
                    for s in syns:
                        tokens.add(_normalize_text(s).replace(" ", ""))

    for synlist in VN_SYNONYMS.values():
        for s in synlist:
            if s in joined:
                tokens.add(_normalize_text(s).replace(" ", ""))
    return tokens

def filter_hits_by_query(hits, q, lang="vi"):
    """Gi·ªØ l·∫°i hit c√≥ √≠t nh·∫•t 1 token/c·ª•m c·ªßa c√¢u h·ªèi xu·∫•t hi·ªán trong title/tags/type/variant."""
    if not hits:
        return []
    qtoks = _query_tokens(q, lang=lang)
    kept = []
    for d in hits:
        hay = _normalize_text(" ".join([d.get("title",""), d.get("tags",""), d.get("product_type",""), d.get("variant","")]))
        hay_no_space = hay.replace(" ", "")
        ok = any((t in hay) or (t in hay_no_space) for t in qtoks)
        if ok:
            kept.append(d)
    return kept

def should_relax_filter(q: str, hits: list) -> bool:
    qn = _normalize_text(q)
    return len(qn.split()) <= 2 and len(hits) > 0

# ====== Compose tr·∫£ l·ªùi ======
def compose_product_reply(hits, lang: str = "vi"):
    items = []
    for d in hits[:2]:
        title   = d.get("title") or "S·∫£n ph·∫©m"
        price   = d.get("price")
        variant = d.get("variant")
        stock   = _stock_line(d)
        line = f"‚Ä¢ {title}"
        if variant: line += f" ({variant})"
        if price:   line += f" ‚Äî {price} ƒë"
        line += f" ‚Äî {stock}"
        items.append(line)
    if not items:
        return t(lang, "fallback")
    raw = f"{t(lang,'suggest_hdr')}\n" + "\n".join(items) + "\n\n" + t(lang,"product_pts")
    return rephrase_casual(raw, intent="product", lang=lang)

def compose_product_info(hits, lang: str = "vi"):
    if not hits:
        return t(lang, "fallback")
    d = hits[0]
    title = d.get("title") or "S·∫£n ph·∫©m"
    price = d.get("price")
    stock = _stock_line(d)
    bullets = _extract_features_from_text(d.get("text",""))
    body = "\n".join(bullets) if bullets else "‚Ä¢ Thi·∫øt k·∫ø t·ªëi gi·∫£n, d·ªÖ ph·ªëi ƒë·ªì\n‚Ä¢ Ch·∫•t li·ªáu tho√°ng, d·ªÖ v·ªá sinh"
    price_line = f"Gi√° tham kh·∫£o: {price} ƒë" if price else ""
    raw = (
        f"{t(lang,'highlights', title=title)}\n"
        f"{body}\n"
        f"{price_line}\n"
        f"T√¨nh tr·∫°ng: {stock}\n"
        f"{t(lang,'product_pts')}"
    ).strip()
    return rephrase_casual(raw, intent="product", lang=lang)

def compose_contextual_answer(context, question, history):
    msgs = build_messages(SYSTEM_STYLE, history, context, question)
    _, reply = call_openai(msgs, temperature=0.6)
    return reply

def answer_with_rag(user_id, user_question):
    s = _get_sess(user_id)
    hist = s["hist"]

    intent = detect_intent(user_question)
    lang = detect_lang(user_question)
    print(f"üîé intent={intent} | üó£Ô∏è lang={lang}")

  # trong answer_with_rag, ngay sau print(...)
    if intent == "greet":
        return greet_text(lang), []

    if intent == "smalltalk":               # <-- ƒë∆∞a smalltalk l√™n tr∆∞·ªõc
        return handle_smalltalk(user_question, lang=lang), []

    if intent == "browse":
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return t(lang, "browse", url=url), []

    if intent == "new_items":
        items = get_new_arrivals(days=30, topk=4)
        return compose_new_arrivals(lang=lang, items=items), items[:2]


    prod_hits, prod_scores = search_products_with_scores(user_question, topk=8)
    best = max(prod_scores or [0.0])

    filtered_hits = filter_hits_by_query(prod_hits, user_question, lang=lang) if STRICT_MATCH else prod_hits
    if STRICT_MATCH and not filtered_hits and should_relax_filter(user_question, prod_hits):
        print("üîß relaxed_filter=True (fallback to unfiltered hits)")
        filtered_hits = prod_hits

    print(f"üìà best_score={best:.3f}, hits={len(prod_hits)}, kept_after_filter={len(filtered_hits)}")
    context = retrieve_context(user_question, topk=6)

    if intent == "policy" and context:
        ans = compose_contextual_answer(context, user_question, hist)
        ans = f"{t(lang,'policy_hint')} {ans}"
        return rephrase_casual(ans, intent="policy", temperature=0.5, lang=lang), []

    if intent in {"product","product_info","other"}:
        if not filtered_hits or best < SCORE_MIN:
            url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
            return (t(lang, "oos", url=url)), []

    if intent == "product_info":
        return compose_product_info(filtered_hits, lang=lang), filtered_hits[:1]

    if intent in {"product","other"} and filtered_hits and best >= SCORE_MIN:
        return compose_product_reply(filtered_hits, lang=lang), filtered_hits[:2]

    if context:
        ans = compose_contextual_answer(context, user_question, hist)
        return rephrase_casual(ans, intent="generic", temperature=0.7, lang=lang), []
    
    

    return (t(lang, "fallback")), []

@app.route("/webhook", methods=["GET", "POST"])
def webhook():
    if request.method == "GET":
        token = request.args.get("hub.verify_token")
        challenge = request.args.get("hub.challenge")
        return (challenge, 200) if token == VERIFY_TOKEN else ("Invalid verification token", 403)

    payload = request.json or {}

    for entry in payload.get("entry", []):
        owner_id = str(entry.get("id"))           # Page ID ho·∫∑c IG Account ID
        access_token = TOKEN_MAP.get(owner_id)
        if not access_token:
            print("‚ö†Ô∏è No token mapped for:", owner_id); 
            continue

        for event in entry.get("messaging", []):
            if event.get("message", {}).get("is_echo"):
                continue
            if event.get("message") and "text" in event["message"]:
                psid = event["sender"]["id"]
                text = event["message"]["text"]
                mid  = event["message"].get("mid")

                sess = _get_sess(psid)
                if mid and sess["last_mid"] == mid: 
                    continue
                sess["last_mid"] = mid

                fb_mark_seen(psid, access_token)
                fb_typing_on(psid, access_token)

                _remember(psid, "user", text)
                reply, btn_hits = answer_with_rag(psid, text)
                _remember(psid, "assistant", reply)

                fb_send_text(psid, reply, access_token)

                if btn_hits:
                    buttons = []
                    for h in btn_hits[:2]:
                        if h.get("url"):
                            buttons.append({"type":"web_url","url":h["url"],"title":(h.get("title") or "Xem s·∫£n ph·∫©m")[:20]})
                    if buttons:
                        fb_send_buttons(psid, "Xem nhanh:", buttons, access_token)

            elif event.get("postback", {}).get("payload"):
                psid = event["sender"]["id"]
                fb_send_text(psid, f"B·∫°n v·ª´a ch·ªçn: {event['postback']['payload']}", access_token)
            # quick reply payload (n·∫øu d√πng quick_replies)
            elif event.get("message", {}).get("quick_reply", {}).get("payload"):
                psid = event["sender"]["id"]
                qr_payload = event["message"]["quick_reply"]["payload"]
                fb_send_text(psid, f"B·∫°n v·ª´a ch·ªçn: {qr_payload}", access_token)


    return "ok", 200


# ========= API =========
@app.route("/api/chat", methods=["POST"])
def chat():
    data = request.json or {}
    messages = data.get("messages", [])
    print("üåê /api/chat len =", len(messages))
    openai_raw, _ = call_openai(messages)
    return jsonify(openai_raw)

@app.route("/api/chat_rag", methods=["POST"])
def chat_rag():
    data = request.json or {}
    q = data.get("question", "")
    print("üåê /api/chat_rag question:", q)
    if not q:
        return jsonify({"error": "Missing 'question'"}), 400
    reply, _ = answer_with_rag("anonymous", q)
    return jsonify({"reply": reply})

@app.route("/api/product_search")
def api_product_search():
    q = (request.args.get("q") or "").strip()
    if not q:
        return jsonify({"ok": False, "msg": "missing q"}), 400
    lang = detect_lang(q)
    hits, scores = search_products_with_scores(q, topk=8)
    best = max(scores or [0.0])
    kept = filter_hits_by_query(hits, q, lang=lang) if STRICT_MATCH else hits
    if STRICT_MATCH and not kept and should_relax_filter(q, hits):
        kept = hits
    if not kept or best < SCORE_MIN:
        url = SHOP_URL_MAP.get(lang, SHOP_URL_MAP.get(DEFAULT_LANG, SHOP_URL))
        return jsonify({"ok": True, "reply": t(lang, "oos", url=url), "items": []})
    reply = compose_product_reply(kept, lang=lang)
    return jsonify({"ok": True, "reply": reply, "items": kept[:2]})

# ========= IG OAuth callback & policy pages =========
@app.route("/auth/callback")
def auth_callback():
    code = request.args.get("code")
    print("üîÅ /auth/callback code:", code)
    return f"Auth success! Code: {code}"

@app.route("/privacy")
def privacy():
    return """
    <h1>Privacy Policy - Aloha Bot</h1>
    <p>Ch√∫ng t√¥i ch·ªâ x·ª≠ l√Ω n·ªôi dung tin nh·∫Øn m√† ng∆∞·ªùi d√πng g·ª≠i t·ªõi Fanpage ƒë·ªÉ tr·∫£ l·ªùi.
    Kh√¥ng b√°n/chia s·∫ª d·ªØ li·ªáu c√° nh√¢n. D·ªØ li·ªáu phi√™n tr√≤ chuy·ªán (session) ch·ªâ l∆∞u t·∫°m th·ªùi
    t·ªëi ƒëa 30 ph√∫t ph·ª•c v·ª• tr·∫£ l·ªùi v√† s·∫Ω t·ª± xo√° sau ƒë√≥. Ch·ªâ s·ªë s·∫£n ph·∫©m (vectors) l√† d·ªØ li·ªáu c√¥ng khai t·ª´ c·ª≠a h√†ng.</p>
    <p>Li√™n h·ªá xo√° d·ªØ li·ªáu: g·ª≠i tin nh·∫Øn 'delete my data' t·ªõi Fanpage ho·∫∑c email: <b>hoclac1225@email.com</b>.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

@app.route("/data_deletion")
def data_deletion():
    return """
    <h1>Data Deletion Instructions</h1>
    <p>ƒê·ªÉ y√™u c·∫ßu xo√° d·ªØ li·ªáu: (1) nh·∫Øn 'delete my data' t·ªõi Fanpage, ho·∫∑c (2) g·ª≠i email t·ªõi <b>hoclac1225@email.com</b>
    k√®m ID cu·ªôc tr√≤ chuy·ªán. Ch√∫ng t√¥i s·∫Ω x·ª≠ l√Ω trong th·ªùi gian s·ªõm nh·∫•t.</p>
    """, 200, {"Content-Type": "text/html; charset=utf-8"}

# ========= Debug & Health =========
@app.route("/debug/rag_status")
def rag_status():
    return jsonify({
        "vector_dir": os.path.abspath(VECTOR_DIR),
        "products_index": bool(IDX_PROD),
        "products_chunks": len(META_PROD) if META_PROD else 0,
        "policies_index": bool(IDX_POL),
        "policies_chunks": len(META_POL) if META_POL else 0,
        "sessions": len(SESS),
    })

@app.route("/health")
def health():
    return jsonify({
        "ok": True,
        "products": len(META_PROD) if META_PROD else 0,
        "policies": len(META_POL) if META_POL else 0
    })

# ========= Watcher: t·ª± reload khi vector ƒë·ªïi =========
from apscheduler.schedulers.background import BackgroundScheduler

_last_vec_mtime = 0
def _mtime(path):
    try:
        return os.path.getmtime(path)
    except Exception:
        return 0

def _watch_vectors():
    global _last_vec_mtime
    idx_p = os.path.join(VECTOR_DIR, "products.index")
    meta_p = os.path.join(VECTOR_DIR, "products.meta.json")
    idx_k = os.path.join(VECTOR_DIR, "policies.index")
    meta_k = os.path.join(VECTOR_DIR, "policies.meta.json")

    newest = max(_mtime(idx_p), _mtime(meta_p), _mtime(idx_k), _mtime(meta_k))
    if newest and newest != _last_vec_mtime:
        print("üïµÔ∏è Detected vector change ‚Üí reload")
        if _reload_vectors():
            _last_vec_mtime = newest

def _start_vector_watcher():
    try:
        sch = BackgroundScheduler(timezone="Asia/Ho_Chi_Minh")
        sch.add_job(_watch_vectors, "interval", seconds=30, id="watch_vectors")
        sch.start()
        print("‚è±Ô∏è Vector watcher started (30s)")
    except Exception as e:
        print("‚ö†Ô∏è Scheduler error:", repr(e))

# ======== MAIN ========
if __name__ == "__main__":
    _start_vector_watcher()
    port = int(os.getenv("PORT", 3000))
    print(f"üöÄ Starting app on 0.0.0.0:{port}")
    # app.run(host="0.0.0.0", port=port, debug=False)  # khi ch·∫°y local
